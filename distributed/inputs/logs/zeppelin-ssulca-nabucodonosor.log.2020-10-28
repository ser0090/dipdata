 INFO [2020-10-28 00:01:38,213] ({qtp89387388-517} VFSNotebookRepo.java[save]:196) - Saving note:2FNR8STJW
 INFO [2020-10-28 00:02:05,193] ({qtp89387388-536} VFSNotebookRepo.java[save]:196) - Saving note:2FNR8STJW
 INFO [2020-10-28 00:02:10,561] ({qtp89387388-535} VFSNotebookRepo.java[save]:196) - Saving note:2FNR8STJW
 INFO [2020-10-28 00:04:07,259] ({qtp89387388-551} VFSNotebookRepo.java[save]:196) - Saving note:2FNR8STJW
 INFO [2020-10-28 00:05:27,501] ({qtp89387388-552} VFSNotebookRepo.java[save]:196) - Saving note:2FNR8STJW
 INFO [2020-10-28 00:05:27,616] ({qtp89387388-551} VFSNotebookRepo.java[save]:196) - Saving note:2FNR8STJW
 INFO [2020-10-28 00:05:27,621] ({pool-2-thread-3} SchedulerFactory.java[jobStarted]:114) - Job 20171013-175507_696892344 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 00:05:27,621] ({pool-2-thread-3} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20171013-175507_696892344, interpreter: pyspark, note_id: 2FNR8STJW, user: anonymous]
 INFO [2020-10-28 00:05:27,926] ({pool-2-thread-3} NotebookServer.java[afterStatusChange]:2314) - Job 20171013-175507_696892344 is finished successfully, status: FINISHED
 INFO [2020-10-28 00:05:27,967] ({pool-2-thread-3} VFSNotebookRepo.java[save]:196) - Saving note:2FNR8STJW
 INFO [2020-10-28 00:05:27,971] ({pool-2-thread-3} SchedulerFactory.java[jobFinished]:120) - Job 20171013-175507_696892344 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 00:05:50,154] ({qtp89387388-558} VFSNotebookRepo.java[save]:196) - Saving note:2FNR8STJW
 INFO [2020-10-28 00:05:50,159] ({pool-2-thread-14} SchedulerFactory.java[jobStarted]:114) - Job 20201023-002135_640378642 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 00:05:50,160] ({pool-2-thread-14} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20201023-002135_640378642, interpreter: pyspark, note_id: 2FNR8STJW, user: anonymous]
 INFO [2020-10-28 00:05:50,338] ({pool-2-thread-14} NotebookServer.java[afterStatusChange]:2314) - Job 20201023-002135_640378642 is finished successfully, status: FINISHED
 INFO [2020-10-28 00:05:50,386] ({pool-2-thread-14} VFSNotebookRepo.java[save]:196) - Saving note:2FNR8STJW
 INFO [2020-10-28 00:05:50,392] ({pool-2-thread-14} SchedulerFactory.java[jobFinished]:120) - Job 20201023-002135_640378642 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 00:08:13,078] ({qtp89387388-558} VFSNotebookRepo.java[save]:196) - Saving note:2FNR8STJW
 INFO [2020-10-28 00:08:13,083] ({pool-2-thread-8} SchedulerFactory.java[jobStarted]:114) - Job 20201023-002155_1881617494 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 00:08:13,083] ({pool-2-thread-8} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20201023-002155_1881617494, interpreter: pyspark, note_id: 2FNR8STJW, user: anonymous]
 INFO [2020-10-28 00:08:13,306] ({pool-2-thread-8} NotebookServer.java[afterStatusChange]:2314) - Job 20201023-002155_1881617494 is finished successfully, status: FINISHED
 INFO [2020-10-28 00:08:13,345] ({pool-2-thread-8} VFSNotebookRepo.java[save]:196) - Saving note:2FNR8STJW
 INFO [2020-10-28 00:08:13,350] ({pool-2-thread-8} SchedulerFactory.java[jobFinished]:120) - Job 20201023-002155_1881617494 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 00:08:28,616] ({qtp89387388-558} VFSNotebookRepo.java[save]:196) - Saving note:2FNR8STJW
 INFO [2020-10-28 00:08:28,621] ({pool-2-thread-15} SchedulerFactory.java[jobStarted]:114) - Job 20201023-002135_640378642 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 00:08:28,622] ({pool-2-thread-15} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20201023-002135_640378642, interpreter: pyspark, note_id: 2FNR8STJW, user: anonymous]
 INFO [2020-10-28 00:08:28,783] ({pool-2-thread-15} NotebookServer.java[afterStatusChange]:2314) - Job 20201023-002135_640378642 is finished successfully, status: FINISHED
 INFO [2020-10-28 00:08:28,829] ({pool-2-thread-15} VFSNotebookRepo.java[save]:196) - Saving note:2FNR8STJW
 INFO [2020-10-28 00:08:28,834] ({pool-2-thread-15} SchedulerFactory.java[jobFinished]:120) - Job 20201023-002135_640378642 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 00:13:32,353] ({qtp89387388-615} VFSNotebookRepo.java[save]:196) - Saving note:2FNR8STJW
 INFO [2020-10-28 00:13:36,595] ({qtp89387388-615} VFSNotebookRepo.java[save]:196) - Saving note:2FNR8STJW
 INFO [2020-10-28 00:13:44,844] ({qtp89387388-615} VFSNotebookRepo.java[save]:196) - Saving note:2FNR8STJW
 INFO [2020-10-28 00:14:14,159] ({qtp89387388-615} VFSNotebookRepo.java[save]:196) - Saving note:2FNR8STJW
 INFO [2020-10-28 00:14:40,702] ({qtp89387388-616} VFSNotebookRepo.java[save]:196) - Saving note:2FNR8STJW
 INFO [2020-10-28 00:14:41,474] ({qtp89387388-615} VFSNotebookRepo.java[save]:196) - Saving note:2FNR8STJW
 INFO [2020-10-28 00:14:41,479] ({pool-2-thread-5} SchedulerFactory.java[jobStarted]:114) - Job 20191123-214023_2104486544 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 00:14:41,479] ({pool-2-thread-5} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20191123-214023_2104486544, interpreter: pyspark, note_id: 2FNR8STJW, user: anonymous]
 INFO [2020-10-28 00:14:41,778] ({pool-2-thread-5} NotebookServer.java[afterStatusChange]:2314) - Job 20191123-214023_2104486544 is finished successfully, status: FINISHED
 INFO [2020-10-28 00:14:41,817] ({pool-2-thread-5} VFSNotebookRepo.java[save]:196) - Saving note:2FNR8STJW
 INFO [2020-10-28 00:14:41,821] ({pool-2-thread-5} SchedulerFactory.java[jobFinished]:120) - Job 20191123-214023_2104486544 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 00:15:03,962] ({qtp89387388-615} VFSNotebookRepo.java[save]:196) - Saving note:2FNR8STJW
 INFO [2020-10-28 00:15:04,061] ({qtp89387388-616} VFSNotebookRepo.java[save]:196) - Saving note:2FNR8STJW
 INFO [2020-10-28 00:15:04,066] ({pool-2-thread-16} SchedulerFactory.java[jobStarted]:114) - Job 20191123-214023_2104486544 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 00:15:04,066] ({pool-2-thread-16} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20191123-214023_2104486544, interpreter: pyspark, note_id: 2FNR8STJW, user: anonymous]
 INFO [2020-10-28 00:15:04,375] ({pool-2-thread-16} NotebookServer.java[afterStatusChange]:2314) - Job 20191123-214023_2104486544 is finished successfully, status: FINISHED
 INFO [2020-10-28 00:15:04,415] ({pool-2-thread-16} VFSNotebookRepo.java[save]:196) - Saving note:2FNR8STJW
 INFO [2020-10-28 00:15:04,419] ({pool-2-thread-16} SchedulerFactory.java[jobFinished]:120) - Job 20191123-214023_2104486544 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 00:19:19,015] ({qtp89387388-352} VFSNotebookRepo.java[save]:196) - Saving note:2FNR8STJW
 INFO [2020-10-28 00:20:59,724] ({qtp89387388-670} VFSNotebookRepo.java[save]:196) - Saving note:2FNR8STJW
 INFO [2020-10-28 00:21:17,393] ({qtp89387388-352} VFSNotebookRepo.java[save]:196) - Saving note:2FNR8STJW
 INFO [2020-10-28 00:21:43,984] ({qtp89387388-352} VFSNotebookRepo.java[save]:196) - Saving note:2FNR8STJW
 INFO [2020-10-28 00:21:44,096] ({qtp89387388-675} VFSNotebookRepo.java[save]:196) - Saving note:2FNR8STJW
 INFO [2020-10-28 00:21:44,101] ({pool-2-thread-9} SchedulerFactory.java[jobStarted]:114) - Job 20191124-133441_1910745321 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 00:21:44,102] ({pool-2-thread-9} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20191124-133441_1910745321, interpreter: pyspark, note_id: 2FNR8STJW, user: anonymous]
 WARN [2020-10-28 00:21:44,146] ({pool-2-thread-9} NotebookServer.java[afterStatusChange]:2316) - Job 20191124-133441_1910745321 is finished, status: ERROR, exception: null, result: %text Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/... /diplodatos_bigdata/ds/flights.csv
	at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)
	at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)
	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)
	at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:199)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:250)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:250)
	at org.apache.spark.api.python.PythonRDD.getPartitions(PythonRDD.scala:53)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:250)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:935)
	at org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:467)
	at org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)
	at sun.reflect.GeneratedMethodAccessor69.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:280)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:745)

(<class 'py4j.protocol.Py4JJavaError'>, Py4JJavaError('An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n', JavaObject id=o696), <traceback object at 0x7f84bb9d1b08>)
 INFO [2020-10-28 00:21:44,186] ({pool-2-thread-9} VFSNotebookRepo.java[save]:196) - Saving note:2FNR8STJW
 INFO [2020-10-28 00:21:44,190] ({pool-2-thread-9} SchedulerFactory.java[jobFinished]:120) - Job 20191124-133441_1910745321 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 00:22:02,318] ({qtp89387388-386} VFSNotebookRepo.java[save]:196) - Saving note:2FNR8STJW
 INFO [2020-10-28 00:22:04,524] ({qtp89387388-386} VFSNotebookRepo.java[save]:196) - Saving note:2FNR8STJW
 INFO [2020-10-28 00:22:04,528] ({qtp89387388-386} NotebookServer.java[broadcastNewParagraph]:688) - Broadcasting paragraph on run call instead of note.
 INFO [2020-10-28 00:22:15,801] ({qtp89387388-386} VFSNotebookRepo.java[save]:196) - Saving note:2FNR8STJW
 INFO [2020-10-28 00:22:15,909] ({qtp89387388-352} VFSNotebookRepo.java[save]:196) - Saving note:2FNR8STJW
 INFO [2020-10-28 00:22:15,914] ({pool-2-thread-17} SchedulerFactory.java[jobStarted]:114) - Job 20201028-002204_1410344323 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-sh:shared_process-shared_session
 INFO [2020-10-28 00:22:15,915] ({pool-2-thread-17} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20201028-002204_1410344323, interpreter: sh, note_id: 2FNR8STJW, user: anonymous]
 INFO [2020-10-28 00:22:15,946] ({pool-2-thread-17} NotebookServer.java[afterStatusChange]:2314) - Job 20201028-002204_1410344323 is finished successfully, status: FINISHED
 INFO [2020-10-28 00:22:15,987] ({pool-2-thread-17} VFSNotebookRepo.java[save]:196) - Saving note:2FNR8STJW
 INFO [2020-10-28 00:22:15,991] ({pool-2-thread-17} SchedulerFactory.java[jobFinished]:120) - Job 20201028-002204_1410344323 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-sh:shared_process-shared_session
 INFO [2020-10-28 00:22:28,402] ({qtp89387388-681} VFSNotebookRepo.java[save]:196) - Saving note:2FNR8STJW
 INFO [2020-10-28 00:22:28,487] ({qtp89387388-386} VFSNotebookRepo.java[save]:196) - Saving note:2FNR8STJW
 INFO [2020-10-28 00:22:28,493] ({pool-2-thread-2} SchedulerFactory.java[jobStarted]:114) - Job 20191124-133441_1910745321 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 00:22:28,494] ({pool-2-thread-2} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20191124-133441_1910745321, interpreter: pyspark, note_id: 2FNR8STJW, user: anonymous]
 WARN [2020-10-28 00:22:28,538] ({pool-2-thread-2} NotebookServer.java[afterStatusChange]:2316) - Job 20191124-133441_1910745321 is finished, status: ERROR, exception: null, result: %text Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/users/ssulca/diplodatos_bigdata/diplodatos_bigdata/ds/flights.csv
	at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)
	at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)
	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)
	at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:199)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:250)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:250)
	at org.apache.spark.api.python.PythonRDD.getPartitions(PythonRDD.scala:53)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:250)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:935)
	at org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:467)
	at org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)
	at sun.reflect.GeneratedMethodAccessor69.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:280)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:745)

(<class 'py4j.protocol.Py4JJavaError'>, Py4JJavaError('An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n', JavaObject id=o745), <traceback object at 0x7f84bb9df4c8>)
 INFO [2020-10-28 00:22:28,582] ({pool-2-thread-2} VFSNotebookRepo.java[save]:196) - Saving note:2FNR8STJW
 INFO [2020-10-28 00:22:28,586] ({pool-2-thread-2} SchedulerFactory.java[jobFinished]:120) - Job 20191124-133441_1910745321 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 00:22:52,595] ({qtp89387388-388} VFSNotebookRepo.java[save]:196) - Saving note:2FNR8STJW
 INFO [2020-10-28 00:22:52,688] ({qtp89387388-681} VFSNotebookRepo.java[save]:196) - Saving note:2FNR8STJW
 INFO [2020-10-28 00:22:52,693] ({pool-2-thread-18} SchedulerFactory.java[jobStarted]:114) - Job 20191124-133441_1910745321 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 00:22:52,693] ({pool-2-thread-18} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20191124-133441_1910745321, interpreter: pyspark, note_id: 2FNR8STJW, user: anonymous]
 INFO [2020-10-28 00:22:53,522] ({pool-2-thread-18} NotebookServer.java[afterStatusChange]:2314) - Job 20191124-133441_1910745321 is finished successfully, status: FINISHED
 INFO [2020-10-28 00:22:53,563] ({pool-2-thread-18} VFSNotebookRepo.java[save]:196) - Saving note:2FNR8STJW
 INFO [2020-10-28 00:22:53,567] ({pool-2-thread-18} SchedulerFactory.java[jobFinished]:120) - Job 20191124-133441_1910745321 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 00:23:31,947] ({qtp89387388-681} VFSNotebookRepo.java[save]:196) - Saving note:2FNR8STJW
 INFO [2020-10-28 00:23:32,063] ({qtp89387388-703} VFSNotebookRepo.java[save]:196) - Saving note:2FNR8STJW
 INFO [2020-10-28 00:23:32,068] ({pool-2-thread-10} SchedulerFactory.java[jobStarted]:114) - Job 20191124-133441_1910745321 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 00:23:32,068] ({pool-2-thread-10} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20191124-133441_1910745321, interpreter: pyspark, note_id: 2FNR8STJW, user: anonymous]
 INFO [2020-10-28 00:23:32,871] ({pool-2-thread-10} NotebookServer.java[afterStatusChange]:2314) - Job 20191124-133441_1910745321 is finished successfully, status: FINISHED
 INFO [2020-10-28 00:23:32,912] ({pool-2-thread-10} VFSNotebookRepo.java[save]:196) - Saving note:2FNR8STJW
 INFO [2020-10-28 00:23:32,916] ({pool-2-thread-10} SchedulerFactory.java[jobFinished]:120) - Job 20191124-133441_1910745321 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 00:23:51,508] ({qtp89387388-386} NotebookServer.java[onClose]:372) - Closed connection to 127.0.0.1 : 40298. (1001) null
 INFO [2020-10-28 00:23:57,562] ({qtp89387388-703} NotebookServer.java[onOpen]:151) - New connection from 127.0.0.1 : 40910
 INFO [2020-10-28 00:23:57,644] ({qtp89387388-386} NotebookServer.java[sendNote]:828) - New operation from 127.0.0.1 : 40910 : anonymous : GET_NOTE : 2FNR8STJW
 WARN [2020-10-28 00:23:57,680] ({qtp89387388-703} GitNotebookRepo.java[revisionHistory]:158) - No Head found for 2FNR8STJW, No HEAD exists and no explicit starting revision was specified
 WARN [2020-10-28 00:23:57,720] ({qtp89387388-386} InterpreterSettingManager.java[compare]:886) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-10-28 00:23:57,720] ({qtp89387388-386} InterpreterSettingManager.java[compare]:886) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-10-28 00:23:57,720] ({qtp89387388-386} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-10-28 00:23:57,721] ({qtp89387388-386} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-10-28 00:23:57,721] ({qtp89387388-386} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-10-28 00:23:57,721] ({qtp89387388-386} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 INFO [2020-10-28 00:24:02,865] ({qtp89387388-703} NotebookServer.java[sendNote]:828) - New operation from 127.0.0.1 : 40910 : anonymous : GET_NOTE : 2FP1YEJHN
 WARN [2020-10-28 00:24:02,875] ({qtp89387388-703} GitNotebookRepo.java[revisionHistory]:158) - No Head found for 2FP1YEJHN, No HEAD exists and no explicit starting revision was specified
 WARN [2020-10-28 00:24:02,936] ({qtp89387388-386} InterpreterSettingManager.java[compare]:886) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-10-28 00:24:02,937] ({qtp89387388-386} InterpreterSettingManager.java[compare]:886) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-10-28 00:24:02,937] ({qtp89387388-386} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-10-28 00:24:02,937] ({qtp89387388-386} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-10-28 00:24:02,937] ({qtp89387388-386} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-10-28 00:24:02,937] ({qtp89387388-386} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 INFO [2020-10-28 00:24:15,029] ({qtp89387388-781} NotebookServer.java[onClose]:372) - Closed connection to 127.0.0.1 : 40910. (1006) WebSocket Read EOF
 INFO [2020-10-28 00:24:23,382] ({Thread-39} ZeppelinServer.java[run]:253) - Shutting down Zeppelin Server ... 
 INFO [2020-10-28 00:24:23,392] ({Thread-39} AbstractConnector.java[doStop]:341) - Stopped ServerConnector@feb98ef{HTTP/1.1,[http/1.1]}{127.0.0.1:9322}
 INFO [2020-10-28 00:24:23,392] ({Thread-39} HouseKeeper.java[stopScavenging]:167) - node0 Stopped scavenging
 INFO [2020-10-28 00:24:23,706] ({Exec Default Executor} RemoteInterpreterManagedProcess.java[onProcessFailed]:250) - Interpreter process failed {}
org.apache.commons.exec.ExecuteException: Process exited with an error: 130 (Exit value: 130)
	at org.apache.commons.exec.DefaultExecutor.executeInternal(DefaultExecutor.java:404)
	at org.apache.commons.exec.DefaultExecutor.access$200(DefaultExecutor.java:48)
	at org.apache.commons.exec.DefaultExecutor$1.run(DefaultExecutor.java:200)
	at java.lang.Thread.run(Thread.java:745)
 INFO [2020-10-28 00:24:23,772] ({Exec Default Executor} RemoteInterpreterManagedProcess.java[onProcessFailed]:250) - Interpreter process failed {}
org.apache.commons.exec.ExecuteException: Process exited with an error: 130 (Exit value: 130)
	at org.apache.commons.exec.DefaultExecutor.executeInternal(DefaultExecutor.java:404)
	at org.apache.commons.exec.DefaultExecutor.access$200(DefaultExecutor.java:48)
	at org.apache.commons.exec.DefaultExecutor$1.run(DefaultExecutor.java:200)
	at java.lang.Thread.run(Thread.java:745)
 INFO [2020-10-28 00:24:25,358] ({Thread-39} ContextHandler.java[doStop]:1045) - Stopped o.e.j.w.WebAppContext@15761df8{zeppelin-web,/,null,UNAVAILABLE}{/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/zeppelin-web-0.8.2.war}
 INFO [2020-10-28 00:24:25,360] ({Thread-677} InterpreterSetting.java[close]:483) - Close InterpreterSetting: ignite
 INFO [2020-10-28 00:24:25,361] ({Thread-678} InterpreterSetting.java[close]:483) - Close InterpreterSetting: ignite
 INFO [2020-10-28 00:24:25,361] ({Thread-681} InterpreterSetting.java[close]:483) - Close InterpreterSetting: sap
 INFO [2020-10-28 00:24:25,361] ({Thread-680} InterpreterSetting.java[close]:483) - Close InterpreterSetting: python
 INFO [2020-10-28 00:24:25,361] ({Thread-679} InterpreterSetting.java[close]:483) - Close InterpreterSetting: python
 INFO [2020-10-28 00:24:25,362] ({Thread-688} InterpreterSetting.java[close]:483) - Close InterpreterSetting: flink
 INFO [2020-10-28 00:24:25,362] ({Thread-687} InterpreterSetting.java[close]:483) - Close InterpreterSetting: lens
 INFO [2020-10-28 00:24:25,363] ({Thread-694} InterpreterSetting.java[close]:483) - Close InterpreterSetting: livy
 INFO [2020-10-28 00:24:25,362] ({Thread-685} InterpreterSetting.java[close]:483) - Close InterpreterSetting: pig
 INFO [2020-10-28 00:24:25,361] ({Thread-684} InterpreterSetting.java[close]:483) - Close InterpreterSetting: lens
 INFO [2020-10-28 00:24:25,364] ({Thread-702} InterpreterSetting.java[close]:483) - Close InterpreterSetting: spark
 INFO [2020-10-28 00:24:25,365] ({Thread-706} InterpreterSetting.java[close]:483) - Close InterpreterSetting: md
 INFO [2020-10-28 00:24:25,361] ({Thread-686} InterpreterSetting.java[close]:483) - Close InterpreterSetting: jdbc
 INFO [2020-10-28 00:24:25,361] ({Thread-682} InterpreterSetting.java[close]:483) - Close InterpreterSetting: sap
 INFO [2020-10-28 00:24:25,361] ({Thread-683} InterpreterSetting.java[close]:483) - Close InterpreterSetting: jdbc
 INFO [2020-10-28 00:24:25,366] ({Thread-714} InterpreterSetting.java[close]:483) - Close InterpreterSetting: md
 INFO [2020-10-28 00:24:25,366] ({Thread-713} InterpreterSetting.java[close]:483) - Close InterpreterSetting: sh
 INFO [2020-10-28 00:24:25,366] ({Thread-711} InterpreterSetting.java[close]:483) - Close InterpreterSetting: kylin
 INFO [2020-10-28 00:24:25,367] ({Thread-718} InterpreterSetting.java[close]:483) - Close InterpreterSetting: kylin
 INFO [2020-10-28 00:24:25,366] ({Thread-712} InterpreterSetting.java[close]:483) - Close InterpreterSetting: cassandra
 INFO [2020-10-28 00:24:25,366] ({Thread-710} InterpreterSetting.java[close]:483) - Close InterpreterSetting: spark
 INFO [2020-10-28 00:24:25,365] ({Thread-709} InterpreterSetting.java[close]:483) - Close InterpreterSetting: hbase
 INFO [2020-10-28 00:24:25,365] ({Thread-705} InterpreterSetting.java[close]:483) - Close InterpreterSetting: elasticsearch
 INFO [2020-10-28 00:24:25,365] ({Thread-708} InterpreterSetting.java[close]:483) - Close InterpreterSetting: bigquery
 INFO [2020-10-28 00:24:25,365] ({Thread-706} ManagedInterpreterGroup.java[close]:89) - Close InterpreterGroup: md:shared_process
 INFO [2020-10-28 00:24:25,365] ({Thread-707} InterpreterSetting.java[close]:483) - Close InterpreterSetting: alluxio
 INFO [2020-10-28 00:24:25,365] ({Thread-702} ManagedInterpreterGroup.java[close]:89) - Close InterpreterGroup: spark:shared_process
 INFO [2020-10-28 00:24:25,365] ({Thread-700} InterpreterSetting.java[close]:483) - Close InterpreterSetting: file
 INFO [2020-10-28 00:24:25,364] ({Thread-704} InterpreterSetting.java[close]:483) - Close InterpreterSetting: sh
 INFO [2020-10-28 00:24:25,364] ({Thread-703} InterpreterSetting.java[close]:483) - Close InterpreterSetting: cassandra
 INFO [2020-10-28 00:24:25,364] ({Thread-701} InterpreterSetting.java[close]:483) - Close InterpreterSetting: elasticsearch
 INFO [2020-10-28 00:24:25,363] ({Thread-698} InterpreterSetting.java[close]:483) - Close InterpreterSetting: neo4j
 INFO [2020-10-28 00:24:25,363] ({Thread-699} InterpreterSetting.java[close]:483) - Close InterpreterSetting: file
 INFO [2020-10-28 00:24:25,363] ({Thread-697} InterpreterSetting.java[close]:483) - Close InterpreterSetting: neo4j
 INFO [2020-10-28 00:24:25,363] ({Thread-696} InterpreterSetting.java[close]:483) - Close InterpreterSetting: groovy
 INFO [2020-10-28 00:24:25,363] ({Thread-695} InterpreterSetting.java[close]:483) - Close InterpreterSetting: groovy
 INFO [2020-10-28 00:24:25,363] ({Thread-693} InterpreterSetting.java[close]:483) - Close InterpreterSetting: livy
 INFO [2020-10-28 00:24:25,363] ({Thread-689} InterpreterSetting.java[close]:483) - Close InterpreterSetting: angular
 INFO [2020-10-28 00:24:25,363] ({Thread-692} InterpreterSetting.java[close]:483) - Close InterpreterSetting: angular
 INFO [2020-10-28 00:24:25,362] ({Thread-691} InterpreterSetting.java[close]:483) - Close InterpreterSetting: flink
 INFO [2020-10-28 00:24:25,362] ({Thread-690} InterpreterSetting.java[close]:483) - Close InterpreterSetting: pig
 INFO [2020-10-28 00:24:25,370] ({Thread-702} ManagedInterpreterGroup.java[close]:100) - Close Session: shared_session for interpreter setting: spark
 INFO [2020-10-28 00:24:25,370] ({Thread-706} ManagedInterpreterGroup.java[close]:100) - Close Session: shared_session for interpreter setting: md
 INFO [2020-10-28 00:24:25,367] ({Thread-717} InterpreterSetting.java[close]:483) - Close InterpreterSetting: hbase
 INFO [2020-10-28 00:24:25,367] ({Thread-713} ManagedInterpreterGroup.java[close]:89) - Close InterpreterGroup: sh:shared_process
 INFO [2020-10-28 00:24:25,373] ({Thread-713} ManagedInterpreterGroup.java[close]:100) - Close Session: shared_session for interpreter setting: sh
 INFO [2020-10-28 00:24:25,367] ({Thread-716} InterpreterSetting.java[close]:483) - Close InterpreterSetting: bigquery
 INFO [2020-10-28 00:24:25,366] ({Thread-715} InterpreterSetting.java[close]:483) - Close InterpreterSetting: alluxio
 WARN [2020-10-28 00:24:25,372] ({Thread-706} RemoteInterpreter.java[close]:199) - close is called when RemoterInterpreter is not opened for org.apache.zeppelin.markdown.Markdown
 WARN [2020-10-28 00:24:25,372] ({Thread-702} RemoteInterpreter.java[close]:199) - close is called when RemoterInterpreter is not opened for org.apache.zeppelin.spark.SparkInterpreter
 WARN [2020-10-28 00:24:25,375] ({Thread-702} RemoteInterpreter.java[close]:199) - close is called when RemoterInterpreter is not opened for org.apache.zeppelin.spark.SparkSqlInterpreter
 INFO [2020-10-28 00:24:25,375] ({Thread-706} ManagedInterpreterGroup.java[close]:105) - Remove this InterpreterGroup: md:shared_process as all the sessions are closed
 INFO [2020-10-28 00:24:25,375] ({Thread-714} ManagedInterpreterGroup.java[close]:89) - Close InterpreterGroup: md:shared_process
 WARN [2020-10-28 00:24:25,376] ({Thread-702} RemoteInterpreter.java[close]:199) - close is called when RemoterInterpreter is not opened for org.apache.zeppelin.spark.DepInterpreter
 INFO [2020-10-28 00:24:25,380] ({Thread-710} ManagedInterpreterGroup.java[close]:89) - Close InterpreterGroup: spark:shared_process
 INFO [2020-10-28 00:24:25,380] ({Thread-704} ManagedInterpreterGroup.java[close]:89) - Close InterpreterGroup: sh:shared_process
 INFO [2020-10-28 00:24:25,381] ({Thread-39} NotebookRepoSync.java[close]:428) - Closing all notebook storages
 INFO [2020-10-28 00:24:28,386] ({Thread-39} ZeppelinServer.java[run]:264) - Bye
 INFO [2020-10-28 20:32:34,219] ({main} ZeppelinConfiguration.java[create]:121) - Load configuration from file:/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/zeppelin-site.xml
 WARN [2020-10-28 20:32:34,305] ({main} ZeppelinConfiguration.java[create]:124) - Failed to load configuration from file:/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/zeppelin-site.xml proceeding with a default
org.apache.commons.configuration.ConfigurationException: Error parsing file:/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/zeppelin-site.xml
	at org.apache.commons.configuration.XMLConfiguration.load(XMLConfiguration.java:950)
	at org.apache.commons.configuration.XMLConfiguration.load(XMLConfiguration.java:908)
	at org.apache.commons.configuration.XMLConfiguration$XMLFileConfigurationDelegate.load(XMLConfiguration.java:1583)
	at org.apache.commons.configuration.AbstractFileConfiguration.load(AbstractFileConfiguration.java:324)
	at org.apache.commons.configuration.AbstractHierarchicalFileConfiguration.load(AbstractHierarchicalFileConfiguration.java:199)
	at org.apache.zeppelin.conf.ZeppelinConfiguration.<init>(ZeppelinConfiguration.java:52)
	at org.apache.zeppelin.conf.ZeppelinConfiguration.create(ZeppelinConfiguration.java:122)
	at org.apache.zeppelin.server.ZeppelinServer.main(ZeppelinServer.java:219)
Caused by: org.xml.sax.SAXParseException; systemId: file:/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/zeppelin-site.xml; lineNumber: 1; columnNumber: 1; Premature end of file.
	at com.sun.org.apache.xerces.internal.parsers.DOMParser.parse(DOMParser.java:257)
	at com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderImpl.parse(DocumentBuilderImpl.java:339)
	at org.apache.commons.configuration.XMLConfiguration.load(XMLConfiguration.java:942)
	... 7 more
 INFO [2020-10-28 20:32:34,334] ({main} ZeppelinConfiguration.java[create]:129) - Server Host: 127.0.0.1
 INFO [2020-10-28 20:32:34,335] ({main} ZeppelinConfiguration.java[create]:131) - Server Port: 9322
 INFO [2020-10-28 20:32:34,335] ({main} ZeppelinConfiguration.java[create]:135) - Context Path: /
 INFO [2020-10-28 20:32:34,336] ({main} ZeppelinConfiguration.java[create]:136) - Zeppelin Version: 0.8.2
 INFO [2020-10-28 20:32:34,364] ({main} Log.java[initialized]:193) - Logging initialized @814ms to org.eclipse.jetty.util.log.Slf4jLog
 WARN [2020-10-28 20:32:34,495] ({main} ServerConnector.java[setSoLingerTime]:458) - Ignoring deprecated socket close linger time
 INFO [2020-10-28 20:32:34,555] ({main} ZeppelinServer.java[setupWebAppContext]:413) - ZeppelinServer Webapp path: /users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/webapps
 INFO [2020-10-28 20:32:34,648] ({main} ZeppelinServer.java[main]:239) - Starting zeppelin server
 INFO [2020-10-28 20:32:34,649] ({main} Server.java[doStart]:370) - jetty-9.4.14.v20181114; built: 2018-11-14T21:20:31.478Z; git: c4550056e785fb5665914545889f21dc136ad9e6; jvm 1.8.0_152-release-1056-b12
 INFO [2020-10-28 20:32:40,242] ({main} StandardDescriptorProcessor.java[visitServlet]:283) - NO JSP Support for /, did not find org.eclipse.jetty.jsp.JettyJspServlet
 INFO [2020-10-28 20:32:40,265] ({main} DefaultSessionIdManager.java[doStart]:365) - DefaultSessionIdManager workerName=node0
 INFO [2020-10-28 20:32:40,265] ({main} DefaultSessionIdManager.java[doStart]:370) - No SessionScavenger set, using defaults
 INFO [2020-10-28 20:32:40,268] ({main} HouseKeeper.java[startScavenging]:149) - node0 Scavenging every 660000ms
 INFO [2020-10-28 20:32:40,682] ({main} SchedulerFactory.java[<init>]:59) - Scheduler Thread Pool Size: 100
 WARN [2020-10-28 20:32:40,711] ({main} ZeppelinConfiguration.java[getConfigFSDir]:527) - zeppelin.config.fs.dir is not specified, fall back to local conf directory zeppelin.conf.dir
 WARN [2020-10-28 20:32:40,711] ({main} ZeppelinConfiguration.java[getConfigFSDir]:527) - zeppelin.config.fs.dir is not specified, fall back to local conf directory zeppelin.conf.dir
 INFO [2020-10-28 20:32:40,840] ({main} InterpreterSettingManager.java[<init>]:165) - Using RecoveryStorage: org.apache.zeppelin.interpreter.recovery.NullRecoveryStorage
 INFO [2020-10-28 20:32:40,842] ({main} InterpreterSettingManager.java[<init>]:169) - Using LifecycleManager: org.apache.zeppelin.interpreter.lifecycle.NullLifecycleManager
 INFO [2020-10-28 20:32:40,880] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: cassandra
 WARN [2020-10-28 20:32:40,887] ({main} InterpreterSettingManager.java[init]:331) - No interpreter-setting.json found in /users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/${interpreter.name}
 INFO [2020-10-28 20:32:40,899] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: angular
 INFO [2020-10-28 20:32:40,904] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: md
 INFO [2020-10-28 20:32:40,909] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: alluxio
 INFO [2020-10-28 20:32:40,913] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: groovy
 INFO [2020-10-28 20:32:40,917] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: pig
 INFO [2020-10-28 20:32:40,922] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: file
 INFO [2020-10-28 20:32:40,927] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: elasticsearch
 INFO [2020-10-28 20:32:40,931] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: lens
 INFO [2020-10-28 20:32:40,935] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: kylin
 INFO [2020-10-28 20:32:40,940] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: sap
 INFO [2020-10-28 20:32:40,944] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: ignite
 INFO [2020-10-28 20:32:40,959] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: neo4j
 INFO [2020-10-28 20:32:40,964] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: python
 INFO [2020-10-28 20:32:40,968] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: bigquery
 WARN [2020-10-28 20:32:41,274] ({main} InterpreterSettingManager.java[init]:331) - No interpreter-setting.json found in /users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/scio
 INFO [2020-10-28 20:32:41,283] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: spark
 INFO [2020-10-28 20:32:41,288] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: livy
 WARN [2020-10-28 20:32:41,293] ({main} InterpreterSettingManager.java[init]:331) - No interpreter-setting.json found in /users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/lib
 INFO [2020-10-28 20:32:41,297] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: sh
 INFO [2020-10-28 20:32:41,300] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: flink
 INFO [2020-10-28 20:32:41,304] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: hbase
 INFO [2020-10-28 20:32:41,307] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: jdbc
 INFO [2020-10-28 20:32:41,308] ({main} LocalConfigStorage.java[loadInterpreterSettings]:63) - Load Interpreter Setting from file: /users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/conf/interpreter.json
 INFO [2020-10-28 20:32:41,389] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting ignite from interpreter.json
 INFO [2020-10-28 20:32:41,389] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting python from interpreter.json
 INFO [2020-10-28 20:32:41,390] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting sap from interpreter.json
 INFO [2020-10-28 20:32:41,392] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting jdbc from interpreter.json
 INFO [2020-10-28 20:32:41,393] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting lens from interpreter.json
 INFO [2020-10-28 20:32:41,393] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting pig from interpreter.json
 INFO [2020-10-28 20:32:41,394] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting flink from interpreter.json
 INFO [2020-10-28 20:32:41,394] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting angular from interpreter.json
 INFO [2020-10-28 20:32:41,396] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting livy from interpreter.json
 INFO [2020-10-28 20:32:41,397] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting groovy from interpreter.json
 INFO [2020-10-28 20:32:41,397] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting neo4j from interpreter.json
 INFO [2020-10-28 20:32:41,398] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting file from interpreter.json
 INFO [2020-10-28 20:32:41,398] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting elasticsearch from interpreter.json
 INFO [2020-10-28 20:32:41,400] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting spark from interpreter.json
 INFO [2020-10-28 20:32:41,401] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting cassandra from interpreter.json
 INFO [2020-10-28 20:32:41,402] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting sh from interpreter.json
 INFO [2020-10-28 20:32:41,402] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting md from interpreter.json
 INFO [2020-10-28 20:32:41,402] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting alluxio from interpreter.json
 INFO [2020-10-28 20:32:41,403] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting bigquery from interpreter.json
 INFO [2020-10-28 20:32:41,403] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting hbase from interpreter.json
 INFO [2020-10-28 20:32:41,404] ({main} InterpreterSettingManager.java[loadFromFile]:279) - Create Interpreter Setting kylin from interpreter.json
 INFO [2020-10-28 20:32:41,412] ({main} LocalConfigStorage.java[save]:53) - Save Interpreter Setting to /users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/conf/interpreter.json
 INFO [2020-10-28 20:32:41,613] ({main} VfsLog.java[info]:138) - Using "/tmp/vfs_cache" as temporary files store.
 INFO [2020-10-28 20:32:41,669] ({main} GitNotebookRepo.java[<init>]:64) - Opening a git repo at '/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/notebook'
 INFO [2020-10-28 20:32:41,745] ({main} NotebookRepoSync.java[<init>]:77) - Instantiate NotebookRepo: org.apache.zeppelin.notebook.repo.GitNotebookRepo
 WARN [2020-10-28 20:32:41,879] ({main} NotebookAuthorization.java[getInstance]:86) - Notebook authorization module was called without initialization, initializing with default configuration
 INFO [2020-10-28 20:32:41,880] ({main} LocalConfigStorage.java[loadNotebookAuthorization]:80) - Load notebook authorization from file: /users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/conf/notebook-authorization.json
 INFO [2020-10-28 20:32:41,883] ({main} Credentials.java[loadFromFile]:121) - /users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/conf/credentials.json
 INFO [2020-10-28 20:32:41,930] ({main} StdSchedulerFactory.java[instantiate]:1184) - Using default implementation for ThreadExecutor
 INFO [2020-10-28 20:32:41,933] ({main} SimpleThreadPool.java[initialize]:268) - Job execution threads will use class loader of thread: main
 INFO [2020-10-28 20:32:41,946] ({main} SchedulerSignalerImpl.java[<init>]:61) - Initialized Scheduler Signaller of type: class org.quartz.core.SchedulerSignalerImpl
 INFO [2020-10-28 20:32:41,947] ({main} QuartzScheduler.java[<init>]:240) - Quartz Scheduler v.2.2.1 created.
 INFO [2020-10-28 20:32:41,948] ({main} RAMJobStore.java[initialize]:155) - RAMJobStore initialized.
 INFO [2020-10-28 20:32:41,949] ({main} QuartzScheduler.java[initialize]:305) - Scheduler meta-data: Quartz Scheduler (v2.2.1) 'DefaultQuartzScheduler' with instanceId 'NON_CLUSTERED'
  Scheduler class: 'org.quartz.core.QuartzScheduler' - running locally.
  NOT STARTED.
  Currently in standby mode.
  Number of jobs executed: 0
  Using thread pool 'org.quartz.simpl.SimpleThreadPool' - with 10 threads.
  Using job-store 'org.quartz.simpl.RAMJobStore' - which does not support persistence. and is not clustered.

 INFO [2020-10-28 20:32:41,949] ({main} StdSchedulerFactory.java[instantiate]:1339) - Quartz scheduler 'DefaultQuartzScheduler' initialized from default resource file in Quartz package: 'quartz.properties'
 INFO [2020-10-28 20:32:41,949] ({main} StdSchedulerFactory.java[instantiate]:1343) - Quartz scheduler version: 2.2.1
 INFO [2020-10-28 20:32:41,950] ({main} QuartzScheduler.java[start]:575) - Scheduler DefaultQuartzScheduler_$_NON_CLUSTERED started.
 INFO [2020-10-28 20:32:42,224] ({main} FolderView.java[createFolder]:107) - Create folder Zeppelin Tutorial
 INFO [2020-10-28 20:32:42,224] ({main} FolderView.java[createFolder]:107) - Create folder /
 INFO [2020-10-28 20:32:42,224] ({main} Folder.java[setParent]:169) - Set parent of / to /
 INFO [2020-10-28 20:32:42,224] ({main} Folder.java[setParent]:169) - Set parent of Zeppelin Tutorial to /
 INFO [2020-10-28 20:32:42,225] ({main} Folder.java[addNote]:185) - Add note 2A94M5J1Z to folder Zeppelin Tutorial
 WARN [2020-10-28 20:32:42,225] ({main} Notebook.java[refreshCron]:981) - execution of the cron job is skipped cron is not enabled from Zeppelin server
 INFO [2020-10-28 20:32:42,242] ({main} Folder.java[addNote]:185) - Add note 2BWJFTXKJ to folder Zeppelin Tutorial
 WARN [2020-10-28 20:32:42,243] ({main} Notebook.java[refreshCron]:981) - execution of the cron job is skipped cron is not enabled from Zeppelin server
 INFO [2020-10-28 20:32:42,259] ({main} FolderView.java[createFolder]:107) - Create folder Diplodatos
 INFO [2020-10-28 20:32:42,259] ({main} Folder.java[setParent]:169) - Set parent of Diplodatos to /
 INFO [2020-10-28 20:32:42,260] ({main} Folder.java[addNote]:185) - Add note 2FMXBM6HK to folder Diplodatos
 WARN [2020-10-28 20:32:42,260] ({main} Notebook.java[refreshCron]:981) - execution of the cron job is skipped cron is not enabled from Zeppelin server
 INFO [2020-10-28 20:32:42,271] ({main} Folder.java[addNote]:185) - Add note 2FQA9JFA8 to folder Diplodatos
 WARN [2020-10-28 20:32:42,271] ({main} Notebook.java[refreshCron]:981) - execution of the cron job is skipped cron is not enabled from Zeppelin server
 INFO [2020-10-28 20:32:42,282] ({main} Folder.java[addNote]:185) - Add note 2FPTJC7P4 to folder Diplodatos
 WARN [2020-10-28 20:32:42,283] ({main} Notebook.java[refreshCron]:981) - execution of the cron job is skipped cron is not enabled from Zeppelin server
 INFO [2020-10-28 20:32:42,289] ({main} Folder.java[addNote]:185) - Add note 2C2AUG798 to folder Zeppelin Tutorial
 WARN [2020-10-28 20:32:42,290] ({main} Notebook.java[refreshCron]:981) - execution of the cron job is skipped cron is not enabled from Zeppelin server
 INFO [2020-10-28 20:32:42,294] ({main} Folder.java[addNote]:185) - Add note 2C57UKYWR to folder Zeppelin Tutorial
 WARN [2020-10-28 20:32:42,294] ({main} Notebook.java[refreshCron]:981) - execution of the cron job is skipped cron is not enabled from Zeppelin server
 INFO [2020-10-28 20:32:42,312] ({main} Folder.java[addNote]:185) - Add note 2FP1YEJHN to folder Diplodatos
 WARN [2020-10-28 20:32:42,313] ({main} Notebook.java[refreshCron]:981) - execution of the cron job is skipped cron is not enabled from Zeppelin server
 INFO [2020-10-28 20:32:42,322] ({main} Folder.java[addNote]:185) - Add note 2BYEZ5EVK to folder Zeppelin Tutorial
 WARN [2020-10-28 20:32:42,322] ({main} Notebook.java[refreshCron]:981) - execution of the cron job is skipped cron is not enabled from Zeppelin server
 INFO [2020-10-28 20:32:42,340] ({main} Folder.java[addNote]:185) - Add note 2FNR8STJW to folder Diplodatos
 WARN [2020-10-28 20:32:42,341] ({main} Notebook.java[refreshCron]:981) - execution of the cron job is skipped cron is not enabled from Zeppelin server
 INFO [2020-10-28 20:32:42,349] ({main} Folder.java[addNote]:185) - Add note 2C35YU814 to folder Zeppelin Tutorial
 WARN [2020-10-28 20:32:42,349] ({main} Notebook.java[refreshCron]:981) - execution of the cron job is skipped cron is not enabled from Zeppelin server
 INFO [2020-10-28 20:32:42,349] ({main} Notebook.java[<init>]:127) - Notebook indexing started...
 INFO [2020-10-28 20:32:42,583] ({main} LuceneSearch.java[addIndexDocs]:305) - Indexing 11 notebooks took 233ms
 INFO [2020-10-28 20:32:42,584] ({main} Notebook.java[<init>]:129) - Notebook indexing finished: 11 indexed in 0s
 INFO [2020-10-28 20:32:42,585] ({main} Helium.java[loadConf]:103) - Add helium local registry /users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/helium
 INFO [2020-10-28 20:32:42,586] ({main} Helium.java[loadConf]:100) - Add helium online registry https://s3.amazonaws.com/helium-package/helium.json
 WARN [2020-10-28 20:32:42,599] ({main} Helium.java[loadConf]:111) - /users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/conf/helium.json does not exists
 INFO [2020-10-28 20:32:45,284] ({main} ContextHandler.java[doStart]:855) - Started o.e.j.w.WebAppContext@15761df8{zeppelin-web,/,file:///users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/webapps/webapp/,AVAILABLE}{/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/zeppelin-web-0.8.2.war}
 INFO [2020-10-28 20:32:45,304] ({main} AbstractConnector.java[doStart]:292) - Started ServerConnector@f4a3a8d{HTTP/1.1,[http/1.1]}{127.0.0.1:9322}
 INFO [2020-10-28 20:32:45,304] ({main} Server.java[doStart]:407) - Started @11758ms
 INFO [2020-10-28 20:32:45,304] ({main} ZeppelinServer.java[main]:249) - Done, zeppelin server started
 WARN [2020-10-28 20:33:18,479] ({qtp89387388-72} SecurityRestApi.java[ticket]:88) - {"status":"OK","message":"","body":{"principal":"anonymous","ticket":"anonymous","roles":"[]"}}
 INFO [2020-10-28 20:33:18,723] ({qtp89387388-24} NotebookServer.java[onOpen]:151) - New connection from 127.0.0.1 : 43830
 INFO [2020-10-28 20:33:22,185] ({qtp89387388-71} NotebookServer.java[sendNote]:828) - New operation from 127.0.0.1 : 43830 : anonymous : GET_NOTE : 2FP1YEJHN
 WARN [2020-10-28 20:33:22,301] ({qtp89387388-71} GitNotebookRepo.java[revisionHistory]:158) - No Head found for 2FP1YEJHN, No HEAD exists and no explicit starting revision was specified
 WARN [2020-10-28 20:33:22,304] ({qtp89387388-71} InterpreterSettingManager.java[compare]:886) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-10-28 20:33:22,305] ({qtp89387388-71} InterpreterSettingManager.java[compare]:886) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-10-28 20:33:22,305] ({qtp89387388-71} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-10-28 20:33:22,305] ({qtp89387388-71} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-10-28 20:33:22,305] ({qtp89387388-71} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-10-28 20:33:22,305] ({qtp89387388-71} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 INFO [2020-10-28 20:33:22,558] ({qtp89387388-76} InterpreterSetting.java[getOrCreateInterpreterGroup]:419) - Create InterpreterGroup with groupId: spark:shared_process for user: anonymous and note: 2FP1YEJHN
 INFO [2020-10-28 20:33:22,558] ({qtp89387388-73} InterpreterSetting.java[getOrCreateInterpreterGroup]:419) - Create InterpreterGroup with groupId: md:shared_process for user: anonymous and note: 2FP1YEJHN
 INFO [2020-10-28 20:33:22,563] ({qtp89387388-76} InterpreterSetting.java[createInterpreters]:689) - Interpreter org.apache.zeppelin.spark.SparkInterpreter created for user: anonymous, sessionId: shared_session
 INFO [2020-10-28 20:33:22,563] ({qtp89387388-73} InterpreterSetting.java[createInterpreters]:689) - Interpreter org.apache.zeppelin.markdown.Markdown created for user: anonymous, sessionId: shared_session
 INFO [2020-10-28 20:33:22,563] ({qtp89387388-76} InterpreterSetting.java[createInterpreters]:689) - Interpreter org.apache.zeppelin.spark.SparkSqlInterpreter created for user: anonymous, sessionId: shared_session
 INFO [2020-10-28 20:33:22,564] ({qtp89387388-76} InterpreterSetting.java[createInterpreters]:689) - Interpreter org.apache.zeppelin.spark.DepInterpreter created for user: anonymous, sessionId: shared_session
 INFO [2020-10-28 20:33:22,564] ({qtp89387388-76} InterpreterSetting.java[createInterpreters]:689) - Interpreter org.apache.zeppelin.spark.PySparkInterpreter created for user: anonymous, sessionId: shared_session
 INFO [2020-10-28 20:33:22,564] ({qtp89387388-76} InterpreterSetting.java[createInterpreters]:689) - Interpreter org.apache.zeppelin.spark.IPySparkInterpreter created for user: anonymous, sessionId: shared_session
 INFO [2020-10-28 20:33:22,564] ({qtp89387388-73} ManagedInterpreterGroup.java[getOrCreateSession]:158) - Create Session: shared_session in InterpreterGroup: md:shared_process for user: anonymous
 INFO [2020-10-28 20:33:22,564] ({qtp89387388-76} InterpreterSetting.java[createInterpreters]:689) - Interpreter org.apache.zeppelin.spark.SparkRInterpreter created for user: anonymous, sessionId: shared_session
 INFO [2020-10-28 20:33:22,564] ({qtp89387388-76} ManagedInterpreterGroup.java[getOrCreateSession]:158) - Create Session: shared_session in InterpreterGroup: spark:shared_process for user: anonymous
 INFO [2020-10-28 20:33:24,660] ({qtp89387388-72} InterpreterSetting.java[getOrCreateInterpreterGroup]:419) - Create InterpreterGroup with groupId: sh:shared_process for user: anonymous and note: 2FP1YEJHN
 INFO [2020-10-28 20:33:24,661] ({qtp89387388-72} InterpreterSetting.java[createInterpreters]:689) - Interpreter org.apache.zeppelin.shell.ShellInterpreter created for user: anonymous, sessionId: shared_session
 INFO [2020-10-28 20:33:24,661] ({qtp89387388-72} ManagedInterpreterGroup.java[getOrCreateSession]:158) - Create Session: shared_session in InterpreterGroup: sh:shared_process for user: anonymous
 INFO [2020-10-28 20:36:19,939] ({qtp89387388-24} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 20:36:19,961] ({pool-2-thread-2} SchedulerFactory.java[jobStarted]:114) - Job 20171019-160931_1102056402 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 20:36:19,963] ({pool-2-thread-2} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20171019-160931_1102056402, interpreter: pyspark, note_id: 2FP1YEJHN, user: anonymous]
 INFO [2020-10-28 20:36:19,963] ({pool-2-thread-2} ManagedInterpreterGroup.java[getOrCreateInterpreterProcess]:61) - Create InterpreterProcess for InterpreterGroup: spark:shared_process
 INFO [2020-10-28 20:36:19,964] ({pool-2-thread-2} ShellScriptLauncher.java[launch]:48) - Launching Interpreter: spark
 WARN [2020-10-28 20:36:19,983] ({pool-2-thread-2} SparkInterpreterLauncher.java[setupPropertiesForSparkR]:172) - sparkr.zip is not found, SparkR may not work.
 INFO [2020-10-28 20:36:19,984] ({pool-2-thread-2} SparkInterpreterLauncher.java[buildEnvFromProperties]:108) - Run Spark under non-secure mode as no keytab and principal is specified
 INFO [2020-10-28 20:36:19,990] ({pool-2-thread-2} RemoteInterpreterManagedProcess.java[start]:115) - Thrift server for callback will start. Port: 45135
 INFO [2020-10-28 20:36:20,005] ({pool-2-thread-2} RemoteInterpreterManagedProcess.java[start]:190) - Run interpreter process [/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/bin/interpreter.sh, -d, /users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark, -c, 200.16.29.165, -p, 45135, -r, :, -l, /users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/local-repo/spark, -g, spark]
 INFO [2020-10-28 20:36:21,501] ({pool-7-thread-1} RemoteInterpreterManagedProcess.java[callback]:123) - RemoteInterpreterServer Registered: CallbackInfo(host:200.16.29.165, port:35343)
 INFO [2020-10-28 20:36:21,581] ({pool-2-thread-2} RemoteInterpreter.java[call]:168) - Create RemoteInterpreter org.apache.zeppelin.spark.SparkInterpreter
 INFO [2020-10-28 20:36:21,740] ({pool-2-thread-2} RemoteInterpreter.java[call]:168) - Create RemoteInterpreter org.apache.zeppelin.spark.SparkSqlInterpreter
 INFO [2020-10-28 20:36:21,744] ({pool-2-thread-2} RemoteInterpreter.java[call]:168) - Create RemoteInterpreter org.apache.zeppelin.spark.DepInterpreter
 INFO [2020-10-28 20:36:21,767] ({pool-2-thread-2} RemoteInterpreter.java[call]:168) - Create RemoteInterpreter org.apache.zeppelin.spark.PySparkInterpreter
 INFO [2020-10-28 20:36:21,783] ({pool-2-thread-2} RemoteInterpreter.java[call]:168) - Create RemoteInterpreter org.apache.zeppelin.spark.IPySparkInterpreter
 INFO [2020-10-28 20:36:21,790] ({pool-2-thread-2} RemoteInterpreter.java[call]:168) - Create RemoteInterpreter org.apache.zeppelin.spark.SparkRInterpreter
 INFO [2020-10-28 20:36:21,793] ({pool-2-thread-2} RemoteInterpreter.java[call]:142) - Open RemoteInterpreter org.apache.zeppelin.spark.PySparkInterpreter
 INFO [2020-10-28 20:36:21,793] ({pool-2-thread-2} RemoteInterpreter.java[pushAngularObjectRegistryToRemote]:436) - Push local angular object registry from ZeppelinServer to remote interpreter group spark:shared_process
 WARN [2020-10-28 20:36:38,371] ({pool-2-thread-2} NotebookServer.java[afterStatusChange]:2316) - Job 20171019-160931_1102056402 is finished, status: ERROR, exception: null, result: %text Fail to execute line 2:                     format="csv", delimiter="\t", header=True, inferSchema=True)
Traceback (most recent call last):
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/sql/utils.py", line 63, in deco
    return f(*a, **kw)
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/protocol.py", line 319, in get_return_value
    format(target_id, ".", name), value)
py4j.protocol.Py4JJavaError: An error occurred while calling o54.load.
: org.apache.spark.sql.AnalysisException: Path does not exist: file:/users/ssulca/diplodatos_bigdata/diplodatos_bigdata/ds/userid-profile.tsv;
	at org.apache.spark.sql.execution.datasources.DataSource$.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:626)
	at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$14.apply(DataSource.scala:350)
	at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$14.apply(DataSource.scala:350)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)
	at scala.collection.immutable.List.flatMap(List.scala:344)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:349)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:178)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:156)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:280)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:745)


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/tmp/zeppelin_pyspark-3213863130067318734.py", line 380, in <module>
    exec(code, _zcUserQueryNameSpace)
  File "<stdin>", line 2, in <module>
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/sql/readwriter.py", line 159, in load
    return self._df(self._jreader.load(path))
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/java_gateway.py", line 1133, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/sql/utils.py", line 69, in deco
    raise AnalysisException(s.split(': ', 1)[1], stackTrace)
pyspark.sql.utils.AnalysisException: 'Path does not exist: file:/users/ssulca/diplodatos_bigdata/diplodatos_bigdata/ds/userid-profile.tsv;'

 INFO [2020-10-28 20:36:38,493] ({pool-2-thread-2} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 20:36:38,502] ({pool-2-thread-2} SchedulerFactory.java[jobFinished]:120) - Job 20171019-160931_1102056402 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 20:37:10,351] ({qtp89387388-24} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 20:37:10,361] ({pool-2-thread-3} SchedulerFactory.java[jobStarted]:114) - Job 20181011-192611_2092112872 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-sh:shared_process-shared_session
 INFO [2020-10-28 20:37:10,361] ({pool-2-thread-3} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20181011-192611_2092112872, interpreter: sh, note_id: 2FP1YEJHN, user: anonymous]
 INFO [2020-10-28 20:37:10,362] ({pool-2-thread-3} ManagedInterpreterGroup.java[getOrCreateInterpreterProcess]:61) - Create InterpreterProcess for InterpreterGroup: sh:shared_process
 INFO [2020-10-28 20:37:10,362] ({pool-2-thread-3} ShellScriptLauncher.java[launch]:48) - Launching Interpreter: sh
 INFO [2020-10-28 20:37:10,363] ({pool-2-thread-3} RemoteInterpreterManagedProcess.java[start]:115) - Thrift server for callback will start. Port: 42211
 INFO [2020-10-28 20:37:10,864] ({pool-2-thread-3} RemoteInterpreterManagedProcess.java[start]:190) - Run interpreter process [/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/bin/interpreter.sh, -d, /users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/sh, -c, 200.16.29.165, -p, 42211, -r, :, -l, /users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/local-repo/sh, -g, sh]
 INFO [2020-10-28 20:37:12,401] ({pool-9-thread-1} RemoteInterpreterManagedProcess.java[callback]:123) - RemoteInterpreterServer Registered: CallbackInfo(host:200.16.29.165, port:39503)
 INFO [2020-10-28 20:37:12,403] ({pool-2-thread-3} RemoteInterpreter.java[call]:168) - Create RemoteInterpreter org.apache.zeppelin.shell.ShellInterpreter
 INFO [2020-10-28 20:37:12,541] ({pool-2-thread-3} RemoteInterpreter.java[call]:142) - Open RemoteInterpreter org.apache.zeppelin.shell.ShellInterpreter
 INFO [2020-10-28 20:37:12,541] ({pool-2-thread-3} RemoteInterpreter.java[pushAngularObjectRegistryToRemote]:436) - Push local angular object registry from ZeppelinServer to remote interpreter group sh:shared_process
 WARN [2020-10-28 20:37:12,808] ({pool-2-thread-3} NotebookServer.java[afterStatusChange]:2316) - Job 20181011-192611_2092112872 is finished, status: ERROR, exception: null, result: %text head: cannot open '../../diplodatos_bigdata/ds/userid-profile.tsv' for reading: No such file or directory

%text ExitValue: 1
 INFO [2020-10-28 20:37:12,917] ({pool-2-thread-3} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 20:37:12,923] ({pool-2-thread-3} SchedulerFactory.java[jobFinished]:120) - Job 20181011-192611_2092112872 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-sh:shared_process-shared_session
 INFO [2020-10-28 20:37:20,871] ({qtp89387388-77} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 20:37:20,966] ({qtp89387388-77} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 20:37:20,973] ({pool-2-thread-4} SchedulerFactory.java[jobStarted]:114) - Job 20181011-192611_2092112872 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-sh:shared_process-shared_session
 INFO [2020-10-28 20:37:20,974] ({pool-2-thread-4} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20181011-192611_2092112872, interpreter: sh, note_id: 2FP1YEJHN, user: anonymous]
 WARN [2020-10-28 20:37:20,990] ({pool-2-thread-4} NotebookServer.java[afterStatusChange]:2316) - Job 20181011-192611_2092112872 is finished, status: ERROR, exception: null, result: %text /users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all
head: cannot open '../../diplodatos_bigdata/ds/userid-profile.tsv' for reading: No such file or directory

%text ExitValue: 1
 INFO [2020-10-28 20:37:21,069] ({pool-2-thread-4} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 20:37:21,073] ({pool-2-thread-4} SchedulerFactory.java[jobFinished]:120) - Job 20181011-192611_2092112872 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-sh:shared_process-shared_session
 INFO [2020-10-28 20:37:38,396] ({qtp89387388-72} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 20:37:38,478] ({qtp89387388-76} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 20:37:38,484] ({pool-2-thread-3} SchedulerFactory.java[jobStarted]:114) - Job 20181011-192611_2092112872 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-sh:shared_process-shared_session
 INFO [2020-10-28 20:37:38,485] ({pool-2-thread-3} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20181011-192611_2092112872, interpreter: sh, note_id: 2FP1YEJHN, user: anonymous]
 INFO [2020-10-28 20:37:38,520] ({pool-2-thread-3} NotebookServer.java[afterStatusChange]:2314) - Job 20181011-192611_2092112872 is finished successfully, status: FINISHED
 INFO [2020-10-28 20:37:38,583] ({pool-2-thread-3} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 20:37:38,588] ({pool-2-thread-3} SchedulerFactory.java[jobFinished]:120) - Job 20181011-192611_2092112872 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-sh:shared_process-shared_session
 INFO [2020-10-28 20:37:44,288] ({qtp89387388-76} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 20:37:44,348] ({qtp89387388-76} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 20:37:44,354] ({pool-2-thread-5} SchedulerFactory.java[jobStarted]:114) - Job 20171019-160931_1102056402 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 20:37:44,355] ({pool-2-thread-5} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20171019-160931_1102056402, interpreter: pyspark, note_id: 2FP1YEJHN, user: anonymous]
 INFO [2020-10-28 20:37:46,392] ({pool-2-thread-5} NotebookServer.java[afterStatusChange]:2314) - Job 20171019-160931_1102056402 is finished successfully, status: FINISHED
 INFO [2020-10-28 20:37:46,453] ({pool-2-thread-5} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 20:37:46,458] ({pool-2-thread-5} SchedulerFactory.java[jobFinished]:120) - Job 20171019-160931_1102056402 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 20:38:06,908] ({qtp89387388-26} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 20:38:06,914] ({pool-2-thread-4} SchedulerFactory.java[jobStarted]:114) - Job 20171020-165528_926197483 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 20:38:06,914] ({pool-2-thread-4} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20171020-165528_926197483, interpreter: pyspark, note_id: 2FP1YEJHN, user: anonymous]
 INFO [2020-10-28 20:38:06,940] ({pool-2-thread-4} NotebookServer.java[afterStatusChange]:2314) - Job 20171020-165528_926197483 is finished successfully, status: FINISHED
 INFO [2020-10-28 20:38:06,999] ({pool-2-thread-4} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 20:38:07,004] ({pool-2-thread-4} SchedulerFactory.java[jobFinished]:120) - Job 20171020-165528_926197483 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 20:38:24,793] ({qtp89387388-76} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 20:38:24,866] ({qtp89387388-26} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 20:38:24,871] ({pool-2-thread-6} SchedulerFactory.java[jobStarted]:114) - Job 20171019-160931_1102056402 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 20:38:24,872] ({pool-2-thread-6} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20171019-160931_1102056402, interpreter: pyspark, note_id: 2FP1YEJHN, user: anonymous]
 INFO [2020-10-28 20:38:25,043] ({pool-2-thread-6} NotebookServer.java[afterStatusChange]:2314) - Job 20171019-160931_1102056402 is finished successfully, status: FINISHED
 INFO [2020-10-28 20:38:25,106] ({pool-2-thread-6} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 20:38:25,110] ({pool-2-thread-6} SchedulerFactory.java[jobFinished]:120) - Job 20171019-160931_1102056402 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 20:38:27,520] ({qtp89387388-76} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 20:38:27,526] ({pool-2-thread-3} SchedulerFactory.java[jobStarted]:114) - Job 20171020-165528_926197483 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 20:38:27,527] ({pool-2-thread-3} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20171020-165528_926197483, interpreter: pyspark, note_id: 2FP1YEJHN, user: anonymous]
 INFO [2020-10-28 20:38:27,538] ({pool-2-thread-3} NotebookServer.java[afterStatusChange]:2314) - Job 20171020-165528_926197483 is finished successfully, status: FINISHED
 INFO [2020-10-28 20:38:27,588] ({pool-2-thread-3} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 20:38:27,593] ({pool-2-thread-3} SchedulerFactory.java[jobFinished]:120) - Job 20171020-165528_926197483 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 20:39:06,565] ({qtp89387388-26} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 20:40:13,008] ({qtp89387388-26} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 20:40:13,014] ({pool-2-thread-7} SchedulerFactory.java[jobStarted]:114) - Job 20171019-160931_1102056402 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 20:40:13,014] ({pool-2-thread-7} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20171019-160931_1102056402, interpreter: pyspark, note_id: 2FP1YEJHN, user: anonymous]
 INFO [2020-10-28 20:40:13,261] ({pool-2-thread-7} NotebookServer.java[afterStatusChange]:2314) - Job 20171019-160931_1102056402 is finished successfully, status: FINISHED
 INFO [2020-10-28 20:40:13,306] ({pool-2-thread-7} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 20:40:13,310] ({pool-2-thread-7} SchedulerFactory.java[jobFinished]:120) - Job 20171019-160931_1102056402 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 20:40:15,646] ({qtp89387388-23} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 20:40:15,652] ({pool-2-thread-5} SchedulerFactory.java[jobStarted]:114) - Job 20171020-165528_926197483 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 20:40:15,652] ({pool-2-thread-5} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20171020-165528_926197483, interpreter: pyspark, note_id: 2FP1YEJHN, user: anonymous]
 INFO [2020-10-28 20:40:15,665] ({pool-2-thread-5} NotebookServer.java[afterStatusChange]:2314) - Job 20171020-165528_926197483 is finished successfully, status: FINISHED
 INFO [2020-10-28 20:40:15,714] ({pool-2-thread-5} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 20:40:15,718] ({pool-2-thread-5} SchedulerFactory.java[jobFinished]:120) - Job 20171020-165528_926197483 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 20:40:20,880] ({qtp89387388-23} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 20:40:20,885] ({pool-2-thread-8} SchedulerFactory.java[jobStarted]:114) - Job 20191126-024041_1950604952 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 20:40:20,886] ({pool-2-thread-8} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20191126-024041_1950604952, interpreter: pyspark, note_id: 2FP1YEJHN, user: anonymous]
 INFO [2020-10-28 20:40:21,106] ({pool-2-thread-8} NotebookServer.java[afterStatusChange]:2314) - Job 20191126-024041_1950604952 is finished successfully, status: FINISHED
 INFO [2020-10-28 20:40:21,157] ({pool-2-thread-8} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 20:40:21,161] ({pool-2-thread-8} SchedulerFactory.java[jobFinished]:120) - Job 20191126-024041_1950604952 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 20:40:33,791] ({qtp89387388-23} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 20:40:33,796] ({pool-2-thread-4} SchedulerFactory.java[jobStarted]:114) - Job 20191126-023810_382641811 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 20:40:33,797] ({pool-2-thread-4} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20191126-023810_382641811, interpreter: pyspark, note_id: 2FP1YEJHN, user: anonymous]
 INFO [2020-10-28 20:40:33,903] ({pool-2-thread-4} NotebookServer.java[afterStatusChange]:2314) - Job 20191126-023810_382641811 is finished successfully, status: FINISHED
 INFO [2020-10-28 20:40:33,952] ({pool-2-thread-4} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 20:40:33,957] ({pool-2-thread-4} SchedulerFactory.java[jobFinished]:120) - Job 20191126-023810_382641811 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 20:40:54,547] ({qtp89387388-26} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 20:40:54,552] ({pool-2-thread-9} SchedulerFactory.java[jobStarted]:114) - Job 20171020-195004_405222821 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 20:40:54,552] ({pool-2-thread-9} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20171020-195004_405222821, interpreter: pyspark, note_id: 2FP1YEJHN, user: anonymous]
 INFO [2020-10-28 20:40:55,765] ({pool-2-thread-9} NotebookServer.java[afterStatusChange]:2314) - Job 20171020-195004_405222821 is finished successfully, status: FINISHED
 INFO [2020-10-28 20:40:55,811] ({pool-2-thread-9} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 20:40:55,815] ({pool-2-thread-9} SchedulerFactory.java[jobFinished]:120) - Job 20171020-195004_405222821 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 20:41:19,277] ({qtp89387388-72} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 20:41:19,282] ({pool-2-thread-6} SchedulerFactory.java[jobStarted]:114) - Job 20171020-181216_461915575 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 20:41:19,282] ({pool-2-thread-6} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20171020-181216_461915575, interpreter: pyspark, note_id: 2FP1YEJHN, user: anonymous]
 INFO [2020-10-28 20:41:19,652] ({pool-2-thread-6} NotebookServer.java[afterStatusChange]:2314) - Job 20171020-181216_461915575 is finished successfully, status: FINISHED
 INFO [2020-10-28 20:41:19,697] ({pool-2-thread-6} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 20:41:19,701] ({pool-2-thread-6} SchedulerFactory.java[jobFinished]:120) - Job 20171020-181216_461915575 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 20:41:29,634] ({qtp89387388-72} NotebookServer.java[sendNote]:828) - New operation from 127.0.0.1 : 43830 : anonymous : GET_NOTE : 2FNR8STJW
 WARN [2020-10-28 20:41:29,648] ({qtp89387388-72} GitNotebookRepo.java[revisionHistory]:158) - No Head found for 2FNR8STJW, No HEAD exists and no explicit starting revision was specified
 WARN [2020-10-28 20:41:29,729] ({qtp89387388-23} InterpreterSettingManager.java[compare]:886) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-10-28 20:41:29,730] ({qtp89387388-23} InterpreterSettingManager.java[compare]:886) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-10-28 20:41:29,730] ({qtp89387388-23} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-10-28 20:41:29,730] ({qtp89387388-23} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-10-28 20:41:29,731] ({qtp89387388-23} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-10-28 20:41:29,731] ({qtp89387388-23} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 INFO [2020-10-28 20:41:39,225] ({qtp89387388-318} VFSNotebookRepo.java[save]:196) - Saving note:2FNR8STJW
 INFO [2020-10-28 20:41:39,232] ({pool-2-thread-10} SchedulerFactory.java[jobStarted]:114) - Job 20201024-120547_1835592693 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 20:41:39,232] ({pool-2-thread-10} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20201024-120547_1835592693, interpreter: pyspark, note_id: 2FNR8STJW, user: anonymous]
 INFO [2020-10-28 20:41:39,905] ({pool-2-thread-10} NotebookServer.java[afterStatusChange]:2314) - Job 20201024-120547_1835592693 is finished successfully, status: FINISHED
 INFO [2020-10-28 20:41:39,944] ({pool-2-thread-10} VFSNotebookRepo.java[save]:196) - Saving note:2FNR8STJW
 INFO [2020-10-28 20:41:39,949] ({pool-2-thread-10} SchedulerFactory.java[jobFinished]:120) - Job 20201024-120547_1835592693 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 20:42:24,539] ({qtp89387388-69} NotebookServer.java[sendNote]:828) - New operation from 127.0.0.1 : 43830 : anonymous : GET_NOTE : 2FP1YEJHN
 WARN [2020-10-28 20:42:24,551] ({qtp89387388-69} GitNotebookRepo.java[revisionHistory]:158) - No Head found for 2FP1YEJHN, No HEAD exists and no explicit starting revision was specified
 WARN [2020-10-28 20:42:24,615] ({qtp89387388-26} InterpreterSettingManager.java[compare]:886) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-10-28 20:42:24,616] ({qtp89387388-26} InterpreterSettingManager.java[compare]:886) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-10-28 20:42:24,616] ({qtp89387388-26} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-10-28 20:42:24,616] ({qtp89387388-26} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-10-28 20:42:24,617] ({qtp89387388-26} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-10-28 20:42:24,617] ({qtp89387388-26} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 INFO [2020-10-28 20:43:14,429] ({qtp89387388-73} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 20:43:14,434] ({pool-2-thread-3} SchedulerFactory.java[jobStarted]:114) - Job 20191126-025015_908338399 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 20:43:14,435] ({pool-2-thread-3} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20191126-025015_908338399, interpreter: pyspark, note_id: 2FP1YEJHN, user: anonymous]
 WARN [2020-10-28 20:43:14,460] ({pool-2-thread-3} NotebookServer.java[afterStatusChange]:2316) - Job 20191126-025015_908338399 is finished, status: ERROR, exception: null, result: %text Fail to execute line 3: nUsr4CtryGen = spark.sql("SELECT country, gender, count(*) AS cantidad FROM users GROUP BY ... ORDER BY ...")
Traceback (most recent call last):
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/sql/utils.py", line 63, in deco
    return f(*a, **kw)
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/protocol.py", line 319, in get_return_value
    format(target_id, ".", name), value)
py4j.protocol.Py4JJavaError: An error occurred while calling o46.sql.
: org.apache.spark.sql.catalyst.parser.ParseException: 
mismatched input 'FROM' expecting {<EOF>, 'WHERE', 'GROUP', 'ORDER', 'HAVING', 'LIMIT', 'LATERAL', 'WINDOW', 'UNION', 'EXCEPT', 'MINUS', 'INTERSECT', 'SORT', 'CLUSTER', 'DISTRIBUTE'}(line 1, pos 45)

== SQL ==
SELECT country, gender, count(*) AS cantidad FROM users GROUP BY ... ORDER BY ...
---------------------------------------------^^^

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:217)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:114)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:48)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:68)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:632)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:280)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:745)


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/tmp/zeppelin_pyspark-3213863130067318734.py", line 375, in <module>
    exec(code, _zcUserQueryNameSpace)
  File "<stdin>", line 3, in <module>
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/sql/session.py", line 603, in sql
    return DataFrame(self._jsparkSession.sql(sqlQuery), self._wrapped)
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/java_gateway.py", line 1133, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/sql/utils.py", line 73, in deco
    raise ParseException(s.split(': ', 1)[1], stackTrace)
pyspark.sql.utils.ParseException: "\nmismatched input 'FROM' expecting {<EOF>, 'WHERE', 'GROUP', 'ORDER', 'HAVING', 'LIMIT', 'LATERAL', 'WINDOW', 'UNION', 'EXCEPT', 'MINUS', 'INTERSECT', 'SORT', 'CLUSTER', 'DISTRIBUTE'}(line 1, pos 45)\n\n== SQL ==\nSELECT country, gender, count(*) AS cantidad FROM users GROUP BY ... ORDER BY ...\n---------------------------------------------^^^\n"

 INFO [2020-10-28 20:43:14,510] ({pool-2-thread-3} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 20:43:14,514] ({pool-2-thread-3} SchedulerFactory.java[jobFinished]:120) - Job 20191126-025015_908338399 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 20:45:13,342] ({qtp89387388-71} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 20:45:27,175] ({qtp89387388-71} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 20:45:27,257] ({qtp89387388-317} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 20:45:27,262] ({pool-2-thread-11} SchedulerFactory.java[jobStarted]:114) - Job 20191126-025015_908338399 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 20:45:27,262] ({pool-2-thread-11} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20191126-025015_908338399, interpreter: pyspark, note_id: 2FP1YEJHN, user: anonymous]
 WARN [2020-10-28 20:45:27,303] ({pool-2-thread-11} NotebookServer.java[afterStatusChange]:2316) - Job 20191126-025015_908338399 is finished, status: ERROR, exception: null, result: %text Fail to execute line 3: nUsr4CtryGen = spark.sql("SELECT country, gender, count(*) AS cantidad FROM users GROUP BY gender ORDER BY cantidad DESC")
Traceback (most recent call last):
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/sql/utils.py", line 63, in deco
    return f(*a, **kw)
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/protocol.py", line 319, in get_return_value
    format(target_id, ".", name), value)
py4j.protocol.Py4JJavaError: An error occurred while calling o46.sql.
: org.apache.spark.sql.AnalysisException: expression 'users.`country`' is neither present in the group by, nor is it an aggregate function. Add to group by or wrap in first() (or first_value) if you don't care which value you get.;;
Sort [cantidad#128L DESC NULLS LAST], true
+- Aggregate [gender#59], [country#61, gender#59, count(1) AS cantidad#128L]
   +- SubqueryAlias users
      +- Relation[id#58,gender#59,age#60,country#61,registered#62] csv

	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.failAnalysis(CheckAnalysis.scala:39)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.failAnalysis(Analyzer.scala:91)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$class$$anonfun$$checkValidAggregateExpression$1(CheckAnalysis.scala:247)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$9.apply(CheckAnalysis.scala:280)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$9.apply(CheckAnalysis.scala:280)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:280)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:78)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:78)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:91)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:52)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:67)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:632)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:280)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:745)


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/tmp/zeppelin_pyspark-3213863130067318734.py", line 375, in <module>
    exec(code, _zcUserQueryNameSpace)
  File "<stdin>", line 3, in <module>
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/sql/session.py", line 603, in sql
    return DataFrame(self._jsparkSession.sql(sqlQuery), self._wrapped)
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/java_gateway.py", line 1133, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/sql/utils.py", line 69, in deco
    raise AnalysisException(s.split(': ', 1)[1], stackTrace)
pyspark.sql.utils.AnalysisException: "expression 'users.`country`' is neither present in the group by, nor is it an aggregate function. Add to group by or wrap in first() (or first_value) if you don't care which value you get.;;\nSort [cantidad#128L DESC NULLS LAST], true\n+- Aggregate [gender#59], [country#61, gender#59, count(1) AS cantidad#128L]\n   +- SubqueryAlias users\n      +- Relation[id#58,gender#59,age#60,country#61,registered#62] csv\n"

 INFO [2020-10-28 20:45:27,373] ({pool-2-thread-11} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 20:45:27,377] ({pool-2-thread-11} SchedulerFactory.java[jobFinished]:120) - Job 20191126-025015_908338399 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 20:46:11,555] ({qtp89387388-23} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 20:46:11,615] ({qtp89387388-319} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 20:46:11,620] ({pool-2-thread-7} SchedulerFactory.java[jobStarted]:114) - Job 20191126-025015_908338399 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 20:46:11,621] ({pool-2-thread-7} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20191126-025015_908338399, interpreter: pyspark, note_id: 2FP1YEJHN, user: anonymous]
 WARN [2020-10-28 20:46:11,667] ({pool-2-thread-7} NotebookServer.java[afterStatusChange]:2316) - Job 20191126-025015_908338399 is finished, status: ERROR, exception: null, result: %text Fail to execute line 3: nUsr4CtryGen = spark.sql("SELECT country, gender, count(*) AS cantidad FROM users GROUP BY county, gender ORDER BY cantidad DESC")
Traceback (most recent call last):
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/sql/utils.py", line 63, in deco
    return f(*a, **kw)
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/protocol.py", line 319, in get_return_value
    format(target_id, ".", name), value)
py4j.protocol.Py4JJavaError: An error occurred while calling o46.sql.
: org.apache.spark.sql.AnalysisException: cannot resolve '`county`' given input columns: [age, registered, id, gender, country]; line 1 pos 65;
'Sort ['cantidad DESC NULLS LAST], true
+- 'Aggregate ['county, gender#59], [country#61, gender#59, count(1) AS cantidad#131L]
   +- SubqueryAlias users
      +- Relation[id#58,gender#59,age#60,country#61,registered#62] csv

	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:88)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:85)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:288)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:268)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:268)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:279)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:289)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$1.apply(QueryPlan.scala:293)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:293)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$6.apply(QueryPlan.scala:298)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:298)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:268)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:85)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:78)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:78)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:91)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:52)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:67)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:632)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:280)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:745)


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/tmp/zeppelin_pyspark-3213863130067318734.py", line 375, in <module>
    exec(code, _zcUserQueryNameSpace)
  File "<stdin>", line 3, in <module>
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/sql/session.py", line 603, in sql
    return DataFrame(self._jsparkSession.sql(sqlQuery), self._wrapped)
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/java_gateway.py", line 1133, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/sql/utils.py", line 69, in deco
    raise AnalysisException(s.split(': ', 1)[1], stackTrace)
pyspark.sql.utils.AnalysisException: "cannot resolve '`county`' given input columns: [age, registered, id, gender, country]; line 1 pos 65;\n'Sort ['cantidad DESC NULLS LAST], true\n+- 'Aggregate ['county, gender#59], [country#61, gender#59, count(1) AS cantidad#131L]\n   +- SubqueryAlias users\n      +- Relation[id#58,gender#59,age#60,country#61,registered#62] csv\n"

 INFO [2020-10-28 20:46:11,713] ({pool-2-thread-7} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 20:46:11,717] ({pool-2-thread-7} SchedulerFactory.java[jobFinished]:120) - Job 20191126-025015_908338399 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 20:46:38,152] ({qtp89387388-23} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 20:46:38,260] ({qtp89387388-72} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 20:46:38,266] ({pool-2-thread-12} SchedulerFactory.java[jobStarted]:114) - Job 20191126-025015_908338399 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 20:46:38,267] ({pool-2-thread-12} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20191126-025015_908338399, interpreter: pyspark, note_id: 2FP1YEJHN, user: anonymous]
 WARN [2020-10-28 20:46:38,309] ({pool-2-thread-12} NotebookServer.java[afterStatusChange]:2316) - Job 20191126-025015_908338399 is finished, status: ERROR, exception: null, result: %text Fail to execute line 3: nUsr4CtryGen = spark.sql("SELECT country, gender, count(*) AS cantidad FROM users GROUP BY county ORDER BY cantidad DESC")
Traceback (most recent call last):
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/sql/utils.py", line 63, in deco
    return f(*a, **kw)
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/protocol.py", line 319, in get_return_value
    format(target_id, ".", name), value)
py4j.protocol.Py4JJavaError: An error occurred while calling o46.sql.
: org.apache.spark.sql.AnalysisException: cannot resolve '`county`' given input columns: [age, registered, id, gender, country]; line 1 pos 65;
'Sort ['cantidad DESC NULLS LAST], true
+- 'Aggregate ['county], [country#61, gender#59, count(1) AS cantidad#133L]
   +- SubqueryAlias users
      +- Relation[id#58,gender#59,age#60,country#61,registered#62] csv

	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:88)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:85)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:288)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:268)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:268)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:279)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:289)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$1.apply(QueryPlan.scala:293)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:293)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$6.apply(QueryPlan.scala:298)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:298)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:268)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:85)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:78)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:78)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:91)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:52)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:67)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:632)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:280)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:745)


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/tmp/zeppelin_pyspark-3213863130067318734.py", line 375, in <module>
    exec(code, _zcUserQueryNameSpace)
  File "<stdin>", line 3, in <module>
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/sql/session.py", line 603, in sql
    return DataFrame(self._jsparkSession.sql(sqlQuery), self._wrapped)
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/java_gateway.py", line 1133, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/sql/utils.py", line 69, in deco
    raise AnalysisException(s.split(': ', 1)[1], stackTrace)
pyspark.sql.utils.AnalysisException: "cannot resolve '`county`' given input columns: [age, registered, id, gender, country]; line 1 pos 65;\n'Sort ['cantidad DESC NULLS LAST], true\n+- 'Aggregate ['county], [country#61, gender#59, count(1) AS cantidad#133L]\n   +- SubqueryAlias users\n      +- Relation[id#58,gender#59,age#60,country#61,registered#62] csv\n"

 INFO [2020-10-28 20:46:38,379] ({pool-2-thread-12} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 20:46:38,383] ({pool-2-thread-12} SchedulerFactory.java[jobFinished]:120) - Job 20191126-025015_908338399 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 20:46:43,801] ({qtp89387388-320} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 20:46:44,171] ({qtp89387388-23} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 20:46:44,177] ({pool-2-thread-5} SchedulerFactory.java[jobStarted]:114) - Job 20191126-025015_908338399 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 20:46:44,179] ({pool-2-thread-5} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20191126-025015_908338399, interpreter: pyspark, note_id: 2FP1YEJHN, user: anonymous]
 WARN [2020-10-28 20:46:44,232] ({pool-2-thread-5} NotebookServer.java[afterStatusChange]:2316) - Job 20191126-025015_908338399 is finished, status: ERROR, exception: null, result: %text Fail to execute line 3: nUsr4CtryGen = spark.sql("SELECT country, count(*) AS cantidad FROM users GROUP BY county ORDER BY cantidad DESC")
Traceback (most recent call last):
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/sql/utils.py", line 63, in deco
    return f(*a, **kw)
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/protocol.py", line 319, in get_return_value
    format(target_id, ".", name), value)
py4j.protocol.Py4JJavaError: An error occurred while calling o46.sql.
: org.apache.spark.sql.AnalysisException: cannot resolve '`county`' given input columns: [age, registered, id, gender, country]; line 1 pos 57;
'Sort ['cantidad DESC NULLS LAST], true
+- 'Aggregate ['county], [country#61, count(1) AS cantidad#135L]
   +- SubqueryAlias users
      +- Relation[id#58,gender#59,age#60,country#61,registered#62] csv

	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:88)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:85)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:288)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:268)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:268)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:279)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:289)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$1.apply(QueryPlan.scala:293)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:293)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$6.apply(QueryPlan.scala:298)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:298)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:268)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:85)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:78)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:78)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:91)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:52)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:67)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:632)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:280)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:745)


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/tmp/zeppelin_pyspark-3213863130067318734.py", line 375, in <module>
    exec(code, _zcUserQueryNameSpace)
  File "<stdin>", line 3, in <module>
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/sql/session.py", line 603, in sql
    return DataFrame(self._jsparkSession.sql(sqlQuery), self._wrapped)
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/java_gateway.py", line 1133, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/sql/utils.py", line 69, in deco
    raise AnalysisException(s.split(': ', 1)[1], stackTrace)
pyspark.sql.utils.AnalysisException: "cannot resolve '`county`' given input columns: [age, registered, id, gender, country]; line 1 pos 57;\n'Sort ['cantidad DESC NULLS LAST], true\n+- 'Aggregate ['county], [country#61, count(1) AS cantidad#135L]\n   +- SubqueryAlias users\n      +- Relation[id#58,gender#59,age#60,country#61,registered#62] csv\n"

 INFO [2020-10-28 20:46:44,283] ({pool-2-thread-5} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 20:46:44,286] ({pool-2-thread-5} SchedulerFactory.java[jobFinished]:120) - Job 20191126-025015_908338399 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 20:46:45,056] ({qtp89387388-320} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 20:46:45,061] ({pool-2-thread-13} SchedulerFactory.java[jobStarted]:114) - Job 20191126-025015_908338399 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 20:46:45,062] ({pool-2-thread-13} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20191126-025015_908338399, interpreter: pyspark, note_id: 2FP1YEJHN, user: anonymous]
 WARN [2020-10-28 20:46:45,099] ({pool-2-thread-13} NotebookServer.java[afterStatusChange]:2316) - Job 20191126-025015_908338399 is finished, status: ERROR, exception: null, result: %text Fail to execute line 3: nUsr4CtryGen = spark.sql("SELECT country, count(*) AS cantidad FROM users GROUP BY county ORDER BY cantidad DESC")
Traceback (most recent call last):
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/sql/utils.py", line 63, in deco
    return f(*a, **kw)
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/protocol.py", line 319, in get_return_value
    format(target_id, ".", name), value)
py4j.protocol.Py4JJavaError: An error occurred while calling o46.sql.
: org.apache.spark.sql.AnalysisException: cannot resolve '`county`' given input columns: [age, registered, id, gender, country]; line 1 pos 57;
'Sort ['cantidad DESC NULLS LAST], true
+- 'Aggregate ['county], [country#61, count(1) AS cantidad#137L]
   +- SubqueryAlias users
      +- Relation[id#58,gender#59,age#60,country#61,registered#62] csv

	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:88)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:85)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:288)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:268)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:268)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:279)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:289)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$1.apply(QueryPlan.scala:293)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:293)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$6.apply(QueryPlan.scala:298)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:298)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:268)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:85)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:78)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:78)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:91)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:52)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:67)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:632)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:280)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:745)


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/tmp/zeppelin_pyspark-3213863130067318734.py", line 375, in <module>
    exec(code, _zcUserQueryNameSpace)
  File "<stdin>", line 3, in <module>
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/sql/session.py", line 603, in sql
    return DataFrame(self._jsparkSession.sql(sqlQuery), self._wrapped)
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/java_gateway.py", line 1133, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/sql/utils.py", line 69, in deco
    raise AnalysisException(s.split(': ', 1)[1], stackTrace)
pyspark.sql.utils.AnalysisException: "cannot resolve '`county`' given input columns: [age, registered, id, gender, country]; line 1 pos 57;\n'Sort ['cantidad DESC NULLS LAST], true\n+- 'Aggregate ['county], [country#61, count(1) AS cantidad#137L]\n   +- SubqueryAlias users\n      +- Relation[id#58,gender#59,age#60,country#61,registered#62] csv\n"

 INFO [2020-10-28 20:46:45,149] ({pool-2-thread-13} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 20:46:45,153] ({pool-2-thread-13} SchedulerFactory.java[jobFinished]:120) - Job 20191126-025015_908338399 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 20:47:16,852] ({qtp89387388-23} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 20:47:16,932] ({qtp89387388-72} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 20:47:16,937] ({pool-2-thread-8} SchedulerFactory.java[jobStarted]:114) - Job 20191126-025015_908338399 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 20:47:16,938] ({pool-2-thread-8} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20191126-025015_908338399, interpreter: pyspark, note_id: 2FP1YEJHN, user: anonymous]
 INFO [2020-10-28 20:47:17,271] ({pool-2-thread-8} NotebookServer.java[afterStatusChange]:2314) - Job 20191126-025015_908338399 is finished successfully, status: FINISHED
 INFO [2020-10-28 20:47:17,316] ({pool-2-thread-8} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 20:47:17,320] ({pool-2-thread-8} SchedulerFactory.java[jobFinished]:120) - Job 20191126-025015_908338399 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 20:47:27,931] ({qtp89387388-320} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 20:47:28,009] ({qtp89387388-23} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 20:47:28,013] ({pool-2-thread-14} SchedulerFactory.java[jobStarted]:114) - Job 20191126-025015_908338399 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 20:47:28,014] ({pool-2-thread-14} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20191126-025015_908338399, interpreter: pyspark, note_id: 2FP1YEJHN, user: anonymous]
 INFO [2020-10-28 20:47:28,578] ({pool-2-thread-14} NotebookServer.java[afterStatusChange]:2314) - Job 20191126-025015_908338399 is finished successfully, status: FINISHED
 INFO [2020-10-28 20:47:28,623] ({pool-2-thread-14} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 20:47:28,627] ({pool-2-thread-14} SchedulerFactory.java[jobFinished]:120) - Job 20191126-025015_908338399 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 20:48:06,931] ({qtp89387388-21} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 20:48:23,032] ({qtp89387388-23} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 20:48:23,128] ({qtp89387388-21} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 20:48:23,133] ({pool-2-thread-4} SchedulerFactory.java[jobStarted]:114) - Job 20191126-030740_538459228 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 20:48:23,134] ({pool-2-thread-4} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20191126-030740_538459228, interpreter: pyspark, note_id: 2FP1YEJHN, user: anonymous]
 INFO [2020-10-28 20:48:23,470] ({pool-2-thread-4} NotebookServer.java[afterStatusChange]:2314) - Job 20191126-030740_538459228 is finished successfully, status: FINISHED
 INFO [2020-10-28 20:48:23,510] ({pool-2-thread-4} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 20:48:23,514] ({pool-2-thread-4} SchedulerFactory.java[jobFinished]:120) - Job 20191126-030740_538459228 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 20:49:20,828] ({qtp89387388-80} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 20:49:20,834] ({pool-2-thread-15} SchedulerFactory.java[jobStarted]:114) - Job 20171023-115517_1163412651 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 20:49:20,834] ({pool-2-thread-15} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20171023-115517_1163412651, interpreter: pyspark, note_id: 2FP1YEJHN, user: anonymous]
 INFO [2020-10-28 20:49:22,121] ({pool-2-thread-15} NotebookServer.java[afterStatusChange]:2314) - Job 20171023-115517_1163412651 is finished successfully, status: FINISHED
 INFO [2020-10-28 20:49:22,172] ({pool-2-thread-15} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 20:49:22,179] ({pool-2-thread-15} SchedulerFactory.java[jobFinished]:120) - Job 20171023-115517_1163412651 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 20:49:31,135] ({qtp89387388-74} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 20:49:31,140] ({pool-2-thread-9} SchedulerFactory.java[jobStarted]:114) - Job 20171023-115517_1163412651 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 20:49:31,140] ({pool-2-thread-9} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20171023-115517_1163412651, interpreter: pyspark, note_id: 2FP1YEJHN, user: anonymous]
 INFO [2020-10-28 20:49:32,218] ({pool-2-thread-9} NotebookServer.java[afterStatusChange]:2314) - Job 20171023-115517_1163412651 is finished successfully, status: FINISHED
 INFO [2020-10-28 20:49:32,262] ({pool-2-thread-9} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 20:49:32,266] ({pool-2-thread-9} SchedulerFactory.java[jobFinished]:120) - Job 20171023-115517_1163412651 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 20:49:45,589] ({qtp89387388-21} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 20:49:45,594] ({pool-2-thread-16} SchedulerFactory.java[jobStarted]:114) - Job 20171023-115718_1445962443 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 20:49:45,595] ({pool-2-thread-16} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20171023-115718_1445962443, interpreter: pyspark, note_id: 2FP1YEJHN, user: anonymous]
 INFO [2020-10-28 20:49:46,014] ({pool-2-thread-16} NotebookServer.java[afterStatusChange]:2314) - Job 20171023-115718_1445962443 is finished successfully, status: FINISHED
 INFO [2020-10-28 20:49:46,067] ({pool-2-thread-16} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 20:49:46,070] ({pool-2-thread-16} SchedulerFactory.java[jobFinished]:120) - Job 20171023-115718_1445962443 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 20:49:53,592] ({qtp89387388-21} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 20:49:53,597] ({pool-2-thread-6} SchedulerFactory.java[jobStarted]:114) - Job 20171023-165238_672281439 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-sh:shared_process-shared_session
 INFO [2020-10-28 20:49:53,598] ({pool-2-thread-6} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20171023-165238_672281439, interpreter: sh, note_id: 2FP1YEJHN, user: anonymous]
 INFO [2020-10-28 20:49:53,620] ({pool-2-thread-6} NotebookServer.java[afterStatusChange]:2314) - Job 20171023-165238_672281439 is finished successfully, status: FINISHED
 INFO [2020-10-28 20:49:53,662] ({pool-2-thread-6} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 20:49:53,666] ({pool-2-thread-6} SchedulerFactory.java[jobFinished]:120) - Job 20171023-165238_672281439 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-sh:shared_process-shared_session
 INFO [2020-10-28 21:02:41,400] ({qtp89387388-21} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 21:02:53,568] ({qtp89387388-565} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 21:02:53,664] ({qtp89387388-564} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 21:02:53,669] ({pool-2-thread-17} SchedulerFactory.java[jobStarted]:114) - Job 20201023-123443_40437356 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 21:02:53,670] ({pool-2-thread-17} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20201023-123443_40437356, interpreter: pyspark, note_id: 2FP1YEJHN, user: anonymous]
 WARN [2020-10-28 21:02:53,895] ({pool-2-thread-17} NotebookServer.java[afterStatusChange]:2316) - Job 20201023-123443_40437356 is finished, status: ERROR, exception: null, result: %text Fail to execute line 5: spark.sql("create table gen_prom as SELECT gender, avg(count(*)) AS age_avg FROM users GROUP BY gender")
Traceback (most recent call last):
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/sql/utils.py", line 63, in deco
    return f(*a, **kw)
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/protocol.py", line 319, in get_return_value
    format(target_id, ".", name), value)
py4j.protocol.Py4JJavaError: An error occurred while calling o46.sql.
: org.apache.spark.sql.AnalysisException: It is not allowed to use an aggregate function in the argument of another aggregate function. Please use the inner aggregate function in a sub-query.;;
Aggregate [gender#59], [gender#59, avg(count(1)) AS age_avg#284]
+- SubqueryAlias users
   +- Relation[id#58,gender#59,age#60,country#61,registered#62] csv

	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.failAnalysis(CheckAnalysis.scala:39)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.failAnalysis(Analyzer.scala:91)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$org$apache$spark$sql$catalyst$analysis$CheckAnalysis$class$$anonfun$$checkValidAggregateExpression$1$1$$anonfun$apply$7.apply(CheckAnalysis.scala:221)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$org$apache$spark$sql$catalyst$analysis$CheckAnalysis$class$$anonfun$$checkValidAggregateExpression$1$1$$anonfun$apply$7.apply(CheckAnalysis.scala:219)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreach(TreeNode.scala:117)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$org$apache$spark$sql$catalyst$analysis$CheckAnalysis$class$$anonfun$$checkValidAggregateExpression$1$1.apply(CheckAnalysis.scala:219)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$org$apache$spark$sql$catalyst$analysis$CheckAnalysis$class$$anonfun$$checkValidAggregateExpression$1$1.apply(CheckAnalysis.scala:218)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$class$$anonfun$$checkValidAggregateExpression$1(CheckAnalysis.scala:218)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$org$apache$spark$sql$catalyst$analysis$CheckAnalysis$class$$anonfun$$checkValidAggregateExpression$1$5.apply(CheckAnalysis.scala:253)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$org$apache$spark$sql$catalyst$analysis$CheckAnalysis$class$$anonfun$$checkValidAggregateExpression$1$5.apply(CheckAnalysis.scala:253)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$class$$anonfun$$checkValidAggregateExpression$1(CheckAnalysis.scala:253)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$9.apply(CheckAnalysis.scala:280)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$9.apply(CheckAnalysis.scala:280)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:280)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:78)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:78)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:91)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:52)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:67)
	at org.apache.spark.sql.hive.execution.InsertIntoHiveTable.run(InsertIntoHiveTable.scala:326)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)
	at org.apache.spark.sql.hive.execution.CreateHiveTableAsSelectCommand.run(CreateHiveTableAsSelectCommand.scala:81)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:67)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:183)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:68)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:632)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:280)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:745)


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/tmp/zeppelin_pyspark-3213863130067318734.py", line 375, in <module>
    exec(code, _zcUserQueryNameSpace)
  File "<stdin>", line 5, in <module>
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/sql/session.py", line 603, in sql
    return DataFrame(self._jsparkSession.sql(sqlQuery), self._wrapped)
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/java_gateway.py", line 1133, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/sql/utils.py", line 69, in deco
    raise AnalysisException(s.split(': ', 1)[1], stackTrace)
pyspark.sql.utils.AnalysisException: 'It is not allowed to use an aggregate function in the argument of another aggregate function. Please use the inner aggregate function in a sub-query.;;\nAggregate [gender#59], [gender#59, avg(count(1)) AS age_avg#284]\n+- SubqueryAlias users\n   +- Relation[id#58,gender#59,age#60,country#61,registered#62] csv\n'

 INFO [2020-10-28 21:02:53,961] ({pool-2-thread-17} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 21:02:53,965] ({pool-2-thread-17} SchedulerFactory.java[jobFinished]:120) - Job 20201023-123443_40437356 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 21:03:58,409] ({qtp89387388-571} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 21:03:58,517] ({qtp89387388-564} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 21:03:58,522] ({pool-2-thread-10} SchedulerFactory.java[jobStarted]:114) - Job 20201023-123443_40437356 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 21:03:58,523] ({pool-2-thread-10} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20201023-123443_40437356, interpreter: pyspark, note_id: 2FP1YEJHN, user: anonymous]
 WARN [2020-10-28 21:03:58,699] ({pool-2-thread-10} NotebookServer.java[afterStatusChange]:2316) - Job 20201023-123443_40437356 is finished, status: ERROR, exception: null, result: %text Fail to execute line 5: spark.sql("create table gen_prom as SELECT gender, avg(count(gender)) AS age_avg FROM users GROUP BY gender")
Traceback (most recent call last):
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/sql/utils.py", line 63, in deco
    return f(*a, **kw)
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/protocol.py", line 319, in get_return_value
    format(target_id, ".", name), value)
py4j.protocol.Py4JJavaError: An error occurred while calling o46.sql.
: org.apache.spark.sql.AnalysisException: It is not allowed to use an aggregate function in the argument of another aggregate function. Please use the inner aggregate function in a sub-query.;;
Aggregate [gender#59], [gender#59, avg(count(gender#59)) AS age_avg#290]
+- SubqueryAlias users
   +- Relation[id#58,gender#59,age#60,country#61,registered#62] csv

	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.failAnalysis(CheckAnalysis.scala:39)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.failAnalysis(Analyzer.scala:91)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$org$apache$spark$sql$catalyst$analysis$CheckAnalysis$class$$anonfun$$checkValidAggregateExpression$1$1$$anonfun$apply$7.apply(CheckAnalysis.scala:221)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$org$apache$spark$sql$catalyst$analysis$CheckAnalysis$class$$anonfun$$checkValidAggregateExpression$1$1$$anonfun$apply$7.apply(CheckAnalysis.scala:219)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreach(TreeNode.scala:117)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$org$apache$spark$sql$catalyst$analysis$CheckAnalysis$class$$anonfun$$checkValidAggregateExpression$1$1.apply(CheckAnalysis.scala:219)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$org$apache$spark$sql$catalyst$analysis$CheckAnalysis$class$$anonfun$$checkValidAggregateExpression$1$1.apply(CheckAnalysis.scala:218)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$class$$anonfun$$checkValidAggregateExpression$1(CheckAnalysis.scala:218)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$org$apache$spark$sql$catalyst$analysis$CheckAnalysis$class$$anonfun$$checkValidAggregateExpression$1$5.apply(CheckAnalysis.scala:253)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$org$apache$spark$sql$catalyst$analysis$CheckAnalysis$class$$anonfun$$checkValidAggregateExpression$1$5.apply(CheckAnalysis.scala:253)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$class$$anonfun$$checkValidAggregateExpression$1(CheckAnalysis.scala:253)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$9.apply(CheckAnalysis.scala:280)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$9.apply(CheckAnalysis.scala:280)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:280)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:78)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:78)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:91)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:52)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:67)
	at org.apache.spark.sql.hive.execution.InsertIntoHiveTable.run(InsertIntoHiveTable.scala:326)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)
	at org.apache.spark.sql.hive.execution.CreateHiveTableAsSelectCommand.run(CreateHiveTableAsSelectCommand.scala:81)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:67)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:183)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:68)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:632)
	at sun.reflect.GeneratedMethodAccessor91.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:280)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:745)


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/tmp/zeppelin_pyspark-3213863130067318734.py", line 375, in <module>
    exec(code, _zcUserQueryNameSpace)
  File "<stdin>", line 5, in <module>
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/sql/session.py", line 603, in sql
    return DataFrame(self._jsparkSession.sql(sqlQuery), self._wrapped)
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/java_gateway.py", line 1133, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/sql/utils.py", line 69, in deco
    raise AnalysisException(s.split(': ', 1)[1], stackTrace)
pyspark.sql.utils.AnalysisException: 'It is not allowed to use an aggregate function in the argument of another aggregate function. Please use the inner aggregate function in a sub-query.;;\nAggregate [gender#59], [gender#59, avg(count(gender#59)) AS age_avg#290]\n+- SubqueryAlias users\n   +- Relation[id#58,gender#59,age#60,country#61,registered#62] csv\n'

 INFO [2020-10-28 21:03:58,742] ({pool-2-thread-10} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 21:03:58,745] ({pool-2-thread-10} SchedulerFactory.java[jobFinished]:120) - Job 20201023-123443_40437356 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 21:04:14,010] ({qtp89387388-571} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 21:04:14,109] ({qtp89387388-26} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 21:04:14,113] ({pool-2-thread-18} SchedulerFactory.java[jobStarted]:114) - Job 20201023-123443_40437356 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 21:04:14,114] ({pool-2-thread-18} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20201023-123443_40437356, interpreter: pyspark, note_id: 2FP1YEJHN, user: anonymous]
 INFO [2020-10-28 21:04:18,719] ({pool-2-thread-18} NotebookServer.java[afterStatusChange]:2314) - Job 20201023-123443_40437356 is finished successfully, status: FINISHED
 INFO [2020-10-28 21:04:18,763] ({pool-2-thread-18} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 21:04:18,767] ({pool-2-thread-18} SchedulerFactory.java[jobFinished]:120) - Job 20201023-123443_40437356 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 21:04:35,672] ({qtp89387388-571} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 21:04:35,740] ({qtp89387388-26} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 21:04:35,744] ({pool-2-thread-3} SchedulerFactory.java[jobStarted]:114) - Job 20201023-123443_40437356 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 21:04:35,745] ({pool-2-thread-3} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20201023-123443_40437356, interpreter: pyspark, note_id: 2FP1YEJHN, user: anonymous]
 WARN [2020-10-28 21:04:36,019] ({pool-2-thread-3} NotebookServer.java[afterStatusChange]:2316) - Job 20201023-123443_40437356 is finished, status: ERROR, exception: null, result: %text Fail to execute line 5: spark.sql("create table gen_prom as SELECT gender, avg(count(gender)) AS age_avg FROM users GROUP BY gender")
Traceback (most recent call last):
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/sql/utils.py", line 63, in deco
    return f(*a, **kw)
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/protocol.py", line 319, in get_return_value
    format(target_id, ".", name), value)
py4j.protocol.Py4JJavaError: An error occurred while calling o46.sql.
: org.apache.spark.sql.AnalysisException: It is not allowed to use an aggregate function in the argument of another aggregate function. Please use the inner aggregate function in a sub-query.;;
Aggregate [gender#59], [gender#59, avg(count(gender#59)) AS age_avg#327]
+- SubqueryAlias users
   +- Relation[id#58,gender#59,age#60,country#61,registered#62] csv

	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.failAnalysis(CheckAnalysis.scala:39)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.failAnalysis(Analyzer.scala:91)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$org$apache$spark$sql$catalyst$analysis$CheckAnalysis$class$$anonfun$$checkValidAggregateExpression$1$1$$anonfun$apply$7.apply(CheckAnalysis.scala:221)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$org$apache$spark$sql$catalyst$analysis$CheckAnalysis$class$$anonfun$$checkValidAggregateExpression$1$1$$anonfun$apply$7.apply(CheckAnalysis.scala:219)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreach(TreeNode.scala:117)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$org$apache$spark$sql$catalyst$analysis$CheckAnalysis$class$$anonfun$$checkValidAggregateExpression$1$1.apply(CheckAnalysis.scala:219)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$org$apache$spark$sql$catalyst$analysis$CheckAnalysis$class$$anonfun$$checkValidAggregateExpression$1$1.apply(CheckAnalysis.scala:218)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$class$$anonfun$$checkValidAggregateExpression$1(CheckAnalysis.scala:218)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$org$apache$spark$sql$catalyst$analysis$CheckAnalysis$class$$anonfun$$checkValidAggregateExpression$1$5.apply(CheckAnalysis.scala:253)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$org$apache$spark$sql$catalyst$analysis$CheckAnalysis$class$$anonfun$$checkValidAggregateExpression$1$5.apply(CheckAnalysis.scala:253)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$class$$anonfun$$checkValidAggregateExpression$1(CheckAnalysis.scala:253)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$9.apply(CheckAnalysis.scala:280)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$9.apply(CheckAnalysis.scala:280)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:280)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:78)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:78)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:91)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:52)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:67)
	at org.apache.spark.sql.hive.execution.InsertIntoHiveTable.run(InsertIntoHiveTable.scala:326)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)
	at org.apache.spark.sql.hive.execution.CreateHiveTableAsSelectCommand.run(CreateHiveTableAsSelectCommand.scala:81)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:67)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:183)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:68)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:632)
	at sun.reflect.GeneratedMethodAccessor91.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:280)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:745)


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/tmp/zeppelin_pyspark-3213863130067318734.py", line 375, in <module>
    exec(code, _zcUserQueryNameSpace)
  File "<stdin>", line 5, in <module>
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/sql/session.py", line 603, in sql
    return DataFrame(self._jsparkSession.sql(sqlQuery), self._wrapped)
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/java_gateway.py", line 1133, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/sql/utils.py", line 69, in deco
    raise AnalysisException(s.split(': ', 1)[1], stackTrace)
pyspark.sql.utils.AnalysisException: 'It is not allowed to use an aggregate function in the argument of another aggregate function. Please use the inner aggregate function in a sub-query.;;\nAggregate [gender#59], [gender#59, avg(count(gender#59)) AS age_avg#327]\n+- SubqueryAlias users\n   +- Relation[id#58,gender#59,age#60,country#61,registered#62] csv\n'

 INFO [2020-10-28 21:04:36,063] ({pool-2-thread-3} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 21:04:36,067] ({pool-2-thread-3} SchedulerFactory.java[jobFinished]:120) - Job 20201023-123443_40437356 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 21:04:45,239] ({qtp89387388-564} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 21:04:45,318] ({qtp89387388-571} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 21:04:45,323] ({pool-2-thread-19} SchedulerFactory.java[jobStarted]:114) - Job 20201023-123443_40437356 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 21:04:45,324] ({pool-2-thread-19} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20201023-123443_40437356, interpreter: pyspark, note_id: 2FP1YEJHN, user: anonymous]
 INFO [2020-10-28 21:04:49,964] ({pool-2-thread-19} NotebookServer.java[afterStatusChange]:2314) - Job 20201023-123443_40437356 is finished successfully, status: FINISHED
 INFO [2020-10-28 21:04:50,005] ({pool-2-thread-19} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 21:04:50,009] ({pool-2-thread-19} SchedulerFactory.java[jobFinished]:120) - Job 20201023-123443_40437356 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 21:05:11,548] ({qtp89387388-21} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 21:05:11,641] ({qtp89387388-564} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 21:05:11,646] ({pool-2-thread-11} SchedulerFactory.java[jobStarted]:114) - Job 20201023-123443_40437356 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 21:05:11,647] ({pool-2-thread-11} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20201023-123443_40437356, interpreter: pyspark, note_id: 2FP1YEJHN, user: anonymous]
 WARN [2020-10-28 21:05:11,898] ({pool-2-thread-11} NotebookServer.java[afterStatusChange]:2316) - Job 20201023-123443_40437356 is finished, status: ERROR, exception: null, result: %text Fail to execute line 5: spark.sql("create table gen_prom as SELECT gender, avg(count(*)) AS age_avg FROM users GROUP BY gender")
Traceback (most recent call last):
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/sql/utils.py", line 63, in deco
    return f(*a, **kw)
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/protocol.py", line 319, in get_return_value
    format(target_id, ".", name), value)
py4j.protocol.Py4JJavaError: An error occurred while calling o46.sql.
: org.apache.spark.sql.AnalysisException: It is not allowed to use an aggregate function in the argument of another aggregate function. Please use the inner aggregate function in a sub-query.;;
Aggregate [gender#59], [gender#59, avg(count(1)) AS age_avg#366]
+- SubqueryAlias users
   +- Relation[id#58,gender#59,age#60,country#61,registered#62] csv

	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.failAnalysis(CheckAnalysis.scala:39)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.failAnalysis(Analyzer.scala:91)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$org$apache$spark$sql$catalyst$analysis$CheckAnalysis$class$$anonfun$$checkValidAggregateExpression$1$1$$anonfun$apply$7.apply(CheckAnalysis.scala:221)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$org$apache$spark$sql$catalyst$analysis$CheckAnalysis$class$$anonfun$$checkValidAggregateExpression$1$1$$anonfun$apply$7.apply(CheckAnalysis.scala:219)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreach(TreeNode.scala:117)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$org$apache$spark$sql$catalyst$analysis$CheckAnalysis$class$$anonfun$$checkValidAggregateExpression$1$1.apply(CheckAnalysis.scala:219)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$org$apache$spark$sql$catalyst$analysis$CheckAnalysis$class$$anonfun$$checkValidAggregateExpression$1$1.apply(CheckAnalysis.scala:218)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$class$$anonfun$$checkValidAggregateExpression$1(CheckAnalysis.scala:218)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$org$apache$spark$sql$catalyst$analysis$CheckAnalysis$class$$anonfun$$checkValidAggregateExpression$1$5.apply(CheckAnalysis.scala:253)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$org$apache$spark$sql$catalyst$analysis$CheckAnalysis$class$$anonfun$$checkValidAggregateExpression$1$5.apply(CheckAnalysis.scala:253)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$class$$anonfun$$checkValidAggregateExpression$1(CheckAnalysis.scala:253)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$9.apply(CheckAnalysis.scala:280)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$9.apply(CheckAnalysis.scala:280)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:280)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:78)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:78)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:91)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:52)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:67)
	at org.apache.spark.sql.hive.execution.InsertIntoHiveTable.run(InsertIntoHiveTable.scala:326)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)
	at org.apache.spark.sql.hive.execution.CreateHiveTableAsSelectCommand.run(CreateHiveTableAsSelectCommand.scala:81)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:67)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:183)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:68)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:632)
	at sun.reflect.GeneratedMethodAccessor91.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:280)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:745)


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/tmp/zeppelin_pyspark-3213863130067318734.py", line 375, in <module>
    exec(code, _zcUserQueryNameSpace)
  File "<stdin>", line 5, in <module>
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/sql/session.py", line 603, in sql
    return DataFrame(self._jsparkSession.sql(sqlQuery), self._wrapped)
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/java_gateway.py", line 1133, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/sql/utils.py", line 69, in deco
    raise AnalysisException(s.split(': ', 1)[1], stackTrace)
pyspark.sql.utils.AnalysisException: 'It is not allowed to use an aggregate function in the argument of another aggregate function. Please use the inner aggregate function in a sub-query.;;\nAggregate [gender#59], [gender#59, avg(count(1)) AS age_avg#366]\n+- SubqueryAlias users\n   +- Relation[id#58,gender#59,age#60,country#61,registered#62] csv\n'

 INFO [2020-10-28 21:05:11,940] ({pool-2-thread-11} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 21:05:11,943] ({pool-2-thread-11} SchedulerFactory.java[jobFinished]:120) - Job 20201023-123443_40437356 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 21:05:20,937] ({qtp89387388-26} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 21:05:21,043] ({qtp89387388-21} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 21:05:21,047] ({pool-2-thread-20} SchedulerFactory.java[jobStarted]:114) - Job 20201023-123443_40437356 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 21:05:21,047] ({pool-2-thread-20} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20201023-123443_40437356, interpreter: pyspark, note_id: 2FP1YEJHN, user: anonymous]
 INFO [2020-10-28 21:05:25,142] ({pool-2-thread-20} NotebookServer.java[afterStatusChange]:2314) - Job 20201023-123443_40437356 is finished successfully, status: FINISHED
 INFO [2020-10-28 21:05:25,188] ({pool-2-thread-20} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 21:05:25,192] ({pool-2-thread-20} SchedulerFactory.java[jobFinished]:120) - Job 20201023-123443_40437356 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 21:16:27,639] ({qtp89387388-712} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 21:16:27,643] ({qtp89387388-712} NotebookServer.java[broadcastNewParagraph]:688) - Broadcasting paragraph on run call instead of note.
 INFO [2020-10-28 21:17:01,783] ({qtp89387388-713} NotebookServer.java[sendNote]:828) - New operation from 127.0.0.1 : 43830 : anonymous : GET_NOTE : 2FQA9JFA8
 WARN [2020-10-28 21:17:01,794] ({qtp89387388-713} GitNotebookRepo.java[revisionHistory]:158) - No Head found for 2FQA9JFA8, No HEAD exists and no explicit starting revision was specified
 WARN [2020-10-28 21:17:01,857] ({qtp89387388-712} InterpreterSettingManager.java[compare]:886) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-10-28 21:17:01,857] ({qtp89387388-712} InterpreterSettingManager.java[compare]:886) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-10-28 21:17:01,857] ({qtp89387388-712} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-10-28 21:17:01,858] ({qtp89387388-712} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-10-28 21:17:01,858] ({qtp89387388-712} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-10-28 21:17:01,858] ({qtp89387388-712} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-10-28 21:17:02,943] ({qtp89387388-712} NotebookServer.java[getEditorSetting]:2472) - Fail to get interpreter: pyspark
 WARN [2020-10-28 21:17:02,944] ({qtp89387388-712} NotebookServer.java[getEditorSetting]:2472) - Fail to get interpreter: pyspark
 WARN [2020-10-28 21:17:02,945] ({qtp89387388-712} NotebookServer.java[getEditorSetting]:2472) - Fail to get interpreter: pyspark
 WARN [2020-10-28 21:17:02,945] ({qtp89387388-712} NotebookServer.java[getEditorSetting]:2472) - Fail to get interpreter: pyspark
 WARN [2020-10-28 21:17:02,946] ({qtp89387388-712} NotebookServer.java[getEditorSetting]:2472) - Fail to get interpreter: pyspark
 INFO [2020-10-28 21:17:20,278] ({qtp89387388-66} LocalConfigStorage.java[save]:53) - Save Interpreter Setting to /users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/conf/interpreter.json
 WARN [2020-10-28 21:17:20,282] ({qtp89387388-66} InterpreterSettingManager.java[compare]:886) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-10-28 21:17:20,283] ({qtp89387388-66} InterpreterSettingManager.java[compare]:886) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-10-28 21:17:20,283] ({qtp89387388-66} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-10-28 21:17:20,283] ({qtp89387388-66} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-10-28 21:17:20,283] ({qtp89387388-66} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-10-28 21:17:20,283] ({qtp89387388-66} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-10-28 21:17:38,854] ({qtp89387388-24} SecurityRestApi.java[ticket]:88) - {"status":"OK","message":"","body":{"principal":"anonymous","ticket":"anonymous","roles":"[]"}}
 INFO [2020-10-28 21:17:39,081] ({qtp89387388-72} NotebookServer.java[onOpen]:151) - New connection from 127.0.0.1 : 44480
 INFO [2020-10-28 21:17:39,233] ({qtp89387388-712} NotebookServer.java[sendNote]:828) - New operation from 127.0.0.1 : 44480 : anonymous : GET_NOTE : 2FQA9JFA8
 WARN [2020-10-28 21:17:39,238] ({qtp89387388-712} GitNotebookRepo.java[revisionHistory]:158) - No Head found for 2FQA9JFA8, No HEAD exists and no explicit starting revision was specified
 WARN [2020-10-28 21:17:39,339] ({qtp89387388-318} InterpreterSettingManager.java[compare]:886) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-10-28 21:17:39,339] ({qtp89387388-318} InterpreterSettingManager.java[compare]:886) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-10-28 21:17:39,340] ({qtp89387388-318} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-10-28 21:17:39,340] ({qtp89387388-318} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-10-28 21:17:39,340] ({qtp89387388-318} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-10-28 21:17:39,340] ({qtp89387388-318} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 INFO [2020-10-28 21:18:34,170] ({qtp89387388-73} VFSNotebookRepo.java[save]:196) - Saving note:2FQA9JFA8
 INFO [2020-10-28 21:18:34,174] ({pool-2-thread-7} SchedulerFactory.java[jobStarted]:114) - Job 20201023-001936_119304475 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 21:18:34,175] ({pool-2-thread-7} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20201023-001936_119304475, interpreter: pyspark, note_id: 2FQA9JFA8, user: anonymous]
 INFO [2020-10-28 21:18:34,260] ({pool-2-thread-7} NotebookServer.java[afterStatusChange]:2314) - Job 20201023-001936_119304475 is finished successfully, status: FINISHED
 INFO [2020-10-28 21:18:34,303] ({pool-2-thread-7} VFSNotebookRepo.java[save]:196) - Saving note:2FQA9JFA8
 INFO [2020-10-28 21:18:34,307] ({pool-2-thread-7} SchedulerFactory.java[jobFinished]:120) - Job 20201023-001936_119304475 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 21:18:49,146] ({qtp89387388-66} VFSNotebookRepo.java[save]:196) - Saving note:2FQA9JFA8
 INFO [2020-10-28 21:18:49,155] ({pool-2-thread-21} SchedulerFactory.java[jobStarted]:114) - Job 20201023-001936_119304475 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 21:18:49,156] ({pool-2-thread-21} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20201023-001936_119304475, interpreter: pyspark, note_id: 2FQA9JFA8, user: anonymous]
 INFO [2020-10-28 21:18:49,225] ({pool-2-thread-21} NotebookServer.java[afterStatusChange]:2314) - Job 20201023-001936_119304475 is finished successfully, status: FINISHED
 INFO [2020-10-28 21:18:49,249] ({pool-2-thread-21} VFSNotebookRepo.java[save]:196) - Saving note:2FQA9JFA8
 INFO [2020-10-28 21:18:49,251] ({pool-2-thread-21} SchedulerFactory.java[jobFinished]:120) - Job 20201023-001936_119304475 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 21:18:57,045] ({qtp89387388-66} VFSNotebookRepo.java[save]:196) - Saving note:2FQA9JFA8
 INFO [2020-10-28 21:18:57,049] ({pool-2-thread-12} SchedulerFactory.java[jobStarted]:114) - Job 20171011-153126_91229243 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 21:18:57,050] ({pool-2-thread-12} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20171011-153126_91229243, interpreter: pyspark, note_id: 2FQA9JFA8, user: anonymous]
 INFO [2020-10-28 21:18:57,560] ({pool-2-thread-12} NotebookServer.java[afterStatusChange]:2314) - Job 20171011-153126_91229243 is finished successfully, status: FINISHED
 INFO [2020-10-28 21:18:57,586] ({pool-2-thread-12} VFSNotebookRepo.java[save]:196) - Saving note:2FQA9JFA8
 INFO [2020-10-28 21:18:57,590] ({pool-2-thread-12} SchedulerFactory.java[jobFinished]:120) - Job 20171011-153126_91229243 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 21:19:06,766] ({qtp89387388-72} VFSNotebookRepo.java[save]:196) - Saving note:2FQA9JFA8
 INFO [2020-10-28 21:19:06,770] ({pool-2-thread-22} SchedulerFactory.java[jobStarted]:114) - Job 20170830-114757_1684133948 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 21:19:06,770] ({pool-2-thread-22} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20170830-114757_1684133948, interpreter: pyspark, note_id: 2FQA9JFA8, user: anonymous]
 INFO [2020-10-28 21:19:06,781] ({pool-2-thread-22} NotebookServer.java[afterStatusChange]:2314) - Job 20170830-114757_1684133948 is finished successfully, status: FINISHED
 INFO [2020-10-28 21:19:06,812] ({pool-2-thread-22} VFSNotebookRepo.java[save]:196) - Saving note:2FQA9JFA8
 INFO [2020-10-28 21:19:06,815] ({pool-2-thread-22} SchedulerFactory.java[jobFinished]:120) - Job 20170830-114757_1684133948 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 21:21:57,550] ({qtp89387388-725} VFSNotebookRepo.java[save]:196) - Saving note:2FQA9JFA8
 INFO [2020-10-28 21:22:04,132] ({qtp89387388-726} VFSNotebookRepo.java[save]:196) - Saving note:2FQA9JFA8
 INFO [2020-10-28 21:22:04,136] ({pool-2-thread-5} SchedulerFactory.java[jobStarted]:114) - Job 20201023-001957_322623490 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 21:22:04,137] ({pool-2-thread-5} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20201023-001957_322623490, interpreter: pyspark, note_id: 2FQA9JFA8, user: anonymous]
 INFO [2020-10-28 21:22:04,514] ({pool-2-thread-5} NotebookServer.java[afterStatusChange]:2314) - Job 20201023-001957_322623490 is finished successfully, status: FINISHED
 INFO [2020-10-28 21:22:04,545] ({pool-2-thread-5} VFSNotebookRepo.java[save]:196) - Saving note:2FQA9JFA8
 INFO [2020-10-28 21:22:04,547] ({pool-2-thread-5} SchedulerFactory.java[jobFinished]:120) - Job 20201023-001957_322623490 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 21:22:07,758] ({qtp89387388-725} VFSNotebookRepo.java[save]:196) - Saving note:2FQA9JFA8
 INFO [2020-10-28 21:22:22,659] ({qtp89387388-725} VFSNotebookRepo.java[save]:196) - Saving note:2FQA9JFA8
 INFO [2020-10-28 21:22:22,713] ({qtp89387388-723} VFSNotebookRepo.java[save]:196) - Saving note:2FQA9JFA8
 INFO [2020-10-28 21:22:22,716] ({pool-2-thread-23} SchedulerFactory.java[jobStarted]:114) - Job 20201023-001957_322623490 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 21:22:22,719] ({pool-2-thread-23} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20201023-001957_322623490, interpreter: pyspark, note_id: 2FQA9JFA8, user: anonymous]
 WARN [2020-10-28 21:22:22,736] ({pool-2-thread-23} NotebookServer.java[afterStatusChange]:2316) - Job 20201023-001957_322623490 is finished, status: ERROR, exception: null, result: %text Traceback (most recent call last):
  File "/tmp/zeppelin_pyspark-3213863130067318734.py", line 364, in <module>
    code = compile('\n'.join(stmts), '<stdin>', 'exec', ast.PyCF_ONLY_AST, 1)
  File "<stdin>", line 4
    .flatMap(lambda line: *line) \
                          ^
SyntaxError: invalid syntax

 INFO [2020-10-28 21:22:22,764] ({pool-2-thread-23} VFSNotebookRepo.java[save]:196) - Saving note:2FQA9JFA8
 INFO [2020-10-28 21:22:22,767] ({pool-2-thread-23} SchedulerFactory.java[jobFinished]:120) - Job 20201023-001957_322623490 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 21:23:17,775] ({qtp89387388-712} VFSNotebookRepo.java[save]:196) - Saving note:2FQA9JFA8
 INFO [2020-10-28 21:23:17,831] ({qtp89387388-319} VFSNotebookRepo.java[save]:196) - Saving note:2FQA9JFA8
 INFO [2020-10-28 21:23:17,834] ({pool-2-thread-13} SchedulerFactory.java[jobStarted]:114) - Job 20201023-001957_322623490 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 21:23:17,835] ({pool-2-thread-13} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20201023-001957_322623490, interpreter: pyspark, note_id: 2FQA9JFA8, user: anonymous]
 INFO [2020-10-28 21:23:18,192] ({pool-2-thread-13} NotebookServer.java[afterStatusChange]:2314) - Job 20201023-001957_322623490 is finished successfully, status: FINISHED
 INFO [2020-10-28 21:23:18,218] ({pool-2-thread-13} VFSNotebookRepo.java[save]:196) - Saving note:2FQA9JFA8
 INFO [2020-10-28 21:23:18,221] ({pool-2-thread-13} SchedulerFactory.java[jobFinished]:120) - Job 20201023-001957_322623490 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 21:23:27,918] ({qtp89387388-318} VFSNotebookRepo.java[save]:196) - Saving note:2FQA9JFA8
 INFO [2020-10-28 21:24:10,958] ({qtp89387388-318} VFSNotebookRepo.java[save]:196) - Saving note:2FQA9JFA8
 INFO [2020-10-28 21:24:13,482] ({qtp89387388-720} VFSNotebookRepo.java[save]:196) - Saving note:2FQA9JFA8
 INFO [2020-10-28 21:24:13,486] ({pool-2-thread-24} SchedulerFactory.java[jobStarted]:114) - Job 20201023-001957_322623490 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 21:24:13,486] ({pool-2-thread-24} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20201023-001957_322623490, interpreter: pyspark, note_id: 2FQA9JFA8, user: anonymous]
 WARN [2020-10-28 21:24:13,493] ({pool-2-thread-24} NotebookServer.java[afterStatusChange]:2316) - Job 20201023-001957_322623490 is finished, status: ERROR, exception: null, result: %text Traceback (most recent call last):
  File "/tmp/zeppelin_pyspark-3213863130067318734.py", line 364, in <module>
    code = compile('\n'.join(stmts), '<stdin>', 'exec', ast.PyCF_ONLY_AST, 1)
  File "<stdin>", line 5
    .filter(lambda word: !word)
                         ^
SyntaxError: invalid syntax

 INFO [2020-10-28 21:24:13,519] ({pool-2-thread-24} VFSNotebookRepo.java[save]:196) - Saving note:2FQA9JFA8
 INFO [2020-10-28 21:24:13,522] ({pool-2-thread-24} SchedulerFactory.java[jobFinished]:120) - Job 20201023-001957_322623490 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 21:24:21,644] ({qtp89387388-721} VFSNotebookRepo.java[save]:196) - Saving note:2FQA9JFA8
 INFO [2020-10-28 21:24:21,708] ({qtp89387388-319} VFSNotebookRepo.java[save]:196) - Saving note:2FQA9JFA8
 INFO [2020-10-28 21:24:21,714] ({pool-2-thread-8} SchedulerFactory.java[jobStarted]:114) - Job 20201023-001957_322623490 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 21:24:21,714] ({pool-2-thread-8} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20201023-001957_322623490, interpreter: pyspark, note_id: 2FQA9JFA8, user: anonymous]
 INFO [2020-10-28 21:24:21,900] ({pool-2-thread-8} NotebookServer.java[afterStatusChange]:2314) - Job 20201023-001957_322623490 is finished successfully, status: FINISHED
 INFO [2020-10-28 21:24:21,930] ({pool-2-thread-8} VFSNotebookRepo.java[save]:196) - Saving note:2FQA9JFA8
 INFO [2020-10-28 21:24:21,933] ({pool-2-thread-8} SchedulerFactory.java[jobFinished]:120) - Job 20201023-001957_322623490 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 21:24:31,841] ({qtp89387388-319} VFSNotebookRepo.java[save]:196) - Saving note:2FQA9JFA8
 INFO [2020-10-28 21:24:53,296] ({qtp89387388-319} VFSNotebookRepo.java[save]:196) - Saving note:2FQA9JFA8
 INFO [2020-10-28 21:24:53,374] ({qtp89387388-721} VFSNotebookRepo.java[save]:196) - Saving note:2FQA9JFA8
 INFO [2020-10-28 21:24:53,377] ({pool-2-thread-25} SchedulerFactory.java[jobStarted]:114) - Job 20201023-001957_322623490 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 21:24:53,378] ({pool-2-thread-25} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20201023-001957_322623490, interpreter: pyspark, note_id: 2FQA9JFA8, user: anonymous]
 INFO [2020-10-28 21:24:53,649] ({pool-2-thread-25} NotebookServer.java[afterStatusChange]:2314) - Job 20201023-001957_322623490 is finished successfully, status: FINISHED
 INFO [2020-10-28 21:24:53,676] ({pool-2-thread-25} VFSNotebookRepo.java[save]:196) - Saving note:2FQA9JFA8
 INFO [2020-10-28 21:24:53,678] ({pool-2-thread-25} SchedulerFactory.java[jobFinished]:120) - Job 20201023-001957_322623490 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 21:24:59,015] ({qtp89387388-319} VFSNotebookRepo.java[save]:196) - Saving note:2FQA9JFA8
 INFO [2020-10-28 21:24:59,101] ({qtp89387388-319} VFSNotebookRepo.java[save]:196) - Saving note:2FQA9JFA8
 INFO [2020-10-28 21:24:59,105] ({pool-2-thread-14} SchedulerFactory.java[jobStarted]:114) - Job 20201023-001957_322623490 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 21:24:59,105] ({pool-2-thread-14} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20201023-001957_322623490, interpreter: pyspark, note_id: 2FQA9JFA8, user: anonymous]
 INFO [2020-10-28 21:24:59,536] ({pool-2-thread-14} NotebookServer.java[afterStatusChange]:2314) - Job 20201023-001957_322623490 is finished successfully, status: FINISHED
 INFO [2020-10-28 21:24:59,562] ({pool-2-thread-14} VFSNotebookRepo.java[save]:196) - Saving note:2FQA9JFA8
 INFO [2020-10-28 21:24:59,566] ({pool-2-thread-14} SchedulerFactory.java[jobFinished]:120) - Job 20201023-001957_322623490 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 21:25:09,161] ({qtp89387388-723} VFSNotebookRepo.java[save]:196) - Saving note:2FQA9JFA8
 INFO [2020-10-28 21:25:20,142] ({qtp89387388-319} VFSNotebookRepo.java[save]:196) - Saving note:2FQA9JFA8
 INFO [2020-10-28 21:25:20,224] ({qtp89387388-66} VFSNotebookRepo.java[save]:196) - Saving note:2FQA9JFA8
 INFO [2020-10-28 21:25:20,228] ({pool-2-thread-26} SchedulerFactory.java[jobStarted]:114) - Job 20201023-001957_322623490 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 21:25:20,229] ({pool-2-thread-26} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20201023-001957_322623490, interpreter: pyspark, note_id: 2FQA9JFA8, user: anonymous]
 INFO [2020-10-28 21:25:20,619] ({pool-2-thread-26} NotebookServer.java[afterStatusChange]:2314) - Job 20201023-001957_322623490 is finished successfully, status: FINISHED
 INFO [2020-10-28 21:25:20,647] ({pool-2-thread-26} VFSNotebookRepo.java[save]:196) - Saving note:2FQA9JFA8
 INFO [2020-10-28 21:25:20,650] ({pool-2-thread-26} SchedulerFactory.java[jobFinished]:120) - Job 20201023-001957_322623490 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 21:26:02,636] ({qtp89387388-66} VFSNotebookRepo.java[save]:196) - Saving note:2FQA9JFA8
 INFO [2020-10-28 21:26:02,721] ({qtp89387388-26} VFSNotebookRepo.java[save]:196) - Saving note:2FQA9JFA8
 INFO [2020-10-28 21:26:02,724] ({pool-2-thread-4} SchedulerFactory.java[jobStarted]:114) - Job 20201023-001957_322623490 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 21:26:02,725] ({pool-2-thread-4} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20201023-001957_322623490, interpreter: pyspark, note_id: 2FQA9JFA8, user: anonymous]
 INFO [2020-10-28 21:26:03,099] ({pool-2-thread-4} NotebookServer.java[afterStatusChange]:2314) - Job 20201023-001957_322623490 is finished successfully, status: FINISHED
 INFO [2020-10-28 21:26:03,128] ({pool-2-thread-4} VFSNotebookRepo.java[save]:196) - Saving note:2FQA9JFA8
 INFO [2020-10-28 21:26:03,131] ({pool-2-thread-4} SchedulerFactory.java[jobFinished]:120) - Job 20201023-001957_322623490 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 21:26:12,782] ({qtp89387388-721} VFSNotebookRepo.java[save]:196) - Saving note:2FQA9JFA8
 INFO [2020-10-28 21:26:41,355] ({qtp89387388-723} VFSNotebookRepo.java[save]:196) - Saving note:2FQA9JFA8
 INFO [2020-10-28 21:26:41,358] ({qtp89387388-723} NotebookServer.java[broadcastNewParagraph]:688) - Broadcasting paragraph on run call instead of note.
 WARN [2020-10-28 21:26:43,936] ({qtp89387388-721} NotebookServer.java[getEditorSetting]:2472) - Fail to get interpreter: m
 WARN [2020-10-28 21:26:44,862] ({qtp89387388-721} NotebookServer.java[getEditorSetting]:2472) - Fail to get interpreter: s
 INFO [2020-10-28 21:26:48,256] ({qtp89387388-721} VFSNotebookRepo.java[save]:196) - Saving note:2FQA9JFA8
 INFO [2020-10-28 21:26:48,351] ({qtp89387388-723} VFSNotebookRepo.java[save]:196) - Saving note:2FQA9JFA8
 INFO [2020-10-28 21:26:48,355] ({pool-2-thread-27} SchedulerFactory.java[jobStarted]:114) - Job 20201028-212641_1700175020 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-sh:shared_process-shared_session
 INFO [2020-10-28 21:26:48,356] ({pool-2-thread-27} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20201028-212641_1700175020, interpreter: sh, note_id: 2FQA9JFA8, user: anonymous]
 INFO [2020-10-28 21:26:48,373] ({pool-2-thread-27} NotebookServer.java[afterStatusChange]:2314) - Job 20201028-212641_1700175020 is finished successfully, status: FINISHED
 INFO [2020-10-28 21:26:48,401] ({pool-2-thread-27} VFSNotebookRepo.java[save]:196) - Saving note:2FQA9JFA8
 INFO [2020-10-28 21:26:48,404] ({pool-2-thread-27} SchedulerFactory.java[jobFinished]:120) - Job 20201028-212641_1700175020 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-sh:shared_process-shared_session
 INFO [2020-10-28 21:26:58,425] ({qtp89387388-816} VFSNotebookRepo.java[save]:196) - Saving note:2FQA9JFA8
 INFO [2020-10-28 21:27:06,976] ({qtp89387388-712} VFSNotebookRepo.java[save]:196) - Saving note:2FQA9JFA8
 INFO [2020-10-28 21:27:17,136] ({qtp89387388-712} VFSNotebookRepo.java[save]:196) - Saving note:2FQA9JFA8
 INFO [2020-10-28 21:27:30,056] ({qtp89387388-816} VFSNotebookRepo.java[save]:196) - Saving note:2FQA9JFA8
 INFO [2020-10-28 21:27:40,960] ({qtp89387388-720} VFSNotebookRepo.java[save]:196) - Saving note:2FQA9JFA8
 INFO [2020-10-28 21:27:49,085] ({qtp89387388-722} VFSNotebookRepo.java[save]:196) - Saving note:2FQA9JFA8
 INFO [2020-10-28 21:27:49,172] ({qtp89387388-722} VFSNotebookRepo.java[save]:196) - Saving note:2FQA9JFA8
 INFO [2020-10-28 21:27:49,176] ({pool-2-thread-15} SchedulerFactory.java[jobStarted]:114) - Job 20201028-212641_1700175020 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-sh:shared_process-shared_session
 INFO [2020-10-28 21:27:49,177] ({pool-2-thread-15} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20201028-212641_1700175020, interpreter: sh, note_id: 2FQA9JFA8, user: anonymous]
 INFO [2020-10-28 21:27:49,188] ({pool-2-thread-15} NotebookServer.java[afterStatusChange]:2314) - Job 20201028-212641_1700175020 is finished successfully, status: FINISHED
 INFO [2020-10-28 21:27:49,210] ({pool-2-thread-15} VFSNotebookRepo.java[save]:196) - Saving note:2FQA9JFA8
 INFO [2020-10-28 21:27:49,213] ({pool-2-thread-15} SchedulerFactory.java[jobFinished]:120) - Job 20201028-212641_1700175020 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-sh:shared_process-shared_session
 INFO [2020-10-28 21:27:51,128] ({qtp89387388-720} VFSNotebookRepo.java[save]:196) - Saving note:2FQA9JFA8
 INFO [2020-10-28 21:28:44,192] ({qtp89387388-722} VFSNotebookRepo.java[save]:196) - Saving note:2FQA9JFA8
 INFO [2020-10-28 21:28:44,195] ({qtp89387388-722} NotebookServer.java[broadcastNewParagraph]:688) - Broadcasting paragraph on run call instead of note.
 WARN [2020-10-28 21:28:47,621] ({qtp89387388-816} NotebookServer.java[getEditorSetting]:2472) - Fail to get interpreter: s
 WARN [2020-10-28 21:28:48,169] ({qtp89387388-816} NotebookServer.java[getEditorSetting]:2472) - Fail to get interpreter: p
 WARN [2020-10-28 21:28:48,445] ({qtp89387388-722} NotebookServer.java[getEditorSetting]:2472) - Fail to get interpreter: py
 WARN [2020-10-28 21:28:48,580] ({qtp89387388-816} NotebookServer.java[getEditorSetting]:2472) - Fail to get interpreter: pyt
 WARN [2020-10-28 21:28:48,672] ({qtp89387388-722} NotebookServer.java[getEditorSetting]:2472) - Fail to get interpreter: pyth
 WARN [2020-10-28 21:28:49,590] ({qtp89387388-319} NotebookServer.java[getEditorSetting]:2472) - Fail to get interpreter: pythi
 WARN [2020-10-28 21:28:50,066] ({qtp89387388-869} NotebookServer.java[getEditorSetting]:2472) - Fail to get interpreter: pyth
 WARN [2020-10-28 21:28:50,323] ({qtp89387388-319} NotebookServer.java[getEditorSetting]:2472) - Fail to get interpreter: pythoi
 WARN [2020-10-28 21:28:50,506] ({qtp89387388-869} NotebookServer.java[getEditorSetting]:2472) - Fail to get interpreter: pythoin
 WARN [2020-10-28 21:28:50,882] ({qtp89387388-319} NotebookServer.java[getEditorSetting]:2472) - Fail to get interpreter: pythoi
 WARN [2020-10-28 21:28:51,077] ({qtp89387388-869} NotebookServer.java[getEditorSetting]:2472) - Fail to get interpreter: pytho
 INFO [2020-10-28 21:28:51,315] ({qtp89387388-319} InterpreterSetting.java[getOrCreateInterpreterGroup]:419) - Create InterpreterGroup with groupId: python:shared_process for user: anonymous and note: 2FQA9JFA8
 INFO [2020-10-28 21:28:51,316] ({qtp89387388-319} InterpreterSetting.java[createInterpreters]:689) - Interpreter org.apache.zeppelin.python.PythonInterpreter created for user: anonymous, sessionId: shared_session
 INFO [2020-10-28 21:28:51,316] ({qtp89387388-319} InterpreterSetting.java[createInterpreters]:689) - Interpreter org.apache.zeppelin.python.IPythonInterpreter created for user: anonymous, sessionId: shared_session
 INFO [2020-10-28 21:28:51,316] ({qtp89387388-319} InterpreterSetting.java[createInterpreters]:689) - Interpreter org.apache.zeppelin.python.PythonInterpreterPandasSql created for user: anonymous, sessionId: shared_session
 INFO [2020-10-28 21:28:51,317] ({qtp89387388-319} InterpreterSetting.java[createInterpreters]:689) - Interpreter org.apache.zeppelin.python.PythonCondaInterpreter created for user: anonymous, sessionId: shared_session
 INFO [2020-10-28 21:28:51,317] ({qtp89387388-319} InterpreterSetting.java[createInterpreters]:689) - Interpreter org.apache.zeppelin.python.PythonDockerInterpreter created for user: anonymous, sessionId: shared_session
 INFO [2020-10-28 21:28:51,317] ({qtp89387388-319} ManagedInterpreterGroup.java[getOrCreateSession]:158) - Create Session: shared_session in InterpreterGroup: python:shared_process for user: anonymous
 INFO [2020-10-28 21:28:55,777] ({qtp89387388-869} VFSNotebookRepo.java[save]:196) - Saving note:2FQA9JFA8
 INFO [2020-10-28 21:29:05,930] ({qtp89387388-722} VFSNotebookRepo.java[save]:196) - Saving note:2FQA9JFA8
 INFO [2020-10-28 21:29:08,897] ({qtp89387388-722} VFSNotebookRepo.java[save]:196) - Saving note:2FQA9JFA8
 INFO [2020-10-28 21:29:13,034] ({qtp89387388-869} VFSNotebookRepo.java[save]:196) - Saving note:2FQA9JFA8
 INFO [2020-10-28 21:29:23,190] ({qtp89387388-319} VFSNotebookRepo.java[save]:196) - Saving note:2FQA9JFA8
 WARN [2020-10-28 21:29:40,118] ({qtp89387388-319} NotebookServer.java[getEditorSetting]:2472) - Fail to get interpreter: s
 INFO [2020-10-28 21:29:45,544] ({qtp89387388-319} VFSNotebookRepo.java[save]:196) - Saving note:2FQA9JFA8
 INFO [2020-10-28 21:30:02,809] ({qtp89387388-869} VFSNotebookRepo.java[save]:196) - Saving note:2FQA9JFA8
 INFO [2020-10-28 21:30:08,960] ({qtp89387388-319} VFSNotebookRepo.java[save]:196) - Saving note:2FQA9JFA8
 INFO [2020-10-28 21:30:09,041] ({qtp89387388-713} VFSNotebookRepo.java[save]:196) - Saving note:2FQA9JFA8
 INFO [2020-10-28 21:30:09,045] ({pool-2-thread-28} SchedulerFactory.java[jobStarted]:114) - Job 20201028-212844_1349602512 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-sh:shared_process-shared_session
 INFO [2020-10-28 21:30:09,046] ({pool-2-thread-28} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20201028-212844_1349602512, interpreter: sh, note_id: 2FQA9JFA8, user: anonymous]
 INFO [2020-10-28 21:30:09,080] ({pool-2-thread-28} NotebookServer.java[afterStatusChange]:2314) - Job 20201028-212844_1349602512 is finished successfully, status: FINISHED
 INFO [2020-10-28 21:30:09,103] ({pool-2-thread-28} VFSNotebookRepo.java[save]:196) - Saving note:2FQA9JFA8
 INFO [2020-10-28 21:30:09,106] ({pool-2-thread-28} SchedulerFactory.java[jobFinished]:120) - Job 20201028-212844_1349602512 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-sh:shared_process-shared_session
 INFO [2020-10-28 21:30:19,122] ({qtp89387388-319} VFSNotebookRepo.java[save]:196) - Saving note:2FQA9JFA8
 INFO [2020-10-28 21:30:34,521] ({qtp89387388-869} VFSNotebookRepo.java[save]:196) - Saving note:2FQA9JFA8
 INFO [2020-10-28 21:31:46,669] ({qtp89387388-319} VFSNotebookRepo.java[save]:196) - Saving note:2FQA9JFA8
 INFO [2020-10-28 21:31:46,730] ({qtp89387388-928} VFSNotebookRepo.java[save]:196) - Saving note:2FQA9JFA8
 INFO [2020-10-28 21:31:46,734] ({pool-2-thread-9} SchedulerFactory.java[jobStarted]:114) - Job 20191121-184701_1405603118 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 21:31:46,735] ({pool-2-thread-9} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20191121-184701_1405603118, interpreter: pyspark, note_id: 2FQA9JFA8, user: anonymous]
 WARN [2020-10-28 21:31:46,744] ({pool-2-thread-9} NotebookServer.java[afterStatusChange]:2316) - Job 20191121-184701_1405603118 is finished, status: ERROR, exception: null, result: %text Traceback (most recent call last):
  File "/tmp/zeppelin_pyspark-3213863130067318734.py", line 364, in <module>
    code = compile('\n'.join(stmts), '<stdin>', 'exec', ast.PyCF_ONLY_AST, 1)
  File "<stdin>", line 14
    .reduceByKey(n,m: n+m)
                    ^
SyntaxError: invalid syntax

 INFO [2020-10-28 21:31:46,768] ({pool-2-thread-9} VFSNotebookRepo.java[save]:196) - Saving note:2FQA9JFA8
 INFO [2020-10-28 21:31:46,771] ({pool-2-thread-9} SchedulerFactory.java[jobFinished]:120) - Job 20191121-184701_1405603118 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 21:31:56,807] ({qtp89387388-869} VFSNotebookRepo.java[save]:196) - Saving note:2FQA9JFA8
 INFO [2020-10-28 21:31:56,896] ({qtp89387388-319} VFSNotebookRepo.java[save]:196) - Saving note:2FQA9JFA8
 INFO [2020-10-28 21:31:56,899] ({pool-2-thread-29} SchedulerFactory.java[jobStarted]:114) - Job 20191121-184701_1405603118 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 21:31:56,900] ({pool-2-thread-29} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20191121-184701_1405603118, interpreter: pyspark, note_id: 2FQA9JFA8, user: anonymous]
 INFO [2020-10-28 21:31:57,239] ({pool-2-thread-29} NotebookServer.java[afterStatusChange]:2314) - Job 20191121-184701_1405603118 is finished successfully, status: FINISHED
 INFO [2020-10-28 21:31:57,264] ({pool-2-thread-29} VFSNotebookRepo.java[save]:196) - Saving note:2FQA9JFA8
 INFO [2020-10-28 21:31:57,267] ({pool-2-thread-29} SchedulerFactory.java[jobFinished]:120) - Job 20191121-184701_1405603118 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 21:32:06,955] ({qtp89387388-712} VFSNotebookRepo.java[save]:196) - Saving note:2FQA9JFA8
 WARN [2020-10-28 21:32:42,470] ({qtp89387388-869} InterpreterSettingManager.java[compare]:886) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-10-28 21:32:42,470] ({qtp89387388-869} InterpreterSettingManager.java[compare]:886) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-10-28 21:32:42,470] ({qtp89387388-869} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-10-28 21:32:42,470] ({qtp89387388-869} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-10-28 21:32:42,470] ({qtp89387388-869} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-10-28 21:32:42,470] ({qtp89387388-869} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 INFO [2020-10-28 21:32:42,471] ({qtp89387388-869} LocalConfigStorage.java[save]:53) - Save Interpreter Setting to /users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/conf/interpreter.json
 INFO [2020-10-28 21:32:42,474] ({qtp89387388-869} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 21:32:42,476] ({qtp89387388-869} FolderView.java[onNoteNameChanged]:205) - Note name changed: 2FP83VA8P -> Diplodatos/Clase 02 - Spark Core
 INFO [2020-10-28 21:32:42,477] ({qtp89387388-869} Folder.java[addNote]:185) - Add note 2FP83VA8P to folder Diplodatos
 INFO [2020-10-28 21:32:42,525] ({qtp89387388-869} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 21:32:42,560] ({qtp89387388-869} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 21:32:52,978] ({qtp89387388-319} NotebookServer.java[sendNote]:828) - New operation from 127.0.0.1 : 44480 : anonymous : GET_NOTE : 2FP83VA8P
 WARN [2020-10-28 21:32:52,987] ({qtp89387388-319} GitNotebookRepo.java[revisionHistory]:158) - No Head found for 2FP83VA8P, No HEAD exists and no explicit starting revision was specified
 WARN [2020-10-28 21:32:53,036] ({qtp89387388-712} InterpreterSettingManager.java[compare]:886) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-10-28 21:32:53,036] ({qtp89387388-712} InterpreterSettingManager.java[compare]:886) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-10-28 21:32:53,036] ({qtp89387388-712} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-10-28 21:32:53,036] ({qtp89387388-712} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-10-28 21:32:53,037] ({qtp89387388-712} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-10-28 21:32:53,037] ({qtp89387388-712} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 INFO [2020-10-28 21:33:14,381] ({qtp89387388-26} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 21:33:14,391] ({pool-2-thread-16} SchedulerFactory.java[jobStarted]:114) - Job 20201023-002107_2147167260 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 21:33:14,391] ({pool-2-thread-16} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20201023-002107_2147167260, interpreter: pyspark, note_id: 2FP83VA8P, user: anonymous]
 INFO [2020-10-28 21:33:14,951] ({pool-2-thread-16} NotebookServer.java[afterStatusChange]:2314) - Job 20201023-002107_2147167260 is finished successfully, status: FINISHED
 INFO [2020-10-28 21:33:14,985] ({pool-2-thread-16} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 21:33:14,991] ({pool-2-thread-16} SchedulerFactory.java[jobFinished]:120) - Job 20201023-002107_2147167260 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 21:35:51,105] ({qtp89387388-975} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 21:35:51,110] ({pool-2-thread-30} SchedulerFactory.java[jobStarted]:114) - Job 20201023-002121_604229819 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 21:35:51,111] ({pool-2-thread-30} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20201023-002121_604229819, interpreter: pyspark, note_id: 2FP83VA8P, user: anonymous]
 INFO [2020-10-28 21:35:51,313] ({pool-2-thread-30} NotebookServer.java[afterStatusChange]:2314) - Job 20201023-002121_604229819 is finished successfully, status: FINISHED
 INFO [2020-10-28 21:35:51,350] ({pool-2-thread-30} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 21:35:51,354] ({pool-2-thread-30} SchedulerFactory.java[jobFinished]:120) - Job 20201023-002121_604229819 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 21:39:31,387] ({qtp89387388-66} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 21:39:31,483] ({qtp89387388-1038} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 21:39:31,489] ({pool-2-thread-6} SchedulerFactory.java[jobStarted]:114) - Job 20171013-175507_696892344 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 21:39:31,490] ({pool-2-thread-6} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20171013-175507_696892344, interpreter: pyspark, note_id: 2FP83VA8P, user: anonymous]
 WARN [2020-10-28 21:39:31,500] ({pool-2-thread-6} NotebookServer.java[afterStatusChange]:2316) - Job 20171013-175507_696892344 is finished, status: ERROR, exception: null, result: %text Traceback (most recent call last):
  File "/tmp/zeppelin_pyspark-3213863130067318734.py", line 364, in <module>
    code = compile('\n'.join(stmts), '<stdin>', 'exec', ast.PyCF_ONLY_AST, 1)
  File "<stdin>", line 7
    filter(lambda car: car=='c')
         ^
SyntaxError: invalid syntax

 INFO [2020-10-28 21:39:31,552] ({pool-2-thread-6} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 21:39:31,557] ({pool-2-thread-6} SchedulerFactory.java[jobFinished]:120) - Job 20171013-175507_696892344 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 21:39:38,044] ({qtp89387388-66} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 21:39:38,132] ({qtp89387388-722} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 21:39:38,136] ({pool-2-thread-31} SchedulerFactory.java[jobStarted]:114) - Job 20171013-175507_696892344 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 21:39:38,137] ({pool-2-thread-31} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20171013-175507_696892344, interpreter: pyspark, note_id: 2FP83VA8P, user: anonymous]
 INFO [2020-10-28 21:39:38,740] ({pool-2-thread-31} NotebookServer.java[afterStatusChange]:2314) - Job 20171013-175507_696892344 is finished successfully, status: FINISHED
 INFO [2020-10-28 21:39:38,771] ({pool-2-thread-31} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 21:39:38,775] ({pool-2-thread-31} SchedulerFactory.java[jobFinished]:120) - Job 20171013-175507_696892344 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 21:40:15,718] ({qtp89387388-1039} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 21:40:15,723] ({pool-2-thread-17} SchedulerFactory.java[jobStarted]:114) - Job 20201023-002135_640378642 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 21:40:15,723] ({pool-2-thread-17} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20201023-002135_640378642, interpreter: pyspark, note_id: 2FP83VA8P, user: anonymous]
 INFO [2020-10-28 21:40:15,919] ({pool-2-thread-17} NotebookServer.java[afterStatusChange]:2314) - Job 20201023-002135_640378642 is finished successfully, status: FINISHED
 INFO [2020-10-28 21:40:15,953] ({pool-2-thread-17} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 21:40:15,957] ({pool-2-thread-17} SchedulerFactory.java[jobFinished]:120) - Job 20201023-002135_640378642 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 21:40:26,853] ({qtp89387388-722} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 21:40:26,857] ({pool-2-thread-32} SchedulerFactory.java[jobStarted]:114) - Job 20201023-002155_1881617494 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 21:40:26,858] ({pool-2-thread-32} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20201023-002155_1881617494, interpreter: pyspark, note_id: 2FP83VA8P, user: anonymous]
 INFO [2020-10-28 21:40:27,091] ({pool-2-thread-32} NotebookServer.java[afterStatusChange]:2314) - Job 20201023-002155_1881617494 is finished successfully, status: FINISHED
 INFO [2020-10-28 21:40:27,126] ({pool-2-thread-32} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 21:40:27,130] ({pool-2-thread-32} SchedulerFactory.java[jobFinished]:120) - Job 20201023-002155_1881617494 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 21:41:54,649] ({qtp89387388-66} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 21:42:36,160] ({qtp89387388-1039} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 21:42:36,264] ({qtp89387388-72} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 21:42:36,269] ({pool-2-thread-10} SchedulerFactory.java[jobStarted]:114) - Job 20191123-214023_2104486544 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 21:42:36,270] ({pool-2-thread-10} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20191123-214023_2104486544, interpreter: pyspark, note_id: 2FP83VA8P, user: anonymous]
 INFO [2020-10-28 21:42:36,783] ({pool-2-thread-10} NotebookServer.java[afterStatusChange]:2314) - Job 20191123-214023_2104486544 is finished successfully, status: FINISHED
 INFO [2020-10-28 21:42:36,814] ({pool-2-thread-10} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 21:42:36,818] ({pool-2-thread-10} SchedulerFactory.java[jobFinished]:120) - Job 20191123-214023_2104486544 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 21:44:35,853] ({qtp89387388-66} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 21:45:02,035] ({qtp89387388-1039} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 21:45:15,029] ({qtp89387388-1056} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 21:45:15,099] ({qtp89387388-66} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 21:45:15,107] ({pool-2-thread-33} SchedulerFactory.java[jobStarted]:114) - Job 20191124-133441_1910745321 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 21:45:15,108] ({pool-2-thread-33} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20191124-133441_1910745321, interpreter: pyspark, note_id: 2FP83VA8P, user: anonymous]
 WARN [2020-10-28 21:45:15,115] ({pool-2-thread-33} NotebookServer.java[afterStatusChange]:2316) - Job 20191124-133441_1910745321 is finished, status: ERROR, exception: null, result: %text Traceback (most recent call last):
  File "/tmp/zeppelin_pyspark-3213863130067318734.py", line 364, in <module>
    code = compile('\n'.join(stmts), '<stdin>', 'exec', ast.PyCF_ONLY_AST, 1)
  File "<stdin>", line 15
    print("redireccionados = {}%".format(float(nRedir) * 100 / nTotal))) # Completar
                                                                       ^
SyntaxError: invalid syntax

 INFO [2020-10-28 21:45:15,148] ({pool-2-thread-33} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 21:45:15,152] ({pool-2-thread-33} SchedulerFactory.java[jobFinished]:120) - Job 20191124-133441_1910745321 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 21:45:20,333] ({qtp89387388-1039} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 21:45:20,401] ({qtp89387388-1056} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 21:45:20,406] ({pool-2-thread-18} SchedulerFactory.java[jobStarted]:114) - Job 20191124-133441_1910745321 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 21:45:20,407] ({pool-2-thread-18} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20191124-133441_1910745321, interpreter: pyspark, note_id: 2FP83VA8P, user: anonymous]
 WARN [2020-10-28 21:45:20,463] ({pool-2-thread-18} NotebookServer.java[afterStatusChange]:2316) - Job 20191124-133441_1910745321 is finished, status: ERROR, exception: null, result: %text Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/... /diplodatos_bigdata/ds/flights.csv
	at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)
	at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)
	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)
	at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:199)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:250)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:250)
	at org.apache.spark.api.python.PythonRDD.getPartitions(PythonRDD.scala:53)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:250)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:935)
	at org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:467)
	at org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)
	at sun.reflect.GeneratedMethodAccessor150.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:280)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:745)

(<class 'py4j.protocol.Py4JJavaError'>, Py4JJavaError('An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n', JavaObject id=o1288), <traceback object at 0x7f4ac5ff3848>)
 INFO [2020-10-28 21:45:20,496] ({pool-2-thread-18} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 21:45:20,500] ({pool-2-thread-18} SchedulerFactory.java[jobFinished]:120) - Job 20191124-133441_1910745321 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 21:45:29,700] ({qtp89387388-72} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 21:45:29,792] ({qtp89387388-1039} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 21:45:29,798] ({pool-2-thread-34} SchedulerFactory.java[jobStarted]:114) - Job 20191124-133441_1910745321 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 21:45:29,799] ({pool-2-thread-34} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20191124-133441_1910745321, interpreter: pyspark, note_id: 2FP83VA8P, user: anonymous]
 WARN [2020-10-28 21:45:29,854] ({pool-2-thread-34} NotebookServer.java[afterStatusChange]:2316) - Job 20191124-133441_1910745321 is finished, status: ERROR, exception: null, result: %text Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/... /diplodatos_bigdata/ds/flights.csv
	at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)
	at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)
	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)
	at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:199)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:250)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:250)
	at org.apache.spark.api.python.PythonRDD.getPartitions(PythonRDD.scala:53)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:250)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:935)
	at org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:467)
	at org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)
	at sun.reflect.GeneratedMethodAccessor150.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:280)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:745)

(<class 'py4j.protocol.Py4JJavaError'>, Py4JJavaError('An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n', JavaObject id=o1337), <traceback object at 0x7f4ac5fb7d48>)
 INFO [2020-10-28 21:45:29,892] ({pool-2-thread-34} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 21:45:29,896] ({pool-2-thread-34} SchedulerFactory.java[jobFinished]:120) - Job 20191124-133441_1910745321 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 21:45:50,347] ({qtp89387388-1056} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 21:45:50,413] ({qtp89387388-66} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 21:45:50,418] ({pool-2-thread-3} SchedulerFactory.java[jobStarted]:114) - Job 20191124-133441_1910745321 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 21:45:50,419] ({pool-2-thread-3} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20191124-133441_1910745321, interpreter: pyspark, note_id: 2FP83VA8P, user: anonymous]
 INFO [2020-10-28 21:45:51,251] ({pool-2-thread-3} NotebookServer.java[afterStatusChange]:2314) - Job 20191124-133441_1910745321 is finished successfully, status: FINISHED
 INFO [2020-10-28 21:45:51,284] ({pool-2-thread-3} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 21:45:51,288] ({pool-2-thread-3} SchedulerFactory.java[jobFinished]:120) - Job 20191124-133441_1910745321 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 21:46:24,535] ({qtp89387388-1056} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 21:46:24,538] ({qtp89387388-1056} NotebookServer.java[broadcastNewParagraph]:688) - Broadcasting paragraph on run call instead of note.
 WARN [2020-10-28 21:46:35,813] ({qtp89387388-1141} NotebookServer.java[getEditorSetting]:2472) - Fail to get interpreter: m
 WARN [2020-10-28 21:46:38,286] ({qtp89387388-1039} NotebookServer.java[getEditorSetting]:2472) - Fail to get interpreter: p
 WARN [2020-10-28 21:46:38,618] ({qtp89387388-1141} NotebookServer.java[getEditorSetting]:2472) - Fail to get interpreter: py
 WARN [2020-10-28 21:46:38,819] ({qtp89387388-1039} NotebookServer.java[getEditorSetting]:2472) - Fail to get interpreter: pys
 WARN [2020-10-28 21:46:39,192] ({qtp89387388-1141} NotebookServer.java[getEditorSetting]:2472) - Fail to get interpreter: pysp
 WARN [2020-10-28 21:46:39,303] ({qtp89387388-1141} NotebookServer.java[getEditorSetting]:2472) - Fail to get interpreter: pyspa
 WARN [2020-10-28 21:46:39,612] ({qtp89387388-1056} NotebookServer.java[getEditorSetting]:2472) - Fail to get interpreter: pyspar
 INFO [2020-10-28 21:46:43,574] ({qtp89387388-1056} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 21:46:52,836] ({qtp89387388-1039} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 21:46:58,455] ({qtp89387388-1141} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 21:48:39,094] ({qtp89387388-1189} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 21:48:48,798] ({qtp89387388-1189} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 21:48:49,566] ({qtp89387388-1039} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 21:48:49,571] ({pool-2-thread-35} SchedulerFactory.java[jobStarted]:114) - Job 20201028-214624_69997796 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 21:48:49,571] ({pool-2-thread-35} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20201028-214624_69997796, interpreter: pyspark, note_id: 2FP83VA8P, user: anonymous]
 WARN [2020-10-28 21:48:49,615] ({pool-2-thread-35} NotebookServer.java[afterStatusChange]:2316) - Job 20201028-214624_69997796 is finished, status: ERROR, exception: null, result: %text Fail to execute line 4: grouped.show()
Traceback (most recent call last):
  File "/tmp/zeppelin_pyspark-3213863130067318734.py", line 380, in <module>
    exec(code, _zcUserQueryNameSpace)
  File "<stdin>", line 4, in <module>
AttributeError: 'PipelinedRDD' object has no attribute 'show'

 INFO [2020-10-28 21:48:49,653] ({pool-2-thread-35} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 21:48:49,657] ({pool-2-thread-35} SchedulerFactory.java[jobFinished]:120) - Job 20201028-214624_69997796 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 21:49:22,631] ({qtp89387388-1141} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 21:49:22,731] ({qtp89387388-1056} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 21:49:22,736] ({pool-2-thread-19} SchedulerFactory.java[jobStarted]:114) - Job 20201028-214624_69997796 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 21:49:22,736] ({pool-2-thread-19} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20201028-214624_69997796, interpreter: pyspark, note_id: 2FP83VA8P, user: anonymous]
 WARN [2020-10-28 21:49:22,872] ({pool-2-thread-19} NotebookServer.java[afterStatusChange]:2316) - Job 20201028-214624_69997796 is finished, status: ERROR, exception: null, result: %text Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 113.0 failed 1 times, most recent failure: Lost task 0.0 in stage 113.0 (TID 2483, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/worker.py", line 177, in main
    process()
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/worker.py", line 172, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/rdd.py", line 2423, in pipeline_func
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/rdd.py", line 346, in func
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/rdd.py", line 1842, in combineLocally
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/shuffle.py", line 236, in mergeValues
    for k, v in iterator:
ValueError: too many values to unpack (expected 2)

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:404)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:935)
	at org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:467)
	at org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)
	at sun.reflect.GeneratedMethodAccessor150.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:280)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/worker.py", line 177, in main
    process()
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/worker.py", line 172, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/rdd.py", line 2423, in pipeline_func
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/rdd.py", line 346, in func
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/rdd.py", line 1842, in combineLocally
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/shuffle.py", line 236, in mergeValues
    for k, v in iterator:
ValueError: too many values to unpack (expected 2)

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:404)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	... 1 more

(<class 'py4j.protocol.Py4JJavaError'>, Py4JJavaError('An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n', JavaObject id=o1453), <traceback object at 0x7f4ac5fbfcc8>)
 INFO [2020-10-28 21:49:22,904] ({pool-2-thread-19} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 21:49:22,908] ({pool-2-thread-19} SchedulerFactory.java[jobFinished]:120) - Job 20201028-214624_69997796 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 21:52:11,555] ({qtp89387388-1056} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 21:52:24,092] ({qtp89387388-1245} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 21:53:09,244] ({qtp89387388-1056} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 21:53:09,334] ({qtp89387388-1245} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 21:53:09,339] ({pool-2-thread-36} SchedulerFactory.java[jobStarted]:114) - Job 20201028-214624_69997796 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 21:53:09,340] ({pool-2-thread-36} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20201028-214624_69997796, interpreter: pyspark, note_id: 2FP83VA8P, user: anonymous]
 WARN [2020-10-28 21:53:09,795] ({pool-2-thread-36} NotebookServer.java[afterStatusChange]:2316) - Job 20201028-214624_69997796 is finished, status: ERROR, exception: null, result: %text Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 115.0 failed 1 times, most recent failure: Lost task 1.0 in stage 115.0 (TID 2486, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/worker.py", line 177, in main
    process()
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/worker.py", line 172, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/rdd.py", line 2423, in pipeline_func
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/rdd.py", line 2423, in pipeline_func
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/rdd.py", line 346, in func
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/rdd.py", line 1842, in combineLocally
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/shuffle.py", line 236, in mergeValues
    for k, v in iterator:
ValueError: too many values to unpack (expected 2)

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:404)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:935)
	at org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:467)
	at org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)
	at sun.reflect.GeneratedMethodAccessor150.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:280)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/worker.py", line 177, in main
    process()
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/worker.py", line 172, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/rdd.py", line 2423, in pipeline_func
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/rdd.py", line 2423, in pipeline_func
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/rdd.py", line 346, in func
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/rdd.py", line 1842, in combineLocally
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/shuffle.py", line 236, in mergeValues
    for k, v in iterator:
ValueError: too many values to unpack (expected 2)

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:404)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	... 1 more

(<class 'py4j.protocol.Py4JJavaError'>, Py4JJavaError('An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n', JavaObject id=o1525), <traceback object at 0x7f4ac5fc5708>)
 INFO [2020-10-28 21:53:09,829] ({pool-2-thread-36} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 21:53:09,834] ({pool-2-thread-36} SchedulerFactory.java[jobFinished]:120) - Job 20201028-214624_69997796 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 21:53:41,896] ({qtp89387388-1245} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 21:53:41,968] ({qtp89387388-1056} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 21:53:41,973] ({pool-2-thread-11} SchedulerFactory.java[jobStarted]:114) - Job 20201028-214624_69997796 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 21:53:41,974] ({pool-2-thread-11} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20201028-214624_69997796, interpreter: pyspark, note_id: 2FP83VA8P, user: anonymous]
 WARN [2020-10-28 21:53:42,501] ({pool-2-thread-11} NotebookServer.java[afterStatusChange]:2316) - Job 20201028-214624_69997796 is finished, status: ERROR, exception: null, result: %text Fail to execute line 5: airtime.max().collect()
Traceback (most recent call last):
  File "/tmp/zeppelin_pyspark-3213863130067318734.py", line 380, in <module>
    exec(code, _zcUserQueryNameSpace)
  File "<stdin>", line 5, in <module>
AttributeError: 'list' object has no attribute 'collect'

 INFO [2020-10-28 21:53:42,534] ({pool-2-thread-11} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 21:53:42,538] ({pool-2-thread-11} SchedulerFactory.java[jobFinished]:120) - Job 20201028-214624_69997796 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 21:53:51,021] ({qtp89387388-1250} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 21:53:51,079] ({qtp89387388-1245} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 21:53:51,084] ({pool-2-thread-37} SchedulerFactory.java[jobStarted]:114) - Job 20201028-214624_69997796 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 21:53:51,085] ({pool-2-thread-37} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20201028-214624_69997796, interpreter: pyspark, note_id: 2FP83VA8P, user: anonymous]
 INFO [2020-10-28 21:53:51,631] ({pool-2-thread-37} NotebookServer.java[afterStatusChange]:2314) - Job 20201028-214624_69997796 is finished successfully, status: FINISHED
 INFO [2020-10-28 21:53:51,664] ({pool-2-thread-37} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 21:53:51,668] ({pool-2-thread-37} SchedulerFactory.java[jobFinished]:120) - Job 20201028-214624_69997796 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 21:54:52,552] ({qtp89387388-1250} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 21:54:52,647] ({qtp89387388-1056} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 21:54:52,652] ({pool-2-thread-20} SchedulerFactory.java[jobStarted]:114) - Job 20201028-214624_69997796 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 21:54:52,652] ({pool-2-thread-20} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20201028-214624_69997796, interpreter: pyspark, note_id: 2FP83VA8P, user: anonymous]
 INFO [2020-10-28 21:54:59,405] ({pool-2-thread-20} NotebookServer.java[afterStatusChange]:2314) - Job 20201028-214624_69997796 is finished successfully, status: FINISHED
 INFO [2020-10-28 21:54:59,475] ({pool-2-thread-20} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 WARN [2020-10-28 21:54:59,735] ({pool-6-thread-1} AppendOutputRunner.java[run]:88) - Processing time for buffered append-output is high: 13 milliseconds.
 WARN [2020-10-28 21:55:01,640] ({pool-6-thread-1} AppendOutputRunner.java[run]:104) - Processing size for buffered append-output is high: 18619672 characters.
 INFO [2020-10-28 21:55:02,306] ({pool-2-thread-20} SchedulerFactory.java[jobFinished]:120) - Job 20201028-214624_69997796 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 21:55:56,654] ({qtp89387388-1250} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 21:55:59,030] ({qtp89387388-1250} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 21:56:02,188] ({pool-2-thread-38} SchedulerFactory.java[jobStarted]:114) - Job 20201028-214624_69997796 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 21:56:02,189] ({pool-2-thread-38} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20201028-214624_69997796, interpreter: pyspark, note_id: 2FP83VA8P, user: anonymous]
 INFO [2020-10-28 21:56:04,206] ({qtp89387388-1228} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 21:56:04,213] ({pool-2-thread-7} SchedulerFactory.java[jobStarted]:114) - Job 20201028-214624_69997796 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 21:56:04,214] ({pool-2-thread-7} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20201028-214624_69997796, interpreter: pyspark, note_id: 2FP83VA8P, user: anonymous]
 INFO [2020-10-28 21:56:06,385] ({pool-2-thread-38} NotebookServer.java[afterStatusChange]:2314) - Job 20201028-214624_69997796 is finished successfully, status: FINISHED
 INFO [2020-10-28 21:56:06,415] ({pool-2-thread-38} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 WARN [2020-10-28 21:56:06,765] ({pool-6-thread-1} AppendOutputRunner.java[run]:88) - Processing time for buffered append-output is high: 132 milliseconds.
 WARN [2020-10-28 21:56:08,530] ({pool-6-thread-1} AppendOutputRunner.java[run]:104) - Processing size for buffered append-output is high: 18619672 characters.
 INFO [2020-10-28 21:56:09,190] ({pool-2-thread-38} SchedulerFactory.java[jobFinished]:120) - Job 20201028-214624_69997796 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 21:56:11,005] ({pool-2-thread-7} SchedulerFactory.java[jobFinished]:120) - Job 20201028-214624_69997796 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 WARN [2020-10-28 21:56:11,234] ({pool-6-thread-1} AppendOutputRunner.java[run]:88) - Processing time for buffered append-output is high: 14 milliseconds.
 WARN [2020-10-28 21:56:12,700] ({pool-6-thread-1} AppendOutputRunner.java[run]:104) - Processing size for buffered append-output is high: 18619672 characters.
 INFO [2020-10-28 21:56:18,054] ({qtp89387388-1228} NotebookServer.java[onClose]:372) - Closed connection to 127.0.0.1 : 43830. (1001) null
 INFO [2020-10-28 21:58:14,833] ({qtp89387388-928} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 21:58:20,900] ({qtp89387388-928} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 21:58:24,759] ({pool-2-thread-39} SchedulerFactory.java[jobStarted]:114) - Job 20201028-214624_69997796 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 21:58:24,760] ({pool-2-thread-39} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20201028-214624_69997796, interpreter: pyspark, note_id: 2FP83VA8P, user: anonymous]
 INFO [2020-10-28 21:58:24,792] ({qtp89387388-928} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 WARN [2020-10-28 21:58:24,838] ({pool-2-thread-39} NotebookServer.java[afterStatusChange]:2316) - Job 20201028-214624_69997796 is finished, status: ERROR, exception: null, result: %text Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/file.csv
	at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)
	at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)
	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)
	at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:199)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:250)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:250)
	at org.apache.spark.api.python.PythonRDD.getPartitions(PythonRDD.scala:53)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:250)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:935)
	at org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:467)
	at org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)
	at sun.reflect.GeneratedMethodAccessor150.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:280)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:745)

(<class 'py4j.protocol.Py4JJavaError'>, Py4JJavaError('An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n', JavaObject id=o1676), <traceback object at 0x7f4ab8955148>)
 INFO [2020-10-28 21:58:24,899] ({pool-2-thread-39} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 21:58:24,903] ({pool-2-thread-39} SchedulerFactory.java[jobFinished]:120) - Job 20201028-214624_69997796 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 21:58:24,904] ({pool-2-thread-21} SchedulerFactory.java[jobStarted]:114) - Job 20201028-214624_69997796 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 21:58:24,905] ({pool-2-thread-21} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20201028-214624_69997796, interpreter: pyspark, note_id: 2FP83VA8P, user: anonymous]
 INFO [2020-10-28 21:58:24,971] ({pool-2-thread-21} SchedulerFactory.java[jobFinished]:120) - Job 20201028-214624_69997796 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 21:58:32,629] ({qtp89387388-722} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 21:58:32,634] ({pool-2-thread-40} SchedulerFactory.java[jobStarted]:114) - Job 20201028-214624_69997796 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 21:58:32,635] ({pool-2-thread-40} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20201028-214624_69997796, interpreter: pyspark, note_id: 2FP83VA8P, user: anonymous]
 WARN [2020-10-28 21:58:32,743] ({pool-2-thread-40} NotebookServer.java[afterStatusChange]:2316) - Job 20201028-214624_69997796 is finished, status: ERROR, exception: null, result: %text Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/file.csv
	at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)
	at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)
	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)
	at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:199)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:250)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:250)
	at org.apache.spark.api.python.PythonRDD.getPartitions(PythonRDD.scala:53)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:250)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:935)
	at org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:467)
	at org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)
	at sun.reflect.GeneratedMethodAccessor150.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:280)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:745)

(<class 'py4j.protocol.Py4JJavaError'>, Py4JJavaError('An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n', JavaObject id=o1778), <traceback object at 0x7f4ab89541c8>)
 INFO [2020-10-28 21:58:32,783] ({pool-2-thread-40} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 21:58:32,788] ({pool-2-thread-40} SchedulerFactory.java[jobFinished]:120) - Job 20201028-214624_69997796 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 21:59:08,743] ({qtp89387388-1322} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 21:59:08,858] ({qtp89387388-928} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 21:59:08,867] ({pool-2-thread-12} SchedulerFactory.java[jobStarted]:114) - Job 20201028-214624_69997796 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 21:59:08,868] ({pool-2-thread-12} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20201028-214624_69997796, interpreter: pyspark, note_id: 2FP83VA8P, user: anonymous]
 WARN [2020-10-28 21:59:08,969] ({pool-2-thread-12} NotebookServer.java[afterStatusChange]:2316) - Job 20201028-214624_69997796 is finished, status: ERROR, exception: null, result: %text Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 122.0 failed 1 times, most recent failure: Lost task 0.0 in stage 122.0 (TID 2497, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/worker.py", line 177, in main
    process()
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/worker.py", line 172, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/serializers.py", line 268, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "<stdin>", line 4, in <lambda>
TypeError: 'builtin_function_or_method' object is not subscriptable

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:935)
	at org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:467)
	at org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)
	at sun.reflect.GeneratedMethodAccessor150.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:280)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/worker.py", line 177, in main
    process()
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/worker.py", line 172, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/serializers.py", line 268, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "<stdin>", line 4, in <lambda>
TypeError: 'builtin_function_or_method' object is not subscriptable

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	... 1 more

(<class 'py4j.protocol.Py4JJavaError'>, Py4JJavaError('An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n', JavaObject id=o1827), <traceback object at 0x7f4aaf35c608>)
 INFO [2020-10-28 21:59:09,000] ({pool-2-thread-12} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 21:59:09,004] ({pool-2-thread-12} SchedulerFactory.java[jobFinished]:120) - Job 20201028-214624_69997796 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 21:59:17,499] ({qtp89387388-1322} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 21:59:17,590] ({qtp89387388-928} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 21:59:17,595] ({pool-2-thread-41} SchedulerFactory.java[jobStarted]:114) - Job 20201028-214624_69997796 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 21:59:17,595] ({pool-2-thread-41} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20201028-214624_69997796, interpreter: pyspark, note_id: 2FP83VA8P, user: anonymous]
 INFO [2020-10-28 21:59:22,364] ({pool-2-thread-41} NotebookServer.java[afterStatusChange]:2314) - Job 20201028-214624_69997796 is finished successfully, status: FINISHED
 INFO [2020-10-28 21:59:22,465] ({pool-2-thread-41} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 WARN [2020-10-28 21:59:22,530] ({pool-6-thread-1} AppendOutputRunner.java[run]:88) - Processing time for buffered append-output is high: 14 milliseconds.
 WARN [2020-10-28 21:59:24,281] ({pool-6-thread-1} AppendOutputRunner.java[run]:104) - Processing size for buffered append-output is high: 18619672 characters.
 INFO [2020-10-28 21:59:24,986] ({pool-2-thread-41} SchedulerFactory.java[jobFinished]:120) - Job 20201028-214624_69997796 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 22:00:13,739] ({qtp89387388-1039} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 22:00:45,259] ({qtp89387388-1382} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 22:00:47,609] ({qtp89387388-1382} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 22:00:51,358] ({pool-2-thread-22} SchedulerFactory.java[jobStarted]:114) - Job 20201028-214624_69997796 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 22:00:51,359] ({pool-2-thread-22} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20201028-214624_69997796, interpreter: pyspark, note_id: 2FP83VA8P, user: anonymous]
 WARN [2020-10-28 22:00:51,366] ({pool-2-thread-22} NotebookServer.java[afterStatusChange]:2316) - Job 20201028-214624_69997796 is finished, status: ERROR, exception: null, result: %text Traceback (most recent call last):
  File "/tmp/zeppelin_pyspark-3213863130067318734.py", line 364, in <module>
    code = compile('\n'.join(stmts), '<stdin>', 'exec', ast.PyCF_ONLY_AST, 1)
  File "<stdin>", line 4
    .filter(lambda line: line not 'NA' ) \
                                     ^
SyntaxError: invalid syntax

 INFO [2020-10-28 22:00:51,475] ({qtp89387388-1382} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 22:00:51,483] ({pool-2-thread-22} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 22:00:51,483] ({pool-2-thread-42} SchedulerFactory.java[jobStarted]:114) - Job 20201028-214624_69997796 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 22:00:51,484] ({pool-2-thread-42} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20201028-214624_69997796, interpreter: pyspark, note_id: 2FP83VA8P, user: anonymous]
 INFO [2020-10-28 22:00:51,487] ({pool-2-thread-22} SchedulerFactory.java[jobFinished]:120) - Job 20201028-214624_69997796 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 WARN [2020-10-28 22:00:51,490] ({pool-2-thread-42} NotebookServer.java[afterStatusChange]:2316) - Job 20201028-214624_69997796 is finished, status: ERROR, exception: null, result: %text Traceback (most recent call last):
  File "/tmp/zeppelin_pyspark-3213863130067318734.py", line 364, in <module>
    code = compile('\n'.join(stmts), '<stdin>', 'exec', ast.PyCF_ONLY_AST, 1)
  File "<stdin>", line 4
    .filter(lambda line: line not 'NA' ) \
                                     ^
SyntaxError: invalid syntax

 INFO [2020-10-28 22:00:51,526] ({pool-2-thread-42} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 22:00:51,530] ({pool-2-thread-42} SchedulerFactory.java[jobFinished]:120) - Job 20201028-214624_69997796 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 22:01:04,262] ({qtp89387388-928} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 22:01:20,846] ({qtp89387388-928} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 22:01:20,925] ({qtp89387388-1382} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 22:01:20,930] ({pool-2-thread-5} SchedulerFactory.java[jobStarted]:114) - Job 20201028-214624_69997796 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 22:01:20,935] ({pool-2-thread-5} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20201028-214624_69997796, interpreter: pyspark, note_id: 2FP83VA8P, user: anonymous]
 INFO [2020-10-28 22:01:25,626] ({pool-2-thread-5} NotebookServer.java[afterStatusChange]:2314) - Job 20201028-214624_69997796 is finished successfully, status: FINISHED
 INFO [2020-10-28 22:01:25,660] ({pool-2-thread-5} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 WARN [2020-10-28 22:01:25,911] ({pool-6-thread-1} AppendOutputRunner.java[run]:88) - Processing time for buffered append-output is high: 13 milliseconds.
 WARN [2020-10-28 22:01:28,134] ({pool-6-thread-1} AppendOutputRunner.java[run]:104) - Processing size for buffered append-output is high: 18619672 characters.
 INFO [2020-10-28 22:01:28,987] ({pool-2-thread-5} SchedulerFactory.java[jobFinished]:120) - Job 20201028-214624_69997796 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 22:01:54,833] ({qtp89387388-319} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 22:01:57,223] ({qtp89387388-319} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 22:02:01,036] ({pool-2-thread-43} SchedulerFactory.java[jobStarted]:114) - Job 20201028-214624_69997796 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 22:02:01,037] ({pool-2-thread-43} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20201028-214624_69997796, interpreter: pyspark, note_id: 2FP83VA8P, user: anonymous]
 INFO [2020-10-28 22:02:01,071] ({qtp89387388-319} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 WARN [2020-10-28 22:02:01,308] ({pool-2-thread-43} NotebookServer.java[afterStatusChange]:2316) - Job 20201028-214624_69997796 is finished, status: ERROR, exception: null, result: %text Fail to execute line 4:     .filter(lambda line: line != 'NA' ) \
Traceback (most recent call last):
  File "/tmp/zeppelin_pyspark-3213863130067318734.py", line 380, in <module>
    exec(code, _zcUserQueryNameSpace)
  File "<stdin>", line 4, in <module>
AttributeError: 'list' object has no attribute 'collect'

 INFO [2020-10-28 22:02:01,360] ({pool-2-thread-43} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 22:02:01,365] ({pool-2-thread-43} SchedulerFactory.java[jobFinished]:120) - Job 20201028-214624_69997796 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 22:02:01,365] ({pool-2-thread-23} SchedulerFactory.java[jobStarted]:114) - Job 20201028-214624_69997796 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 22:02:01,366] ({pool-2-thread-23} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20201028-214624_69997796, interpreter: pyspark, note_id: 2FP83VA8P, user: anonymous]
 WARN [2020-10-28 22:02:01,623] ({pool-2-thread-23} NotebookServer.java[afterStatusChange]:2316) - Job 20201028-214624_69997796 is finished, status: ERROR, exception: null, result: %text Fail to execute line 4:     .filter(lambda line: line != 'NA' ) \
Traceback (most recent call last):
  File "/tmp/zeppelin_pyspark-3213863130067318734.py", line 380, in <module>
    exec(code, _zcUserQueryNameSpace)
  File "<stdin>", line 4, in <module>
AttributeError: 'list' object has no attribute 'collect'

 INFO [2020-10-28 22:02:01,659] ({pool-2-thread-23} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 22:02:01,663] ({pool-2-thread-23} SchedulerFactory.java[jobFinished]:120) - Job 20201028-214624_69997796 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 22:02:04,860] ({qtp89387388-722} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 22:02:04,865] ({pool-2-thread-44} SchedulerFactory.java[jobStarted]:114) - Job 20201028-214624_69997796 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 22:02:04,866] ({pool-2-thread-44} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20201028-214624_69997796, interpreter: pyspark, note_id: 2FP83VA8P, user: anonymous]
 WARN [2020-10-28 22:02:05,135] ({pool-2-thread-44} NotebookServer.java[afterStatusChange]:2316) - Job 20201028-214624_69997796 is finished, status: ERROR, exception: null, result: %text Fail to execute line 4:     .filter(lambda line: line != 'NA' ) \
Traceback (most recent call last):
  File "/tmp/zeppelin_pyspark-3213863130067318734.py", line 380, in <module>
    exec(code, _zcUserQueryNameSpace)
  File "<stdin>", line 4, in <module>
AttributeError: 'list' object has no attribute 'collect'

 INFO [2020-10-28 22:02:05,172] ({pool-2-thread-44} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 22:02:05,176] ({pool-2-thread-44} SchedulerFactory.java[jobFinished]:120) - Job 20201028-214624_69997796 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 22:02:24,684] ({qtp89387388-319} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 22:02:25,377] ({qtp89387388-66} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 22:02:25,382] ({pool-2-thread-13} SchedulerFactory.java[jobStarted]:114) - Job 20201028-214624_69997796 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 22:02:25,383] ({pool-2-thread-13} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20201028-214624_69997796, interpreter: pyspark, note_id: 2FP83VA8P, user: anonymous]
 INFO [2020-10-28 22:02:25,725] ({pool-2-thread-13} NotebookServer.java[afterStatusChange]:2314) - Job 20201028-214624_69997796 is finished successfully, status: FINISHED
 INFO [2020-10-28 22:02:25,762] ({pool-2-thread-13} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 22:02:25,766] ({pool-2-thread-13} SchedulerFactory.java[jobFinished]:120) - Job 20201028-214624_69997796 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 22:03:53,422] ({qtp89387388-66} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 22:03:53,492] ({qtp89387388-1039} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 22:03:53,497] ({pool-2-thread-45} SchedulerFactory.java[jobStarted]:114) - Job 20201028-214624_69997796 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 22:03:53,497] ({pool-2-thread-45} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20201028-214624_69997796, interpreter: pyspark, note_id: 2FP83VA8P, user: anonymous]
 WARN [2020-10-28 22:03:53,575] ({pool-2-thread-45} NotebookServer.java[afterStatusChange]:2316) - Job 20201028-214624_69997796 is finished, status: ERROR, exception: null, result: %text Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 129.0 failed 1 times, most recent failure: Lost task 0.0 in stage 129.0 (TID 2511, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/worker.py", line 177, in main
    process()
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/worker.py", line 172, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/serializers.py", line 268, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/rdd.py", line 830, in func
  File "<stdin>", line 4, in <lambda>
ValueError: could not convert string to float: 'AirTime'

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:935)
	at org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:467)
	at org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)
	at sun.reflect.GeneratedMethodAccessor150.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:280)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/worker.py", line 177, in main
    process()
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/worker.py", line 172, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/serializers.py", line 268, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/rdd.py", line 830, in func
  File "<stdin>", line 4, in <lambda>
ValueError: could not convert string to float: 'AirTime'

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	... 1 more

(<class 'py4j.protocol.Py4JJavaError'>, Py4JJavaError('An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n', JavaObject id=o1944), <traceback object at 0x7f4ac71d8cc8>)
 INFO [2020-10-28 22:03:53,611] ({pool-2-thread-45} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 22:03:53,615] ({pool-2-thread-45} SchedulerFactory.java[jobFinished]:120) - Job 20201028-214624_69997796 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 22:04:57,308] ({qtp89387388-1039} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 22:04:57,368] ({qtp89387388-1250} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 22:04:57,373] ({pool-2-thread-46} SchedulerFactory.java[jobStarted]:114) - Job 20171016-232257_285172371 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-md:shared_process-shared_session
 INFO [2020-10-28 22:04:57,374] ({pool-2-thread-46} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20171016-232257_285172371, interpreter: md, note_id: 2FP83VA8P, user: anonymous]
 INFO [2020-10-28 22:04:57,374] ({pool-2-thread-46} ManagedInterpreterGroup.java[getOrCreateInterpreterProcess]:61) - Create InterpreterProcess for InterpreterGroup: md:shared_process
 INFO [2020-10-28 22:04:57,374] ({pool-2-thread-46} ShellScriptLauncher.java[launch]:48) - Launching Interpreter: md
 INFO [2020-10-28 22:04:57,375] ({pool-2-thread-46} RemoteInterpreterManagedProcess.java[start]:115) - Thrift server for callback will start. Port: 37453
 INFO [2020-10-28 22:04:57,405] ({qtp89387388-1250} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 22:04:57,875] ({pool-2-thread-46} RemoteInterpreterManagedProcess.java[start]:190) - Run interpreter process [/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/bin/interpreter.sh, -d, /users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/md, -c, 200.16.29.165, -p, 37453, -r, :, -l, /users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/local-repo/md, -g, md]
 INFO [2020-10-28 22:04:59,317] ({pool-11-thread-1} RemoteInterpreterManagedProcess.java[callback]:123) - RemoteInterpreterServer Registered: CallbackInfo(host:200.16.29.165, port:40527)
 INFO [2020-10-28 22:04:59,320] ({pool-2-thread-46} RemoteInterpreter.java[call]:168) - Create RemoteInterpreter org.apache.zeppelin.markdown.Markdown
 INFO [2020-10-28 22:04:59,457] ({pool-2-thread-46} RemoteInterpreter.java[call]:142) - Open RemoteInterpreter org.apache.zeppelin.markdown.Markdown
 INFO [2020-10-28 22:04:59,457] ({pool-2-thread-46} RemoteInterpreter.java[pushAngularObjectRegistryToRemote]:436) - Push local angular object registry from ZeppelinServer to remote interpreter group md:shared_process
 INFO [2020-10-28 22:05:00,014] ({pool-2-thread-46} NotebookServer.java[afterStatusChange]:2314) - Job 20171016-232257_285172371 is finished successfully, status: FINISHED
 INFO [2020-10-28 22:05:00,050] ({pool-2-thread-46} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 22:05:00,054] ({pool-2-thread-46} SchedulerFactory.java[jobFinished]:120) - Job 20171016-232257_285172371 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-md:shared_process-shared_session
 INFO [2020-10-28 22:05:01,541] ({qtp89387388-1250} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 22:05:01,546] ({pool-2-thread-8} SchedulerFactory.java[jobStarted]:114) - Job 20201028-214624_69997796 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 22:05:01,548] ({pool-2-thread-8} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20201028-214624_69997796, interpreter: pyspark, note_id: 2FP83VA8P, user: anonymous]
 WARN [2020-10-28 22:05:01,636] ({pool-2-thread-8} NotebookServer.java[afterStatusChange]:2316) - Job 20201028-214624_69997796 is finished, status: ERROR, exception: null, result: %text Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 130.0 failed 1 times, most recent failure: Lost task 0.0 in stage 130.0 (TID 2513, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/worker.py", line 177, in main
    process()
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/worker.py", line 172, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/serializers.py", line 268, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/rdd.py", line 830, in func
  File "<stdin>", line 4, in <lambda>
ValueError: could not convert string to float: 'ArrDelay'

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:935)
	at org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:467)
	at org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)
	at sun.reflect.GeneratedMethodAccessor150.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:280)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/worker.py", line 177, in main
    process()
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/worker.py", line 172, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/serializers.py", line 268, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/rdd.py", line 830, in func
  File "<stdin>", line 4, in <lambda>
ValueError: could not convert string to float: 'ArrDelay'

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	... 1 more

(<class 'py4j.protocol.Py4JJavaError'>, Py4JJavaError('An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n', JavaObject id=o1993), <traceback object at 0x7f4ab89b3648>)
 INFO [2020-10-28 22:05:01,670] ({pool-2-thread-8} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 22:05:01,674] ({pool-2-thread-8} SchedulerFactory.java[jobFinished]:120) - Job 20201028-214624_69997796 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 22:05:24,731] ({qtp89387388-26} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 22:07:13,336] ({qtp89387388-1039} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 22:07:13,406] ({qtp89387388-1250} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 22:07:13,414] ({pool-2-thread-47} SchedulerFactory.java[jobStarted]:114) - Job 20201028-214624_69997796 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 22:07:13,415] ({pool-2-thread-47} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20201028-214624_69997796, interpreter: pyspark, note_id: 2FP83VA8P, user: anonymous]
 INFO [2020-10-28 22:07:13,704] ({pool-2-thread-47} NotebookServer.java[afterStatusChange]:2314) - Job 20201028-214624_69997796 is finished successfully, status: FINISHED
 INFO [2020-10-28 22:07:13,739] ({pool-2-thread-47} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 22:07:13,743] ({pool-2-thread-47} SchedulerFactory.java[jobFinished]:120) - Job 20201028-214624_69997796 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 22:24:57,899] ({qtp89387388-1619} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 22:24:57,970] ({qtp89387388-1614} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 22:24:57,976] ({pool-2-thread-25} SchedulerFactory.java[jobStarted]:114) - Job 20201028-214624_69997796 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 22:24:57,978] ({pool-2-thread-25} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20201028-214624_69997796, interpreter: pyspark, note_id: 2FP83VA8P, user: anonymous]
 WARN [2020-10-28 22:24:58,009] ({pool-2-thread-25} NotebookServer.java[afterStatusChange]:2316) - Job 20201028-214624_69997796 is finished, status: ERROR, exception: null, result: %text Fail to execute line 5:     .max(line[13])
Traceback (most recent call last):
  File "/tmp/zeppelin_pyspark-3213863130067318734.py", line 380, in <module>
    exec(code, _zcUserQueryNameSpace)
  File "<stdin>", line 5, in <module>
NameError: name 'line' is not defined

 INFO [2020-10-28 22:24:58,052] ({pool-2-thread-25} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 22:24:58,057] ({pool-2-thread-25} SchedulerFactory.java[jobFinished]:120) - Job 20201028-214624_69997796 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 22:25:06,576] ({qtp89387388-975} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 22:25:06,621] ({qtp89387388-975} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 22:25:06,626] ({pool-2-thread-48} SchedulerFactory.java[jobStarted]:114) - Job 20201028-214624_69997796 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 22:25:06,627] ({pool-2-thread-48} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20201028-214624_69997796, interpreter: pyspark, note_id: 2FP83VA8P, user: anonymous]
 INFO [2020-10-28 22:25:06,917] ({pool-2-thread-48} NotebookServer.java[afterStatusChange]:2314) - Job 20201028-214624_69997796 is finished successfully, status: FINISHED
 INFO [2020-10-28 22:25:06,952] ({pool-2-thread-48} VFSNotebookRepo.java[save]:196) - Saving note:2FP83VA8P
 INFO [2020-10-28 22:25:06,956] ({pool-2-thread-48} SchedulerFactory.java[jobFinished]:120) - Job 20201028-214624_69997796 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 22:27:34,396] ({qtp89387388-1637} NotebookServer.java[sendNote]:828) - New operation from 127.0.0.1 : 44480 : anonymous : GET_NOTE : 2FP1YEJHN
 WARN [2020-10-28 22:27:34,408] ({qtp89387388-1637} GitNotebookRepo.java[revisionHistory]:158) - No Head found for 2FP1YEJHN, No HEAD exists and no explicit starting revision was specified
 WARN [2020-10-28 22:27:34,502] ({qtp89387388-1038} InterpreterSettingManager.java[compare]:886) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-10-28 22:27:34,503] ({qtp89387388-1038} InterpreterSettingManager.java[compare]:886) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-10-28 22:27:34,503] ({qtp89387388-1038} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-10-28 22:27:34,503] ({qtp89387388-1038} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-10-28 22:27:34,504] ({qtp89387388-1038} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 WARN [2020-10-28 22:27:34,504] ({qtp89387388-1038} InterpreterSettingManager.java[compare]:892) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
 INFO [2020-10-28 22:47:07,212] ({qtp89387388-1656} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 22:47:07,320] ({qtp89387388-1672} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 22:47:07,325] ({pool-2-thread-14} SchedulerFactory.java[jobStarted]:114) - Job 20201023-123443_40437356 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 22:47:07,325] ({pool-2-thread-14} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20201023-123443_40437356, interpreter: pyspark, note_id: 2FP1YEJHN, user: anonymous]
 INFO [2020-10-28 22:47:11,822] ({pool-2-thread-14} NotebookServer.java[afterStatusChange]:2314) - Job 20201023-123443_40437356 is finished successfully, status: FINISHED
 INFO [2020-10-28 22:47:11,875] ({pool-2-thread-14} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 22:47:11,879] ({pool-2-thread-14} SchedulerFactory.java[jobFinished]:120) - Job 20201023-123443_40437356 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 22:47:39,862] ({qtp89387388-1672} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 22:47:40,564] ({qtp89387388-1672} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 22:47:40,568] ({pool-2-thread-49} SchedulerFactory.java[jobStarted]:114) - Job 20191128-172216_319223823 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 22:47:40,569] ({pool-2-thread-49} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20191128-172216_319223823, interpreter: pyspark, note_id: 2FP1YEJHN, user: anonymous]
 WARN [2020-10-28 22:47:40,574] ({pool-2-thread-49} NotebookServer.java[afterStatusChange]:2316) - Job 20191128-172216_319223823 is finished, status: ERROR, exception: null, result: %text Traceback (most recent call last):
  File "/tmp/zeppelin_pyspark-3213863130067318734.py", line 364, in <module>
    code = compile('\n'.join(stmts), '<stdin>', 'exec', ast.PyCF_ONLY_AST, 1)
  File "<stdin>", line 3
    from pyspark.sql.functions import ...
                                        ^
SyntaxError: invalid syntax

 INFO [2020-10-28 22:47:40,622] ({pool-2-thread-49} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 22:47:40,625] ({pool-2-thread-49} SchedulerFactory.java[jobFinished]:120) - Job 20191128-172216_319223823 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 22:48:32,824] ({qtp89387388-1673} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 22:48:32,913] ({qtp89387388-1672} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 22:48:32,917] ({pool-2-thread-26} SchedulerFactory.java[jobStarted]:114) - Job 20191128-172216_319223823 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 22:48:32,918] ({pool-2-thread-26} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20191128-172216_319223823, interpreter: pyspark, note_id: 2FP1YEJHN, user: anonymous]
 INFO [2020-10-28 22:48:34,246] ({pool-2-thread-26} NotebookServer.java[afterStatusChange]:2314) - Job 20191128-172216_319223823 is finished successfully, status: FINISHED
 INFO [2020-10-28 22:48:34,286] ({pool-2-thread-26} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 22:48:34,290] ({pool-2-thread-26} SchedulerFactory.java[jobFinished]:120) - Job 20191128-172216_319223823 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 22:48:46,424] ({qtp89387388-975} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 22:48:46,428] ({pool-2-thread-50} SchedulerFactory.java[jobStarted]:114) - Job 20201023-123509_121589787 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 22:48:46,429] ({pool-2-thread-50} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20201023-123509_121589787, interpreter: pyspark, note_id: 2FP1YEJHN, user: anonymous]
 INFO [2020-10-28 22:48:46,758] ({pool-2-thread-50} NotebookServer.java[afterStatusChange]:2314) - Job 20201023-123509_121589787 is finished successfully, status: FINISHED
 INFO [2020-10-28 22:48:46,798] ({pool-2-thread-50} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 22:48:46,801] ({pool-2-thread-50} SchedulerFactory.java[jobFinished]:120) - Job 20201023-123509_121589787 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 22:48:54,931] ({qtp89387388-1693} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 22:48:54,935] ({pool-2-thread-4} SchedulerFactory.java[jobStarted]:114) - Job 20201023-123527_1290957379 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 22:48:54,935] ({pool-2-thread-4} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20201023-123527_1290957379, interpreter: pyspark, note_id: 2FP1YEJHN, user: anonymous]
 INFO [2020-10-28 22:48:55,369] ({pool-2-thread-4} NotebookServer.java[afterStatusChange]:2314) - Job 20201023-123527_1290957379 is finished successfully, status: FINISHED
 INFO [2020-10-28 22:48:55,410] ({pool-2-thread-4} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 22:48:55,413] ({pool-2-thread-4} SchedulerFactory.java[jobFinished]:120) - Job 20201023-123527_1290957379 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 22:49:18,375] ({qtp89387388-1673} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 22:49:18,379] ({pool-2-thread-51} SchedulerFactory.java[jobStarted]:114) - Job 20161017-104100_1861019655 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 22:49:18,380] ({pool-2-thread-51} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20161017-104100_1861019655, interpreter: pyspark, note_id: 2FP1YEJHN, user: anonymous]
 WARN [2020-10-28 22:49:18,390] ({pool-2-thread-51} NotebookServer.java[afterStatusChange]:2316) - Job 20161017-104100_1861019655 is finished, status: ERROR, exception: null, result: %text Traceback (most recent call last):
  File "/tmp/zeppelin_pyspark-3213863130067318734.py", line 364, in <module>
    code = compile('\n'.join(stmts), '<stdin>', 'exec', ast.PyCF_ONLY_AST, 1)
  File "<stdin>", line 2
    print ints[:10]
             ^
SyntaxError: Missing parentheses in call to 'print'. Did you mean print(ints[:10])?

 INFO [2020-10-28 22:49:18,436] ({pool-2-thread-51} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 22:49:18,440] ({pool-2-thread-51} SchedulerFactory.java[jobFinished]:120) - Job 20161017-104100_1861019655 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 22:49:35,245] ({qtp89387388-975} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 22:49:35,351] ({qtp89387388-1693} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 22:49:35,355] ({pool-2-thread-27} SchedulerFactory.java[jobStarted]:114) - Job 20161017-104100_1861019655 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 22:49:35,356] ({pool-2-thread-27} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20161017-104100_1861019655, interpreter: pyspark, note_id: 2FP1YEJHN, user: anonymous]
 WARN [2020-10-28 22:49:35,365] ({pool-2-thread-27} NotebookServer.java[afterStatusChange]:2316) - Job 20161017-104100_1861019655 is finished, status: ERROR, exception: null, result: %text Traceback (most recent call last):
  File "/tmp/zeppelin_pyspark-3213863130067318734.py", line 364, in <module>
    code = compile('\n'.join(stmts), '<stdin>', 'exec', ast.PyCF_ONLY_AST, 1)
  File "<stdin>", line 6
    print intsRDD.count()
                ^
SyntaxError: invalid syntax

 INFO [2020-10-28 22:49:35,422] ({pool-2-thread-27} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 22:49:35,426] ({pool-2-thread-27} SchedulerFactory.java[jobFinished]:120) - Job 20161017-104100_1861019655 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 22:50:37,424] ({qtp89387388-1693} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 22:50:37,541] ({qtp89387388-1673} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 22:50:37,545] ({pool-2-thread-52} SchedulerFactory.java[jobStarted]:114) - Job 20161017-104100_1861019655 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 22:50:37,546] ({pool-2-thread-52} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20161017-104100_1861019655, interpreter: pyspark, note_id: 2FP1YEJHN, user: anonymous]
 WARN [2020-10-28 22:50:37,554] ({pool-2-thread-52} NotebookServer.java[afterStatusChange]:2316) - Job 20161017-104100_1861019655 is finished, status: ERROR, exception: null, result: %text Traceback (most recent call last):
  File "/tmp/zeppelin_pyspark-3213863130067318734.py", line 364, in <module>
    code = compile('\n'.join(stmts), '<stdin>', 'exec', ast.PyCF_ONLY_AST, 1)
  File "<stdin>", line 6
    print intsRDD.count()
                ^
SyntaxError: invalid syntax

 INFO [2020-10-28 22:50:37,597] ({pool-2-thread-52} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 22:50:37,600] ({pool-2-thread-52} SchedulerFactory.java[jobFinished]:120) - Job 20161017-104100_1861019655 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 22:50:47,143] ({qtp89387388-1038} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 22:50:47,222] ({qtp89387388-1693} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 22:50:47,226] ({pool-2-thread-15} SchedulerFactory.java[jobStarted]:114) - Job 20161017-104100_1861019655 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 22:50:47,227] ({pool-2-thread-15} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20161017-104100_1861019655, interpreter: pyspark, note_id: 2FP1YEJHN, user: anonymous]
 WARN [2020-10-28 22:50:47,248] ({pool-2-thread-15} NotebookServer.java[afterStatusChange]:2316) - Job 20161017-104100_1861019655 is finished, status: ERROR, exception: null, result: %text Fail to execute line 1: ints = range(pow(10, 6))
Traceback (most recent call last):
  File "/tmp/zeppelin_pyspark-3213863130067318734.py", line 375, in <module>
    exec(code, _zcUserQueryNameSpace)
  File "<stdin>", line 1, in <module>
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/sql/functions.py", line 53, in _
    col2._jc if isinstance(col2, Column) else float(col2))
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/java_gateway.py", line 1133, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/sql/utils.py", line 63, in deco
    return f(*a, **kw)
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/protocol.py", line 323, in get_return_value
    format(target_id, ".", name, value))
py4j.protocol.Py4JError: An error occurred while calling z:org.apache.spark.sql.functions.pow. Trace:
py4j.Py4JException: Method pow([class java.lang.Double, class java.lang.Double]) does not exist
	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318)
	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:339)
	at py4j.Gateway.invoke(Gateway.java:274)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:745)



 INFO [2020-10-28 22:50:47,315] ({pool-2-thread-15} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 22:50:47,320] ({pool-2-thread-15} SchedulerFactory.java[jobFinished]:120) - Job 20161017-104100_1861019655 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 22:51:25,830] ({qtp89387388-1693} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 22:51:25,938] ({qtp89387388-1732} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 22:51:25,942] ({pool-2-thread-53} SchedulerFactory.java[jobStarted]:114) - Job 20161017-104100_1861019655 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 22:51:25,942] ({pool-2-thread-53} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20161017-104100_1861019655, interpreter: pyspark, note_id: 2FP1YEJHN, user: anonymous]
 WARN [2020-10-28 22:51:25,952] ({pool-2-thread-53} NotebookServer.java[afterStatusChange]:2316) - Job 20161017-104100_1861019655 is finished, status: ERROR, exception: null, result: %text Fail to execute line 1: ints = range(pow(10, 6))
Traceback (most recent call last):
  File "/tmp/zeppelin_pyspark-3213863130067318734.py", line 375, in <module>
    exec(code, _zcUserQueryNameSpace)
  File "<stdin>", line 1, in <module>
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/sql/functions.py", line 53, in _
    col2._jc if isinstance(col2, Column) else float(col2))
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/java_gateway.py", line 1133, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/sql/utils.py", line 63, in deco
    return f(*a, **kw)
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/protocol.py", line 323, in get_return_value
    format(target_id, ".", name, value))
py4j.protocol.Py4JError: An error occurred while calling z:org.apache.spark.sql.functions.pow. Trace:
py4j.Py4JException: Method pow([class java.lang.Double, class java.lang.Double]) does not exist
	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318)
	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:339)
	at py4j.Gateway.invoke(Gateway.java:274)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:745)



 INFO [2020-10-28 22:51:25,994] ({pool-2-thread-53} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 22:51:25,998] ({pool-2-thread-53} SchedulerFactory.java[jobFinished]:120) - Job 20161017-104100_1861019655 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 22:51:55,102] ({qtp89387388-1693} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 22:51:55,183] ({qtp89387388-1793} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 22:51:55,186] ({pool-2-thread-28} SchedulerFactory.java[jobStarted]:114) - Job 20161017-104100_1861019655 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 22:51:55,187] ({pool-2-thread-28} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20161017-104100_1861019655, interpreter: pyspark, note_id: 2FP1YEJHN, user: anonymous]
 WARN [2020-10-28 22:51:55,197] ({pool-2-thread-28} NotebookServer.java[afterStatusChange]:2316) - Job 20161017-104100_1861019655 is finished, status: ERROR, exception: null, result: %text Fail to execute line 1: ints = range(pow(10, 6))
Traceback (most recent call last):
  File "/tmp/zeppelin_pyspark-3213863130067318734.py", line 375, in <module>
    exec(code, _zcUserQueryNameSpace)
  File "<stdin>", line 1, in <module>
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/sql/functions.py", line 53, in _
    col2._jc if isinstance(col2, Column) else float(col2))
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/java_gateway.py", line 1133, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/sql/utils.py", line 63, in deco
    return f(*a, **kw)
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/protocol.py", line 323, in get_return_value
    format(target_id, ".", name, value))
py4j.protocol.Py4JError: An error occurred while calling z:org.apache.spark.sql.functions.pow. Trace:
py4j.Py4JException: Method pow([class java.lang.Double, class java.lang.Double]) does not exist
	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318)
	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:339)
	at py4j.Gateway.invoke(Gateway.java:274)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:745)



 INFO [2020-10-28 22:51:55,242] ({pool-2-thread-28} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 22:51:55,245] ({pool-2-thread-28} SchedulerFactory.java[jobFinished]:120) - Job 20161017-104100_1861019655 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 22:52:59,064] ({qtp89387388-1693} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 22:52:59,153] ({qtp89387388-1848} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 22:52:59,157] ({pool-2-thread-54} SchedulerFactory.java[jobStarted]:114) - Job 20161017-104100_1861019655 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 22:52:59,158] ({pool-2-thread-54} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20161017-104100_1861019655, interpreter: pyspark, note_id: 2FP1YEJHN, user: anonymous]
 WARN [2020-10-28 22:52:59,176] ({pool-2-thread-54} NotebookServer.java[afterStatusChange]:2316) - Job 20161017-104100_1861019655 is finished, status: ERROR, exception: null, result: %text Fail to execute line 1: ints = list(range(pow(10, 6)))
Traceback (most recent call last):
  File "/tmp/zeppelin_pyspark-3213863130067318734.py", line 375, in <module>
    exec(code, _zcUserQueryNameSpace)
  File "<stdin>", line 1, in <module>
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/sql/functions.py", line 53, in _
    col2._jc if isinstance(col2, Column) else float(col2))
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/java_gateway.py", line 1133, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/sql/utils.py", line 63, in deco
    return f(*a, **kw)
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/protocol.py", line 323, in get_return_value
    format(target_id, ".", name, value))
py4j.protocol.Py4JError: An error occurred while calling z:org.apache.spark.sql.functions.pow. Trace:
py4j.Py4JException: Method pow([class java.lang.Double, class java.lang.Double]) does not exist
	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318)
	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:339)
	at py4j.Gateway.invoke(Gateway.java:274)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:745)



 INFO [2020-10-28 22:52:59,221] ({pool-2-thread-54} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 22:52:59,224] ({pool-2-thread-54} SchedulerFactory.java[jobFinished]:120) - Job 20161017-104100_1861019655 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 22:53:09,672] ({qtp89387388-1472} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 22:53:09,768] ({qtp89387388-1693} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 22:53:09,771] ({pool-2-thread-9} SchedulerFactory.java[jobStarted]:114) - Job 20161017-104100_1861019655 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 22:53:09,773] ({pool-2-thread-9} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20161017-104100_1861019655, interpreter: pyspark, note_id: 2FP1YEJHN, user: anonymous]
 WARN [2020-10-28 22:53:09,789] ({pool-2-thread-9} NotebookServer.java[afterStatusChange]:2316) - Job 20161017-104100_1861019655 is finished, status: ERROR, exception: null, result: %text Fail to execute line 1: ints = range(pow(10, 6))
Traceback (most recent call last):
  File "/tmp/zeppelin_pyspark-3213863130067318734.py", line 375, in <module>
    exec(code, _zcUserQueryNameSpace)
  File "<stdin>", line 1, in <module>
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/sql/functions.py", line 53, in _
    col2._jc if isinstance(col2, Column) else float(col2))
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/java_gateway.py", line 1133, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/sql/utils.py", line 63, in deco
    return f(*a, **kw)
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/protocol.py", line 323, in get_return_value
    format(target_id, ".", name, value))
py4j.protocol.Py4JError: An error occurred while calling z:org.apache.spark.sql.functions.pow. Trace:
py4j.Py4JException: Method pow([class java.lang.Double, class java.lang.Double]) does not exist
	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318)
	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:339)
	at py4j.Gateway.invoke(Gateway.java:274)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:745)



 INFO [2020-10-28 22:53:09,841] ({pool-2-thread-9} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 22:53:09,845] ({pool-2-thread-9} SchedulerFactory.java[jobFinished]:120) - Job 20161017-104100_1861019655 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 22:53:54,697] ({qtp89387388-1848} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 22:53:54,811] ({qtp89387388-1693} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 22:53:54,815] ({pool-2-thread-55} SchedulerFactory.java[jobStarted]:114) - Job 20161017-104100_1861019655 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 22:53:54,818] ({pool-2-thread-55} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20161017-104100_1861019655, interpreter: pyspark, note_id: 2FP1YEJHN, user: anonymous]
 WARN [2020-10-28 22:53:54,832] ({pool-2-thread-55} NotebookServer.java[afterStatusChange]:2316) - Job 20161017-104100_1861019655 is finished, status: ERROR, exception: null, result: %text Fail to execute line 1: ints = range(pow(10, 2))
Traceback (most recent call last):
  File "/tmp/zeppelin_pyspark-3213863130067318734.py", line 375, in <module>
    exec(code, _zcUserQueryNameSpace)
  File "<stdin>", line 1, in <module>
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/sql/functions.py", line 53, in _
    col2._jc if isinstance(col2, Column) else float(col2))
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/java_gateway.py", line 1133, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/sql/utils.py", line 63, in deco
    return f(*a, **kw)
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/protocol.py", line 323, in get_return_value
    format(target_id, ".", name, value))
py4j.protocol.Py4JError: An error occurred while calling z:org.apache.spark.sql.functions.pow. Trace:
py4j.Py4JException: Method pow([class java.lang.Double, class java.lang.Double]) does not exist
	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318)
	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:339)
	at py4j.Gateway.invoke(Gateway.java:274)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:745)



 INFO [2020-10-28 22:53:54,887] ({pool-2-thread-55} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 22:53:54,890] ({pool-2-thread-55} SchedulerFactory.java[jobFinished]:120) - Job 20161017-104100_1861019655 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 22:54:09,173] ({qtp89387388-1673} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 22:54:09,302] ({qtp89387388-1848} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 22:54:09,305] ({pool-2-thread-29} SchedulerFactory.java[jobStarted]:114) - Job 20161017-104100_1861019655 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 22:54:09,307] ({pool-2-thread-29} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20161017-104100_1861019655, interpreter: pyspark, note_id: 2FP1YEJHN, user: anonymous]
 INFO [2020-10-28 22:54:09,322] ({pool-2-thread-29} NotebookServer.java[afterStatusChange]:2314) - Job 20161017-104100_1861019655 is finished successfully, status: FINISHED
 INFO [2020-10-28 22:54:09,361] ({pool-2-thread-29} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 22:54:09,364] ({pool-2-thread-29} SchedulerFactory.java[jobFinished]:120) - Job 20161017-104100_1861019655 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 22:54:28,274] ({qtp89387388-1673} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 22:54:28,383] ({qtp89387388-1848} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 22:54:28,387] ({pool-2-thread-56} SchedulerFactory.java[jobStarted]:114) - Job 20161017-104100_1861019655 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 22:54:28,388] ({pool-2-thread-56} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20161017-104100_1861019655, interpreter: pyspark, note_id: 2FP1YEJHN, user: anonymous]
 WARN [2020-10-28 22:54:28,398] ({pool-2-thread-56} NotebookServer.java[afterStatusChange]:2316) - Job 20161017-104100_1861019655 is finished, status: ERROR, exception: null, result: %text Fail to execute line 2: print(ints[:10], pow(10, 6))
Traceback (most recent call last):
  File "/tmp/zeppelin_pyspark-3213863130067318734.py", line 380, in <module>
    exec(code, _zcUserQueryNameSpace)
  File "<stdin>", line 2, in <module>
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/sql/functions.py", line 53, in _
    col2._jc if isinstance(col2, Column) else float(col2))
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/java_gateway.py", line 1133, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/sql/utils.py", line 63, in deco
    return f(*a, **kw)
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/protocol.py", line 323, in get_return_value
    format(target_id, ".", name, value))
py4j.protocol.Py4JError: An error occurred while calling z:org.apache.spark.sql.functions.pow. Trace:
py4j.Py4JException: Method pow([class java.lang.Double, class java.lang.Double]) does not exist
	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318)
	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:339)
	at py4j.Gateway.invoke(Gateway.java:274)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:745)



 INFO [2020-10-28 22:54:28,439] ({pool-2-thread-56} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 22:54:28,442] ({pool-2-thread-56} SchedulerFactory.java[jobFinished]:120) - Job 20161017-104100_1861019655 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 22:54:42,735] ({qtp89387388-1673} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 22:54:42,819] ({qtp89387388-1693} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 22:54:42,823] ({pool-2-thread-16} SchedulerFactory.java[jobStarted]:114) - Job 20161017-104100_1861019655 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 22:54:42,824] ({pool-2-thread-16} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20161017-104100_1861019655, interpreter: pyspark, note_id: 2FP1YEJHN, user: anonymous]
 WARN [2020-10-28 22:54:42,837] ({pool-2-thread-16} NotebookServer.java[afterStatusChange]:2316) - Job 20161017-104100_1861019655 is finished, status: ERROR, exception: null, result: %text Fail to execute line 2: print(pow(10, 6))
Traceback (most recent call last):
  File "/tmp/zeppelin_pyspark-3213863130067318734.py", line 380, in <module>
    exec(code, _zcUserQueryNameSpace)
  File "<stdin>", line 2, in <module>
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/sql/functions.py", line 53, in _
    col2._jc if isinstance(col2, Column) else float(col2))
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/java_gateway.py", line 1133, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/sql/utils.py", line 63, in deco
    return f(*a, **kw)
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/protocol.py", line 323, in get_return_value
    format(target_id, ".", name, value))
py4j.protocol.Py4JError: An error occurred while calling z:org.apache.spark.sql.functions.pow. Trace:
py4j.Py4JException: Method pow([class java.lang.Double, class java.lang.Double]) does not exist
	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318)
	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:339)
	at py4j.Gateway.invoke(Gateway.java:274)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:745)



 INFO [2020-10-28 22:54:42,883] ({pool-2-thread-16} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 22:54:42,886] ({pool-2-thread-16} SchedulerFactory.java[jobFinished]:120) - Job 20161017-104100_1861019655 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 22:56:07,270] ({qtp89387388-319} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 22:56:07,372] ({qtp89387388-1848} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 22:56:07,376] ({pool-2-thread-57} SchedulerFactory.java[jobStarted]:114) - Job 20161017-104100_1861019655 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 22:56:07,377] ({pool-2-thread-57} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20161017-104100_1861019655, interpreter: pyspark, note_id: 2FP1YEJHN, user: anonymous]
 WARN [2020-10-28 22:56:07,388] ({pool-2-thread-57} NotebookServer.java[afterStatusChange]:2316) - Job 20161017-104100_1861019655 is finished, status: ERROR, exception: null, result: %text Fail to execute line 2: print(pow(4, 3))
Traceback (most recent call last):
  File "/tmp/zeppelin_pyspark-3213863130067318734.py", line 380, in <module>
    exec(code, _zcUserQueryNameSpace)
  File "<stdin>", line 2, in <module>
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/sql/functions.py", line 53, in _
    col2._jc if isinstance(col2, Column) else float(col2))
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/java_gateway.py", line 1133, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/sql/utils.py", line 63, in deco
    return f(*a, **kw)
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/protocol.py", line 323, in get_return_value
    format(target_id, ".", name, value))
py4j.protocol.Py4JError: An error occurred while calling z:org.apache.spark.sql.functions.pow. Trace:
py4j.Py4JException: Method pow([class java.lang.Double, class java.lang.Double]) does not exist
	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318)
	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:339)
	at py4j.Gateway.invoke(Gateway.java:274)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:745)



 INFO [2020-10-28 22:56:07,451] ({pool-2-thread-57} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 22:56:07,454] ({pool-2-thread-57} SchedulerFactory.java[jobFinished]:120) - Job 20161017-104100_1861019655 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 22:56:50,571] ({qtp89387388-1637} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 22:56:50,692] ({qtp89387388-1848} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 22:56:50,696] ({pool-2-thread-30} SchedulerFactory.java[jobStarted]:114) - Job 20161017-104100_1861019655 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 22:56:50,697] ({pool-2-thread-30} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20161017-104100_1861019655, interpreter: pyspark, note_id: 2FP1YEJHN, user: anonymous]
 INFO [2020-10-28 22:56:50,709] ({pool-2-thread-30} NotebookServer.java[afterStatusChange]:2314) - Job 20161017-104100_1861019655 is finished successfully, status: FINISHED
 INFO [2020-10-28 22:56:50,751] ({pool-2-thread-30} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 22:56:50,754] ({pool-2-thread-30} SchedulerFactory.java[jobFinished]:120) - Job 20161017-104100_1861019655 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 22:57:05,496] ({qtp89387388-1673} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 22:57:05,572] ({qtp89387388-1637} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 22:57:05,575] ({pool-2-thread-58} SchedulerFactory.java[jobStarted]:114) - Job 20161017-104100_1861019655 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 22:57:05,576] ({pool-2-thread-58} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20161017-104100_1861019655, interpreter: pyspark, note_id: 2FP1YEJHN, user: anonymous]
 INFO [2020-10-28 22:57:05,586] ({pool-2-thread-58} NotebookServer.java[afterStatusChange]:2314) - Job 20161017-104100_1861019655 is finished successfully, status: FINISHED
 INFO [2020-10-28 22:57:05,628] ({pool-2-thread-58} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 22:57:05,631] ({pool-2-thread-58} SchedulerFactory.java[jobFinished]:120) - Job 20161017-104100_1861019655 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 22:57:23,475] ({qtp89387388-1673} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 22:57:23,552] ({qtp89387388-928} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 22:57:23,556] ({pool-2-thread-6} SchedulerFactory.java[jobStarted]:114) - Job 20161017-104100_1861019655 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 22:57:23,557] ({pool-2-thread-6} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20161017-104100_1861019655, interpreter: pyspark, note_id: 2FP1YEJHN, user: anonymous]
 WARN [2020-10-28 22:57:23,568] ({pool-2-thread-6} NotebookServer.java[afterStatusChange]:2316) - Job 20161017-104100_1861019655 is finished, status: ERROR, exception: null, result: %text Fail to execute line 3: ints = range(math.pow(10, 6))
Traceback (most recent call last):
  File "/tmp/zeppelin_pyspark-3213863130067318734.py", line 375, in <module>
    exec(code, _zcUserQueryNameSpace)
  File "<stdin>", line 3, in <module>
TypeError: 'float' object cannot be interpreted as an integer

 INFO [2020-10-28 22:57:23,619] ({pool-2-thread-6} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 22:57:23,622] ({pool-2-thread-6} SchedulerFactory.java[jobFinished]:120) - Job 20161017-104100_1861019655 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 22:57:34,411] ({qtp89387388-1637} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 22:57:34,477] ({qtp89387388-1673} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 22:57:34,480] ({pool-2-thread-59} SchedulerFactory.java[jobStarted]:114) - Job 20161017-104100_1861019655 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 22:57:34,481] ({pool-2-thread-59} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20161017-104100_1861019655, interpreter: pyspark, note_id: 2FP1YEJHN, user: anonymous]
 INFO [2020-10-28 22:57:34,489] ({pool-2-thread-59} NotebookServer.java[afterStatusChange]:2314) - Job 20161017-104100_1861019655 is finished successfully, status: FINISHED
 INFO [2020-10-28 22:57:34,534] ({pool-2-thread-59} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 22:57:34,537] ({pool-2-thread-59} SchedulerFactory.java[jobFinished]:120) - Job 20161017-104100_1861019655 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 22:57:46,821] ({qtp89387388-928} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 22:57:46,903] ({qtp89387388-1637} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 22:57:46,907] ({pool-2-thread-31} SchedulerFactory.java[jobStarted]:114) - Job 20161017-104100_1861019655 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 22:57:46,907] ({pool-2-thread-31} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20161017-104100_1861019655, interpreter: pyspark, note_id: 2FP1YEJHN, user: anonymous]
 INFO [2020-10-28 22:57:47,092] ({pool-2-thread-31} NotebookServer.java[afterStatusChange]:2314) - Job 20161017-104100_1861019655 is finished successfully, status: FINISHED
 INFO [2020-10-28 22:57:47,131] ({pool-2-thread-31} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 22:57:47,134] ({pool-2-thread-31} SchedulerFactory.java[jobFinished]:120) - Job 20161017-104100_1861019655 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 22:58:04,833] ({qtp89387388-1976} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 22:58:04,911] ({qtp89387388-1848} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 22:58:04,915] ({pool-2-thread-60} SchedulerFactory.java[jobStarted]:114) - Job 20161017-104100_1861019655 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 22:58:04,915] ({pool-2-thread-60} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20161017-104100_1861019655, interpreter: pyspark, note_id: 2FP1YEJHN, user: anonymous]
 INFO [2020-10-28 22:58:09,159] ({pool-2-thread-60} NotebookServer.java[afterStatusChange]:2314) - Job 20161017-104100_1861019655 is finished successfully, status: FINISHED
 INFO [2020-10-28 22:58:09,200] ({pool-2-thread-60} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 22:58:09,203] ({pool-2-thread-60} SchedulerFactory.java[jobFinished]:120) - Job 20161017-104100_1861019655 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 22:58:59,518] ({qtp89387388-1673} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 22:58:59,523] ({pool-2-thread-17} SchedulerFactory.java[jobStarted]:114) - Job 20161011-151030_1991021842 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 22:58:59,524] ({pool-2-thread-17} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20161011-151030_1991021842, interpreter: pyspark, note_id: 2FP1YEJHN, user: anonymous]
 WARN [2020-10-28 22:58:59,553] ({pool-2-thread-17} NotebookServer.java[afterStatusChange]:2316) - Job 20161011-151030_1991021842 is finished, status: ERROR, exception: null, result: %text Fail to execute line 1: df = spark.read.json("../../diplodatos_bigdata/ds/people.json")
Traceback (most recent call last):
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/sql/utils.py", line 63, in deco
    return f(*a, **kw)
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/protocol.py", line 319, in get_return_value
    format(target_id, ".", name), value)
py4j.protocol.Py4JJavaError: An error occurred while calling o2231.json.
: org.apache.spark.sql.AnalysisException: Path does not exist: file:/users/ssulca/diplodatos_bigdata/diplodatos_bigdata/ds/people.json;
	at org.apache.spark.sql.execution.datasources.DataSource$.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:626)
	at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$14.apply(DataSource.scala:350)
	at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$14.apply(DataSource.scala:350)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)
	at scala.collection.immutable.List.flatMap(List.scala:344)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:349)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:178)
	at org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:333)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:280)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:745)


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/tmp/zeppelin_pyspark-3213863130067318734.py", line 375, in <module>
    exec(code, _zcUserQueryNameSpace)
  File "<stdin>", line 1, in <module>
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/sql/readwriter.py", line 249, in json
    return self._df(self._jreader.json(self._spark._sc._jvm.PythonUtils.toSeq(path)))
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/java_gateway.py", line 1133, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/interpreter/spark/pyspark/pyspark.zip/pyspark/sql/utils.py", line 69, in deco
    raise AnalysisException(s.split(': ', 1)[1], stackTrace)
pyspark.sql.utils.AnalysisException: 'Path does not exist: file:/users/ssulca/diplodatos_bigdata/diplodatos_bigdata/ds/people.json;'

 INFO [2020-10-28 22:58:59,593] ({pool-2-thread-17} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 22:58:59,600] ({pool-2-thread-17} SchedulerFactory.java[jobFinished]:120) - Job 20161011-151030_1991021842 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 22:59:08,928] ({qtp89387388-1250} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 22:59:09,036] ({qtp89387388-1637} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 22:59:09,040] ({pool-2-thread-61} SchedulerFactory.java[jobStarted]:114) - Job 20161011-151030_1991021842 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 22:59:09,041] ({pool-2-thread-61} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20161011-151030_1991021842, interpreter: pyspark, note_id: 2FP1YEJHN, user: anonymous]
 INFO [2020-10-28 22:59:09,338] ({pool-2-thread-61} NotebookServer.java[afterStatusChange]:2314) - Job 20161011-151030_1991021842 is finished successfully, status: FINISHED
 INFO [2020-10-28 22:59:09,378] ({pool-2-thread-61} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 22:59:09,381] ({pool-2-thread-61} SchedulerFactory.java[jobFinished]:120) - Job 20161011-151030_1991021842 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 22:59:24,577] ({qtp89387388-1250} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 22:59:24,581] ({pool-2-thread-32} SchedulerFactory.java[jobStarted]:114) - Job 20171024-102324_940444908 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 22:59:24,581] ({pool-2-thread-32} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20171024-102324_940444908, interpreter: pyspark, note_id: 2FP1YEJHN, user: anonymous]
 INFO [2020-10-28 22:59:25,009] ({pool-2-thread-32} NotebookServer.java[afterStatusChange]:2314) - Job 20171024-102324_940444908 is finished successfully, status: FINISHED
 INFO [2020-10-28 22:59:25,050] ({pool-2-thread-32} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 22:59:25,054] ({pool-2-thread-32} SchedulerFactory.java[jobFinished]:120) - Job 20171024-102324_940444908 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 23:00:31,428] ({qtp89387388-1250} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 23:07:22,947] ({qtp89387388-2158} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 23:07:22,951] ({qtp89387388-2158} NotebookServer.java[broadcastNewParagraph]:688) - Broadcasting paragraph on run call instead of note.
 INFO [2020-10-28 23:07:29,051] ({qtp89387388-928} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 23:07:38,510] ({qtp89387388-928} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 23:07:38,606] ({qtp89387388-2158} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 23:07:38,610] ({pool-2-thread-62} SchedulerFactory.java[jobStarted]:114) - Job 20201028-230722_941125451 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-sh:shared_process-shared_session
 INFO [2020-10-28 23:07:38,611] ({pool-2-thread-62} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20201028-230722_941125451, interpreter: sh, note_id: 2FP1YEJHN, user: anonymous]
 INFO [2020-10-28 23:07:38,636] ({pool-2-thread-62} NotebookServer.java[afterStatusChange]:2314) - Job 20201028-230722_941125451 is finished successfully, status: FINISHED
 INFO [2020-10-28 23:07:38,681] ({pool-2-thread-62} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 23:07:38,684] ({pool-2-thread-62} SchedulerFactory.java[jobFinished]:120) - Job 20201028-230722_941125451 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-sh:shared_process-shared_session
 INFO [2020-10-28 23:08:43,145] ({qtp89387388-2159} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 23:10:38,299] ({qtp89387388-2159} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 23:10:38,459] ({qtp89387388-2190} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 23:10:38,462] ({pool-2-thread-10} SchedulerFactory.java[jobStarted]:114) - Job 20191128-193328_151154793 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 23:10:38,463] ({pool-2-thread-10} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20191128-193328_151154793, interpreter: pyspark, note_id: 2FP1YEJHN, user: anonymous]
 INFO [2020-10-28 23:10:39,036] ({pool-2-thread-10} NotebookServer.java[afterStatusChange]:2314) - Job 20191128-193328_151154793 is finished successfully, status: FINISHED
 INFO [2020-10-28 23:10:39,075] ({pool-2-thread-10} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 23:10:39,079] ({pool-2-thread-10} SchedulerFactory.java[jobFinished]:120) - Job 20191128-193328_151154793 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 23:12:52,251] ({qtp89387388-1472} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 23:13:39,597] ({qtp89387388-2215} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 23:14:34,286] ({qtp89387388-1472} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 23:14:34,371] ({qtp89387388-2216} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 23:14:34,375] ({pool-2-thread-63} SchedulerFactory.java[jobStarted]:114) - Job 20191128-193328_151154793 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 23:14:34,376] ({pool-2-thread-63} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20191128-193328_151154793, interpreter: pyspark, note_id: 2FP1YEJHN, user: anonymous]
 INFO [2020-10-28 23:14:34,889] ({pool-2-thread-63} NotebookServer.java[afterStatusChange]:2314) - Job 20191128-193328_151154793 is finished successfully, status: FINISHED
 INFO [2020-10-28 23:14:34,931] ({pool-2-thread-63} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 23:14:34,935] ({pool-2-thread-63} SchedulerFactory.java[jobFinished]:120) - Job 20191128-193328_151154793 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 23:18:58,823] ({qtp89387388-1848} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 23:18:58,910] ({qtp89387388-2247} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 23:18:58,914] ({pool-2-thread-33} SchedulerFactory.java[jobStarted]:114) - Job 20191128-193328_151154793 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 23:18:58,915] ({pool-2-thread-33} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20191128-193328_151154793, interpreter: pyspark, note_id: 2FP1YEJHN, user: anonymous]
 INFO [2020-10-28 23:18:59,326] ({pool-2-thread-33} NotebookServer.java[afterStatusChange]:2314) - Job 20191128-193328_151154793 is finished successfully, status: FINISHED
 INFO [2020-10-28 23:18:59,367] ({pool-2-thread-33} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 23:18:59,370] ({pool-2-thread-33} SchedulerFactory.java[jobFinished]:120) - Job 20191128-193328_151154793 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 23:20:20,809] ({qtp89387388-2249} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 23:20:20,893] ({qtp89387388-1637} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 23:20:20,897] ({pool-2-thread-64} SchedulerFactory.java[jobStarted]:114) - Job 20191128-193328_151154793 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 23:20:20,897] ({pool-2-thread-64} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20191128-193328_151154793, interpreter: pyspark, note_id: 2FP1YEJHN, user: anonymous]
 INFO [2020-10-28 23:20:21,377] ({pool-2-thread-64} NotebookServer.java[afterStatusChange]:2314) - Job 20191128-193328_151154793 is finished successfully, status: FINISHED
 INFO [2020-10-28 23:20:21,419] ({pool-2-thread-64} VFSNotebookRepo.java[save]:196) - Saving note:2FP1YEJHN
 INFO [2020-10-28 23:20:21,422] ({pool-2-thread-64} SchedulerFactory.java[jobFinished]:120) - Job 20191128-193328_151154793 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
 INFO [2020-10-28 23:22:21,391] ({qtp89387388-1038} NotebookServer.java[onClose]:372) - Closed connection to 127.0.0.1 : 44480. (1006) WebSocket Read EOF
 INFO [2020-10-28 23:22:24,266] ({Thread-39} ZeppelinServer.java[run]:253) - Shutting down Zeppelin Server ... 
 INFO [2020-10-28 23:22:24,276] ({Thread-39} AbstractConnector.java[doStop]:341) - Stopped ServerConnector@f4a3a8d{HTTP/1.1,[http/1.1]}{127.0.0.1:9322}
 INFO [2020-10-28 23:22:24,276] ({Thread-39} HouseKeeper.java[stopScavenging]:167) - node0 Stopped scavenging
 INFO [2020-10-28 23:22:24,594] ({Exec Default Executor} RemoteInterpreterManagedProcess.java[onProcessFailed]:250) - Interpreter process failed {}
org.apache.commons.exec.ExecuteException: Process exited with an error: 130 (Exit value: 130)
	at org.apache.commons.exec.DefaultExecutor.executeInternal(DefaultExecutor.java:404)
	at org.apache.commons.exec.DefaultExecutor.access$200(DefaultExecutor.java:48)
	at org.apache.commons.exec.DefaultExecutor$1.run(DefaultExecutor.java:200)
	at java.lang.Thread.run(Thread.java:745)
 INFO [2020-10-28 23:22:24,596] ({Exec Default Executor} RemoteInterpreterManagedProcess.java[onProcessFailed]:250) - Interpreter process failed {}
org.apache.commons.exec.ExecuteException: Process exited with an error: 130 (Exit value: 130)
	at org.apache.commons.exec.DefaultExecutor.executeInternal(DefaultExecutor.java:404)
	at org.apache.commons.exec.DefaultExecutor.access$200(DefaultExecutor.java:48)
	at org.apache.commons.exec.DefaultExecutor$1.run(DefaultExecutor.java:200)
	at java.lang.Thread.run(Thread.java:745)
 INFO [2020-10-28 23:22:24,878] ({Exec Default Executor} RemoteInterpreterManagedProcess.java[onProcessFailed]:250) - Interpreter process failed {}
org.apache.commons.exec.ExecuteException: Process exited with an error: 130 (Exit value: 130)
	at org.apache.commons.exec.DefaultExecutor.executeInternal(DefaultExecutor.java:404)
	at org.apache.commons.exec.DefaultExecutor.access$200(DefaultExecutor.java:48)
	at org.apache.commons.exec.DefaultExecutor$1.run(DefaultExecutor.java:200)
	at java.lang.Thread.run(Thread.java:745)
 INFO [2020-10-28 23:22:26,129] ({Thread-39} ContextHandler.java[doStop]:1045) - Stopped o.e.j.w.WebAppContext@15761df8{zeppelin-web,/,null,UNAVAILABLE}{/users/ssulca/diplodatos_bigdata/spark/zeppelin-0.8.2-bin-all/zeppelin-web-0.8.2.war}
 INFO [2020-10-28 23:22:26,131] ({Thread-2018} InterpreterSetting.java[close]:483) - Close InterpreterSetting: ignite
 INFO [2020-10-28 23:22:26,132] ({Thread-2017} InterpreterSetting.java[close]:483) - Close InterpreterSetting: ignite
 INFO [2020-10-28 23:22:26,132] ({Thread-2019} InterpreterSetting.java[close]:483) - Close InterpreterSetting: python
 INFO [2020-10-28 23:22:26,132] ({Thread-2020} InterpreterSetting.java[close]:483) - Close InterpreterSetting: python
 INFO [2020-10-28 23:22:26,132] ({Thread-2019} ManagedInterpreterGroup.java[close]:89) - Close InterpreterGroup: python:shared_process
 INFO [2020-10-28 23:22:26,132] ({Thread-2022} InterpreterSetting.java[close]:483) - Close InterpreterSetting: sap
 INFO [2020-10-28 23:22:26,132] ({Thread-2021} InterpreterSetting.java[close]:483) - Close InterpreterSetting: sap
 INFO [2020-10-28 23:22:26,133] ({Thread-2025} InterpreterSetting.java[close]:483) - Close InterpreterSetting: lens
 INFO [2020-10-28 23:22:26,133] ({Thread-2023} InterpreterSetting.java[close]:483) - Close InterpreterSetting: jdbc
 INFO [2020-10-28 23:22:26,133] ({Thread-2026} InterpreterSetting.java[close]:483) - Close InterpreterSetting: pig
 INFO [2020-10-28 23:22:26,132] ({Thread-2019} ManagedInterpreterGroup.java[close]:100) - Close Session: shared_session for interpreter setting: python
 INFO [2020-10-28 23:22:26,132] ({Thread-2024} InterpreterSetting.java[close]:483) - Close InterpreterSetting: jdbc
 INFO [2020-10-28 23:22:26,135] ({Thread-2035} InterpreterSetting.java[close]:483) - Close InterpreterSetting: elasticsearch
 INFO [2020-10-28 23:22:26,135] ({Thread-2040} InterpreterSetting.java[close]:483) - Close InterpreterSetting: neo4j
 INFO [2020-10-28 23:22:26,135] ({Thread-2039} InterpreterSetting.java[close]:483) - Close InterpreterSetting: groovy
 INFO [2020-10-28 23:22:26,134] ({Thread-2038} InterpreterSetting.java[close]:483) - Close InterpreterSetting: livy
 INFO [2020-10-28 23:22:26,135] ({Thread-2034} InterpreterSetting.java[close]:483) - Close InterpreterSetting: file
 INFO [2020-10-28 23:22:26,136] ({Thread-2049} InterpreterSetting.java[close]:483) - Close InterpreterSetting: hbase
 INFO [2020-10-28 23:22:26,134] ({Thread-2037} InterpreterSetting.java[close]:483) - Close InterpreterSetting: angular
 INFO [2020-10-28 23:22:26,134] ({Thread-2033} InterpreterSetting.java[close]:483) - Close InterpreterSetting: neo4j
 INFO [2020-10-28 23:22:26,137] ({Thread-2051} InterpreterSetting.java[close]:483) - Close InterpreterSetting: spark
 INFO [2020-10-28 23:22:26,137] ({Thread-2055} InterpreterSetting.java[close]:483) - Close InterpreterSetting: alluxio
 INFO [2020-10-28 23:22:26,134] ({Thread-2036} InterpreterSetting.java[close]:483) - Close InterpreterSetting: flink
 INFO [2020-10-28 23:22:26,134] ({Thread-2032} InterpreterSetting.java[close]:483) - Close InterpreterSetting: pig
 INFO [2020-10-28 23:22:26,134] ({Thread-2030} InterpreterSetting.java[close]:483) - Close InterpreterSetting: livy
 INFO [2020-10-28 23:22:26,134] ({Thread-2031} InterpreterSetting.java[close]:483) - Close InterpreterSetting: groovy
 INFO [2020-10-28 23:22:26,134] ({Thread-2028} InterpreterSetting.java[close]:483) - Close InterpreterSetting: lens
 INFO [2020-10-28 23:22:26,133] ({Thread-2029} InterpreterSetting.java[close]:483) - Close InterpreterSetting: angular
 INFO [2020-10-28 23:22:26,133] ({Thread-2027} InterpreterSetting.java[close]:483) - Close InterpreterSetting: flink
 INFO [2020-10-28 23:22:26,138] ({Thread-2058} InterpreterSetting.java[close]:483) - Close InterpreterSetting: kylin
 INFO [2020-10-28 23:22:26,138] ({Thread-2057} InterpreterSetting.java[close]:483) - Close InterpreterSetting: hbase
 INFO [2020-10-28 23:22:26,137] ({Thread-2056} InterpreterSetting.java[close]:483) - Close InterpreterSetting: bigquery
 INFO [2020-10-28 23:22:26,137] ({Thread-2051} ManagedInterpreterGroup.java[close]:89) - Close InterpreterGroup: spark:shared_process
 INFO [2020-10-28 23:22:26,137] ({Thread-2054} InterpreterSetting.java[close]:483) - Close InterpreterSetting: md
 INFO [2020-10-28 23:22:26,137] ({Thread-2053} InterpreterSetting.java[close]:483) - Close InterpreterSetting: sh
 INFO [2020-10-28 23:22:26,137] ({Thread-2052} InterpreterSetting.java[close]:483) - Close InterpreterSetting: cassandra
 INFO [2020-10-28 23:22:26,136] ({Thread-2045} InterpreterSetting.java[close]:483) - Close InterpreterSetting: elasticsearch
 INFO [2020-10-28 23:22:26,136] ({Thread-2050} InterpreterSetting.java[close]:483) - Close InterpreterSetting: kylin
 INFO [2020-10-28 23:22:26,136] ({Thread-2048} InterpreterSetting.java[close]:483) - Close InterpreterSetting: bigquery
 INFO [2020-10-28 23:22:26,136] ({Thread-2047} InterpreterSetting.java[close]:483) - Close InterpreterSetting: alluxio
 INFO [2020-10-28 23:22:26,136] ({Thread-2044} InterpreterSetting.java[close]:483) - Close InterpreterSetting: sh
 INFO [2020-10-28 23:22:26,136] ({Thread-2046} InterpreterSetting.java[close]:483) - Close InterpreterSetting: md
 INFO [2020-10-28 23:22:26,135] ({Thread-2041} InterpreterSetting.java[close]:483) - Close InterpreterSetting: file
 INFO [2020-10-28 23:22:26,135] ({Thread-2043} InterpreterSetting.java[close]:483) - Close InterpreterSetting: cassandra
 INFO [2020-10-28 23:22:26,135] ({Thread-2042} InterpreterSetting.java[close]:483) - Close InterpreterSetting: spark
 WARN [2020-10-28 23:22:26,135] ({Thread-2019} RemoteInterpreter.java[close]:199) - close is called when RemoterInterpreter is not opened for org.apache.zeppelin.python.PythonInterpreter
 INFO [2020-10-28 23:22:26,140] ({Thread-2053} ManagedInterpreterGroup.java[close]:89) - Close InterpreterGroup: sh:shared_process
 WARN [2020-10-28 23:22:26,142] ({Thread-2019} RemoteInterpreter.java[close]:199) - close is called when RemoterInterpreter is not opened for org.apache.zeppelin.python.IPythonInterpreter
 INFO [2020-10-28 23:22:26,140] ({Thread-2054} ManagedInterpreterGroup.java[close]:89) - Close InterpreterGroup: md:shared_process
 INFO [2020-10-28 23:22:26,140] ({Thread-2051} ManagedInterpreterGroup.java[close]:100) - Close Session: shared_session for interpreter setting: spark
 INFO [2020-10-28 23:22:26,142] ({Thread-2054} ManagedInterpreterGroup.java[close]:100) - Close Session: shared_session for interpreter setting: md
 WARN [2020-10-28 23:22:26,142] ({Thread-2019} RemoteInterpreter.java[close]:199) - close is called when RemoterInterpreter is not opened for org.apache.zeppelin.python.PythonInterpreterPandasSql
 INFO [2020-10-28 23:22:26,142] ({Thread-2053} ManagedInterpreterGroup.java[close]:100) - Close Session: shared_session for interpreter setting: sh
 WARN [2020-10-28 23:22:26,143] ({Thread-2019} RemoteInterpreter.java[close]:199) - close is called when RemoterInterpreter is not opened for org.apache.zeppelin.python.PythonCondaInterpreter
 WARN [2020-10-28 23:22:26,142] ({Thread-2051} RemoteInterpreter.java[close]:199) - close is called when RemoterInterpreter is not opened for org.apache.zeppelin.spark.SparkInterpreter
 WARN [2020-10-28 23:22:26,143] ({Thread-2019} RemoteInterpreter.java[close]:199) - close is called when RemoterInterpreter is not opened for org.apache.zeppelin.python.PythonDockerInterpreter
 WARN [2020-10-28 23:22:26,144] ({Thread-2051} RemoteInterpreter.java[close]:199) - close is called when RemoterInterpreter is not opened for org.apache.zeppelin.spark.SparkSqlInterpreter
 INFO [2020-10-28 23:22:26,145] ({Thread-2019} ManagedInterpreterGroup.java[close]:105) - Remove this InterpreterGroup: python:shared_process as all the sessions are closed
 WARN [2020-10-28 23:22:26,145] ({Thread-2051} RemoteInterpreter.java[close]:199) - close is called when RemoterInterpreter is not opened for org.apache.zeppelin.spark.DepInterpreter
 INFO [2020-10-28 23:22:26,145] ({Thread-2020} ManagedInterpreterGroup.java[close]:89) - Close InterpreterGroup: python:shared_process
 INFO [2020-10-28 23:22:26,151] ({Thread-2042} ManagedInterpreterGroup.java[close]:89) - Close InterpreterGroup: spark:shared_process
 INFO [2020-10-28 23:22:26,151] ({Thread-2044} ManagedInterpreterGroup.java[close]:89) - Close InterpreterGroup: sh:shared_process
 INFO [2020-10-28 23:22:26,151] ({Thread-2046} ManagedInterpreterGroup.java[close]:89) - Close InterpreterGroup: md:shared_process
 INFO [2020-10-28 23:22:26,152] ({Thread-39} NotebookRepoSync.java[close]:428) - Closing all notebook storages
 INFO [2020-10-28 23:22:29,163] ({Thread-39} ZeppelinServer.java[run]:264) - Bye
