{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diplomatura en Ciencia de Datos, Aprendizaje Automático y sus Aplicaciones\n",
    "## Programación Distribuida sobre Grandes Volúmenes de Datos\n",
    "\n",
    "Damián Barsotti \n",
    "\n",
    "### Facultad de Matemática Astronomía Física y Computación\n",
    "## Universidad Nacional de Córdoba\n",
    "\n",
    "<img src=\"http://program.ar/wp-content/uploads/2018/07/logo-UNC-FAMAF.png\" alt=\"Drawing\" style=\"width:80%;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Core\n",
    "---\n",
    "Veremos conceptos básicos  aplicables a otras librerías de [Spark](http://spark.apache.org):\n",
    "    \n",
    "<img \n",
    "src=\"https://bitbucket.org/bigdata_famaf/diplodatos_bigdata/raw/b17129f7118b3389b8c7f2f85fd89c6238fe0edd/clases/02_spark_core/core_stack.png\" alt=\"Drawing\" style=\"width: 70%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.- Conceptos básicos\n",
    "---\n",
    "\n",
    "### Driver\n",
    "\n",
    "Toda aplicación Spark tiene un programa **driver**:\n",
    "\n",
    "* lanza las operaciones en el cluster,\n",
    "* contiene nuestro **programa**\n",
    "    - define datos distribuidos y les aplica operaciones.\n",
    "\n",
    "> En Zeppelin escribimos un *programa driver* que de forma interactiva ejecuta las operaciones que queremos correr.\n",
    "\n",
    "### Executors\n",
    "\n",
    "El driver maneja y envía tareas a **executors** en los nodos del cluster (o threads en modo local).\n",
    "\n",
    "<img \n",
    "src=\"https://bitbucket.org/bigdata_famaf/diplodatos_bigdata/raw/b17129f7118b3389b8c7f2f85fd89c6238fe0edd/clases/01_intro_spark/driver_exec.png\" alt=\"Drawing\" style=\"width: 40%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SparkContext\n",
    "\n",
    "* Los programas en el driver se conectan al cluster Spark a través de un objeto `SparkContext`\n",
    "* Le dice a Spark como conectarce con el cluster (o a los distintos threads en modo local)\n",
    "    - (representa la conección al cluster) \n",
    "* En Zeppelin (y shell) está predefinida la variable `sc` de tipo `SparkContext`\n",
    "    - otros programas deben crearla con `new`\n",
    "    \n",
    "<img \n",
    "src=\"https://bitbucket.org/bigdata_famaf/diplodatos_bigdata/raw/b17129f7118b3389b8c7f2f85fd89c6238fe0edd/clases/02_spark_core/cluster-overview.png\" alt=\"Drawing\" style=\"width: 60%;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"02_core\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "local[*]\n"
     ]
    }
   ],
   "source": [
    "print(sc.defaultParallelism)\n",
    "print(sc.master)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|master            |descripción                                               |\n",
    "|------------------|----------------------------------------------------------|\n",
    "|local             |Spark corre localmente con un solo worker (no paralelismo)|\n",
    "|local[K]          |Spark corre localmente con K threads                      |\n",
    "|spark://HOST:PORT |se conecta a un cluster Spark                             |\n",
    "|mesos://HOST:PORT |se conecta a un cluster Mesos                             |\n",
    "|yarn              |se conecta a un cluster Hadoop Yarn                       |\n",
    "|...               |...                                                       |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.- Resilient Distributed Dataset (RDD)\n",
    "---\n",
    "\n",
    "* **Contenedores** de objetos **inmutables**, distribuidos en el cluster (contiene los datos)\n",
    "\n",
    "* Creados con el SparkContext `sc`.\n",
    "    - al cargar datasets a Spark\n",
    "    - por transformaciones comunes (`map`, `filter`, ...) o binarias (`union`, `intersection`, ...).\n",
    "\n",
    "* Ante fallas se reconstruyen (resilencia).\n",
    "* **Importante**: todo lo que no derive del `SparkContext` corre solo en el **driver**.\n",
    "\n",
    "### Ejemplo log analysis\n",
    "\n",
    "    \n",
    "<img \n",
    "src=\"https://bitbucket.org/bigdata_famaf/diplodatos_bigdata/raw/b17129f7118b3389b8c7f2f85fd89c6238fe0edd/clases/02_spark_core/log_linage.png\" alt=\"Drawing\" style=\"width: 40%;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linea 0: ERROR [2020-10-24 12:10:33,336] ({dispatcher-event-loop-6} Logging.scala[logError]:70) - Lost executor driver on localhost: Executor heartbeat timed out after 127728 ms\n",
      "Linea 1: ERROR [2020-10-23 21:28:38,673] ({pool-2-thread-2} NewSparkInterpreter.java[open]:127) - Fail to open SparkInterpreter\n",
      "Linea 2: ERROR [2020-10-23 21:28:38,673] ({pool-2-thread-2} PySparkInterpreter.java[open]:196) - Error\n",
      "Linea 3: ERROR [2020-10-23 21:28:38,674] ({pool-2-thread-2} Job.java[run]:190) - Job failed\n",
      "Linea 4: ERROR [2020-10-23 21:30:23,981] ({pool-2-thread-2} NewSparkInterpreter.java[open]:127) - Fail to open SparkInterpreter\n",
      "Linea 5: ERROR [2020-10-23 21:30:23,981] ({pool-2-thread-2} PySparkInterpreter.java[open]:196) - Error\n",
      "Linea 6: ERROR [2020-10-23 21:30:23,982] ({pool-2-thread-2} Job.java[run]:190) - Job failed\n",
      "Linea 7: ERROR [2020-10-23 21:34:22,264] ({pool-2-thread-4} NewSparkInterpreter.java[open]:127) - Fail to open SparkInterpreter\n",
      "Linea 8: ERROR [2020-10-23 21:34:22,264] ({pool-2-thread-4} PySparkInterpreter.java[open]:196) - Error\n",
      "Linea 9: ERROR [2020-10-23 21:34:22,265] ({pool-2-thread-4} Job.java[run]:190) - Job failed\n"
     ]
    }
   ],
   "source": [
    "inputRDD = sc.textFile(\"../inputs/logs/\") # RDD de entrada\n",
    "\n",
    "# se crea un nuevo RDD:\n",
    "errorRDD = inputRDD.filter(lambda line: \"ERROR\" in line) \n",
    "# se crea otro nuevo RDD\n",
    "configRDD = inputRDD.filter(lambda line: \"config\" in line) \n",
    "\n",
    "errOrConfRDD = errorRDD.union(configRDD) \n",
    "\n",
    "for ln, l in enumerate(errOrConfRDD.collect()[:10]):\n",
    "    print(\"Linea {}:\".format(ln), l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementación\n",
    "\n",
    "* El RDD se distribuye en **particiones** en nodos del cluster (o fs local).\n",
    "* Se construye el **grafo de operaciones**.\n",
    "* Las operaciones de dividen en **tasks** (tareas).\n",
    "* A cada **partición** se le aplica una **task**.\n",
    "* Las tareas son ejecutadas por los executors en nodos (o threads locales)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio\n",
    "Cree una celda nueva y copie en ella el último programa sin las líneas 13 en adelante.\n",
    "Observe en Spark UI las tareas ejecutadas.\n",
    "\n",
    "Rta:\n",
    "No se Ejecuta ninguna tarea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputRDD = sc.textFile(\"../inputs/logs/\") # RDD de entrada\n",
    "\n",
    "# se crea un nuevo RDD:\n",
    "errorRDD = inputRDD.filter(lambda line: \"ERROR\" in line) \n",
    "# se crea otro nuevo RDD\n",
    "configRDD = inputRDD.filter(lambda line: \"config\" in line) \n",
    "errOrConfRDD = errorRDD.union(configRDD) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.- Evaluación Lazy\n",
    "---\n",
    "\n",
    "En Spark todas las **transformaciones** (`map`, `filter`, `union`, etc.) son evaluadas de forma **lazy**:\n",
    "\n",
    "* son acumuladas como *grafo de operaciones*\n",
    "* se ejecutan al momento de traer los datos al driver (`collect`, `take`, etc.)\n",
    "    - se llama a una **acción**.\n",
    "\n",
    "Esto permite:\n",
    "\n",
    "* hacer **optimizaciones**\n",
    "    - se computa solo lo que hace falta (tiene mucho sentido en Big Data)\n",
    "    - se hace un *pipeling* de transformaciones sin guardar resultados intermedios \n",
    "* recalcular las dependencias si hay algún fallo (resilencia)\n",
    "\n",
    "### Logs análisis (muestra solo 2 lineas)\n",
    "Ahora se ejecutan  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linea 0: ERROR [2020-10-24 12:10:33,336] ({dispatcher-event-loop-6} Logging.scala[logError]:70) - Lost executor driver on localhost: Executor heartbeat timed out after 127728 ms\n",
      "Linea 1: ERROR [2020-10-23 21:28:38,673] ({pool-2-thread-2} NewSparkInterpreter.java[open]:127) - Fail to open SparkInterpreter\n"
     ]
    }
   ],
   "source": [
    "inputRDD = sc.textFile(\"../inputs/logs/\") # RDD de entrada\n",
    "errorRDD = inputRDD.filter(lambda line: \"ERROR\" in line) #  se crea un nuevo RDD\n",
    "configRDD = inputRDD.filter(lambda line: \"config\" in line) # se crea un nuevo RDD\n",
    "\n",
    "errOrConfRDD = errorRDD.union(configRDD) \n",
    "\n",
    "for ln, l in enumerate(errOrConfRDD.take(2)): # take(2) en vez de collect\n",
    "    print(\"Linea {}:\".format(ln), l)\n",
    "\n",
    "# Compara con primer programa en Spark UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio\n",
    "\n",
    "Complete los `...` en el siguiente programa para contar la cantidad de veces que aparece la letra 'c' en los archivos en `../inputs/logs/`.\n",
    "\n",
    "#### Ayuda\n",
    "\n",
    "\n",
    "* Se puede usar el método `.filter` (ya visto en ejemplos anteriores) para crear un RDD solo con la letra C.\n",
    "* El método `count` de RDD cuenta la cantidad de elementos.\n",
    "* La letra 'c' se escribe `'c'` en Scala."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aparecen 775948 letras c en los logs.\n"
     ]
    }
   ],
   "source": [
    "linesRDD = sc.textFile(\"../inputs/logs/\")\n",
    "\n",
    "# lugar de tener una rdd de linas la idea es tener un\n",
    "# rdd de characters\n",
    "charsRDD = linesRDD.flatMap(lambda l: l) \n",
    "onlyCRDD = charsRDD.filter(lambda car: car=='c')\n",
    "\n",
    "print(\"Aparecen {} letras c en los logs.\".format(onlyCRDD.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.- Persistencia\n",
    "---\n",
    "Spark **recomputa** el grafo de dependencias cuando se llama una **acción**:\n",
    "\n",
    "Entonces en lugar de reutilizar los valores, los recalcula de nuevo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La media es  285.1666666666667\n",
      "0\n",
      "1\n",
      "4\n",
      "9\n",
      "16\n",
      "25\n",
      "36\n",
      "49\n",
      "64\n",
      "81\n",
      "100\n",
      "121\n",
      "144\n",
      "169\n",
      "196\n",
      "225\n",
      "256\n",
      "289\n",
      "324\n",
      "361\n",
      "400\n",
      "441\n",
      "484\n",
      "529\n",
      "576\n",
      "625\n",
      "676\n",
      "729\n",
      "784\n",
      "841\n"
     ]
    }
   ],
   "source": [
    "input = sc.parallelize(range(30)) # Se crea la lista [0,...,29] y se lo convierte en RDD \n",
    "\n",
    "result = input.map(lambda x: x*x)\n",
    "\n",
    "print(\"La media es \", result.mean()) # computa los cuadrados\n",
    "\n",
    "for r in result.collect():\n",
    "     print(r) # recomputa los cuadrados :("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.- Implementación API Python\n",
    "---\n",
    "\n",
    "Spark paraleliza tambien la laectura, ya que cada thread va a a leer una parte\n",
    "distita de un archivo.\n",
    "\n",
    "* Spark esta originalmente implementado en Scala/Java.\n",
    "* `SparkContext` de Python usa [Py4J](https://www.py4j.org/), lanza JVM local y crea `JavaSparkContext`.\n",
    "* [Py4J](https://www.py4j.org/) solo se usa en driver.\n",
    "* En máquinas remotas los executors corren en JVM asegurando resilencia.\n",
    "* Estas JVM lanzan procesos Python.\n",
    "* [Mas info](https://medium.com/@ketanvatsalya/a-scenic-route-through-pyspark-internals-feaf74ed660d).\n",
    "\n",
    "\n",
    "    \n",
    "<img \n",
    "src=\"https://bitbucket.org/bigdata_famaf/diplodatos_bigdata/raw/b17129f7118b3389b8c7f2f85fd89c6238fe0edd/clases/02_spark_core/python-spark.png\" alt=\"Drawing\" style=\"width: 60%;\"/>\n",
    "\n",
    "\n",
    "### Ejercicio\n",
    "\n",
    "Complete el siguiente programa para que cuente la cantidad de lineas que comienzan con la palabra `INFO`, `WARN` y `ERROR`.\n",
    "\n",
    "También, haga cache de los RDD para hacer el programa más eficiente. \n",
    "\n",
    "Cuando se ponde `chache` se le dice a los nodos que guarden la parte de rrdd\n",
    "que tiene cada nodo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de lineas INFO:  205710\n",
      "Cantidad de lineas WARN:  1690\n",
      "Cantidad de lineas ERROR:  104\n"
     ]
    }
   ],
   "source": [
    "linesRDD = sc.textFile(\"../inputs/logs/\") # RDD de entrada\n",
    "linesStrip = linesRDD.map(lambda l: l.strip()).cache() # Borro espacios en borde\n",
    "linesInfo = linesStrip.filter(lambda l: l.startswith(\"INFO\"))\n",
    "linesWarn = linesStrip.filter(lambda l: l.startswith(\"WARN\"))\n",
    "linesError = linesStrip.filter(lambda l: l.startswith(\"ERROR\"))\n",
    "\n",
    "print(\"Cantidad de lineas INFO: \", linesInfo.count())\n",
    "\n",
    "print(\"Cantidad de lineas WARN: \", linesWarn.count()) #Completar\n",
    "\n",
    "print(\"Cantidad de lineas ERROR: \", linesError.count())  #Completar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio\n",
    "\n",
    "El archivo en `../inputs/ds/flights.csv` contiene información de vuelos realizados en 2008 (solo 100.000), uno por línea.\n",
    "\n",
    "Los datos estan separados por coma y la columna `Cancelled` (la 22) tiene un `1` si el vuelo fue cancelado. Además si el vuelo fue redirigido se indica con '1' en la columna `Diverted` (la 24).\n",
    "\n",
    "Completar el siguiente programa que devuelve el porcentaje de vuelos cancelados y el porcentaje de redirigidos.\n",
    "\n",
    "Utilizar cache si lo cree conveniente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Year: string (nullable = true)\n",
      " |-- Month: string (nullable = true)\n",
      " |-- DayofMonth: string (nullable = true)\n",
      " |-- DayOfWeek: string (nullable = true)\n",
      " |-- DepTime: string (nullable = true)\n",
      " |-- CRSDepTime: string (nullable = true)\n",
      " |-- ArrTime: string (nullable = true)\n",
      " |-- CRSArrTime: string (nullable = true)\n",
      " |-- UniqueCarrier: string (nullable = true)\n",
      " |-- FlightNum: string (nullable = true)\n",
      " |-- TailNum: string (nullable = true)\n",
      " |-- ActualElapsedTime: string (nullable = true)\n",
      " |-- CRSElapsedTime: string (nullable = true)\n",
      " |-- AirTime: string (nullable = true)\n",
      " |-- ArrDelay: string (nullable = true)\n",
      " |-- DepDelay: string (nullable = true)\n",
      " |-- Origin: string (nullable = true)\n",
      " |-- Dest: string (nullable = true)\n",
      " |-- Distance: string (nullable = true)\n",
      " |-- TaxiIn: string (nullable = true)\n",
      " |-- TaxiOut: string (nullable = true)\n",
      " |-- Cancelled: string (nullable = true)\n",
      " |-- CancellationCode: string (nullable = true)\n",
      " |-- Diverted: string (nullable = true)\n",
      " |-- CarrierDelay: string (nullable = true)\n",
      " |-- WeatherDelay: string (nullable = true)\n",
      " |-- NASDelay: string (nullable = true)\n",
      " |-- SecurityDelay: string (nullable = true)\n",
      " |-- LateAircraftDelay: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\")\\\n",
    "        .option(\"header\", \"true\")\\\n",
    "        .load(\"../inputs/ds/flights.csv\")\\\n",
    "        .sample(False, 0.0005)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cancelados = 1.142%\n",
      "redireccionados = 0.16%\n"
     ]
    }
   ],
   "source": [
    "input = sc.textFile(\"../inputs/ds/flights.csv\") # Completar el path\n",
    "\n",
    "nTotal = input.count() - 1 # la primer fila tiene el nombre de las columnas\n",
    "parsed = input.map(lambda l: l.split(\",\")).cache()\n",
    "cancel = parsed.filter(lambda l: l[21] == '1') # Completar\n",
    "redir = parsed.filter(lambda l: l[23] == '1') # Completar\n",
    "\n",
    "nCancel = cancel.count()#.cache()\n",
    "nRedir = redir.count() #.cache()\n",
    "\n",
    "print(\"cancelados = {}%\".format(float(nCancel) * 100 / nTotal))\n",
    "print(\"redireccionados = {}%\".format(float(nRedir) * 100 / nTotal)) # Completar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio\n",
    "\n",
    "La columna 14 del mismo archivo tiene el tiempo del vuelo en minutos. Calcular el máximo.\n",
    "\n",
    "#### Ayuda\n",
    "\n",
    "* Busque en la documentacion de la [API RDD](http://spark.apache.org/docs/2.2.1/api/python/pyspark.html#pyspark.RDD) una acción para calcular el máximo.\n",
    "* Ojo que puede haber valores no definidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'126'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.textFile(\"../inputs/ds/flights.csv\") \\\n",
    "    .map(lambda line: line.split(\",\")) \\\n",
    "    .filter(lambda line: line[13].isdigit() ) \\\n",
    "    .max()[13]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fin"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
