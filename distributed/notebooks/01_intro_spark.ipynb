{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<center>\n",
    "    <h1><a href=\"http://diplodatos.famaf.unc.edu.ar/\">Diplomatura en Ciencia de Datos, Aprendizaje Automático y sus Aplicaciones</a></h1>\n",
    "    <h2>Curso <a href=\"https://sites.google.com/view/eleccion-optativas-diplodatos/programaci%C3%B3n-distribu%C3%ADda-sobre-grandes-vol%C3%BAmenes-de-datos\">Programación Distribuida sobre Grandes Volúmenes de Datos</a></h2>\n",
    "</center>\n",
    "\n",
    "<br>\n",
    "\n",
    "<h3 style=\"text-align:center;\"> Damián Barsotti  </h3>\n",
    "\n",
    "<h3 style=\"text-align:center;\">\n",
    "    <a href=\"http://www.famaf.unc.edu.ar\">\n",
    "    Facultad de Matemática Astronomía Física y Computación\n",
    "    </a>\n",
    "<br/>\n",
    "    <a href=\"http://www.unc.edu.ar\">\n",
    "    Universidad Nacional de Córdoba\n",
    "    </a>\n",
    "<br/>\n",
    "    <center>\n",
    "    <a href=\"http://www.famaf.unc.edu.ar\">\n",
    "    <img src=\"https://www.famaf.unc.edu.ar/static/assets/logosFooterBottom.svg\" alt=\"Drawing\" style=\"width:50%;\"/>\n",
    "    </a>\n",
    "    </center>\n",
    "</h3>\n",
    "\n",
    "<p style=\"font-size:15px;\">\n",
    "    <br />\n",
    "        This work is licensed under a\n",
    "        <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.\n",
    "    <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">\n",
    "        <img alt=\"Creative Commons License\" style=\"border-width:0;vertical-align:middle;float:right\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png\" />\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción a Spark\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Características\n",
    "\n",
    "* 100x más rápido que Hadoop MapReduce en memoria.\n",
    "* 10x más rápido en disco.\n",
    "\n",
    "  ![](https://bitbucket.org/bigdata_famaf/diplodatos_bigdata/raw/HEAD/clases/01_intro_spark/spark_speed.png)\n",
    "  \n",
    "* Multiplataforma\n",
    "    * Corre en Hadoop Yarn, Mesos, standalone o en la nube (AWS, Azure, ...)\n",
    "    * Acceso a datos en HDFS, Cassandra, HBase, Hive, Tachyon, JDBC, etc.\n",
    "![](https://bitbucket.org/bigdata_famaf/diplodatos_bigdata/raw/HEAD/clases/01_intro_spark/spark_multi_plataforma.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Múltiples funcionalidades en una plataforma (Stack unificado)\n",
    "\n",
    "<img src=\"https://bitbucket.org/bigdata_famaf/diplodatos_bigdata/raw/b17129f7118b3389b8c7f2f85fd89c6238fe0edd/clases/01_intro_spark/unified_stack.png\" alt=\"Drawing\" style=\"width: 80%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Count (MapReduce)\n",
    "\n",
    "```java\n",
    "public class WordCount {\n",
    "\tpublic static class Map extends MapReduceBase implements Mapper<LongWritable, Text, Text, IntWritable> {\n",
    "\n",
    "\t\tprivate final static IntWritable one = new IntWritable(1);\n",
    "\t\tprivate Text word = new Text();\n",
    "        \n",
    "\t\tpublic void map(LongWritable key, Text value, OutputCollector<Text, IntWritable> output, Reporter reporter) throws IOException {\n",
    "\n",
    "\t\t\tString line = value.toString();\n",
    "\t\t\tStringTokenizer tokenizer = new StringTokenizer(line);\n",
    "\n",
    "\t\t\twhile (tokenizer.hasMoreTokens()) {\n",
    "\t\t\t\tword.set(tokenizer.nextToken());\n",
    "\t\t\t\toutput.collect(word, one);\n",
    "\t\t\t}\n",
    "\t\t}\n",
    "\t}\n",
    "\n",
    "\tpublic static class Reduce extends MapReduceBase implements Reducer<Text, IntWritable, Text, IntWritable> {\n",
    "\n",
    "\t\tpublic void reduce(Text key, Iterator values, OutputCollector<Text, IntWritable> output, Reporter reporter) throws IOException {\n",
    "\n",
    "\t\t\tint sum = 0;\n",
    "\t\t\twhile (values.hasNext()) {\n",
    "\t\t\t\tsum += values.next().get();\n",
    "\t\t\t}\n",
    "\t\t\toutput.collect(key, new IntWritable(sum));\n",
    "\t\t}\n",
    "\t}\n",
    "\n",
    "\tpublic static void main(String[] args) throws Exception {\n",
    "\n",
    "\t\tJobConf conf = new JobConf(WordCount.class);\n",
    "\t\tconf.setJobName(\"wordcount\");\n",
    "\t\tconf.setOutputKeyClass(Text.class);\n",
    "\t\tconf.setOutputValueClass(IntWritable.class);\n",
    "\t\tconf.setMapperClass(Map.class);\n",
    "\t\tconf.setCombinerClass(Reduce.class);\n",
    "\t\tconf.setReducerClass(Reduce.class);\n",
    "\t\tconf.setInputFormat(TextInputFormat.class);\n",
    "\t\tconf.setOutputFormat(TextOutputFormat.class);\n",
    "\t\tFileInputFormat.setInputPaths(conf, new Path(args[0]));\n",
    "\t\tFileOutputFormat.setOutputPath(conf, new Path(args[1]));\n",
    "\t\tJobClient.runJob(conf);\n",
    "\t}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"01_intro\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Count (Spark)\n",
    "\n",
    "* `lines` es un **array distribuido** de lineas de texto (`RDD[str]`).\n",
    "    - una parte del arreglo en cada **nodo del cluster**.\n",
    "\n",
    "* `lines` tiene el método `flatMap` (línea 6):\n",
    "    - `flatMap(lambda line: line.split(\" \"))` toma cada cada elemento del `RDD` (linea), lo convierte en sequencia de palabras y concatena estas secuencias:\n",
    "        - `lambda line: line.split(\" \")` es la **función** que toma una linea y la divide en una secuencia de palabras.\n",
    "        \n",
    "    - Su resultado es un array **distribuido** de palabras (`RDD[str]`).\n",
    "    \n",
    "* Al resultado de `flatMap` se aplica el método `filter` (línea 7):\n",
    "    - `filter(lambda word: word)` saca las palabras que son vacías (pueden aparecer?).\n",
    "    - `lambda word: word` es la **función** que pregunta si la palabra es vacía.\n",
    "    - `filter` devuelve un `RDD` que se almacena en `words`.\n",
    "\n",
    "* `words` tiene el método `map` (línea 11):\n",
    "    - `map(lambda word: (word,1))` agrega a cada palabra de `words` un `1`.\n",
    "    - El resultado es un **arreglo distribuido** de tuplas `RDD[(str, Int)]`.\n",
    "    \n",
    "* A este `RDD` se le aplica el método `reduceByKey` (línea 12):\n",
    "    - `reduceByKey(lambda n,m: n+m)` suma los `1`'s de las palabras iguales (la key es la palabra).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sc.textFile(\"../inputs/README.md\")\n",
    "\n",
    "words = lines \\\n",
    "    .flatMap(lambda line: line.split(\" \")) \\\n",
    "    .filter(lambda word: word)\n",
    "\n",
    "## filter-devuele true si no es vacia. \n",
    "#MapReduce\n",
    "wordCount = words \\\n",
    "    .map(lambda word: (word,1)) \\\n",
    "    .reduceByKey(lambda n,m: n+m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultado Word Count Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from 4\n",
      "Apache 3\n",
      "Zeppelin 3\n",
      "and 3\n",
      "to 3\n",
      "* 2\n",
      "### 2\n",
      "binary 2\n",
      "Please 2\n",
      "[User 2\n"
     ]
    }
   ],
   "source": [
    "result = wordCount.sortBy((lambda p: p[1]), ascending = False) # ordena por cantidad\n",
    "\n",
    "local_result = result.collect() # Traigo desde cluster\n",
    "\n",
    "for word, count in local_result[:10]: # tomo 10\n",
    "    print(word, count) # los imprimo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD de entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linea 0:  INFO [2020-10-31 10:15:43,582] ({pool-2-thread-3} Logging.scala[logInfo]:54) - Successfully started service 'SparkUI' on port 4044.\n"
     ]
    }
   ],
   "source": [
    "inputRDD = sc.textFile(\"../inputs/logs/zeppelin-interpreter-spark-*-nabucodonosor.log\") \n",
    "# se crea un nuevo RDD:\n",
    "skarkRDD = inputRDD.filter(lambda line: \"Successfully started service 'SparkUI' on port \" in line) \n",
    "\n",
    "for ln, l in enumerate(skarkRDD.collect()):\n",
    "    print(\"Linea {}:\".format(ln), l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 0 (word count)\n",
    "* Crear una celda abajo de esta (poner mouse debajo de esta celda y seleccionar \"Add Paragraph\").\n",
    "* Copiar el programa `wordcount` anterior en la misma (esta en 2 celdas).\n",
    "    - [`shift`]-[`flechas`] para seleccionar.\n",
    "    - [`ctrl`]-[`c`] para copiar.\n",
    "    - [`ctrl`]-[`v`] para pegar.\n",
    "* Modificarlo para leer todas la lineas de los archivos en `./licenses/`\n",
    "    - Ayuda: si al método `textFile` se le indica el nombre de un directorio carga todos los archivo del mismo.\n",
    "* Ejecute la celda ([`shift`]-[`enter`])\n",
    "* Ver la cantidad de tareas en SparkUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARE 17\n",
      "SERVICES; 17\n",
      "common 7\n",
      "(ii) 7\n",
      "code, 10\n",
      "\"Not 5\n",
      "Subject 10\n",
      "publicly 10\n",
      "perform, 7\n",
      "litigation 12\n"
     ]
    }
   ],
   "source": [
    "lines = sc.textFile(\"../inputs/licenses/\")\n",
    "\n",
    "words = lines \\\n",
    "    .flatMap(lambda line: line.split(\" \")) \\\n",
    "    .filter(lambda word: word) \n",
    "    \n",
    "## filter-devuele true si no es vacia. \n",
    "\n",
    "#MapReduce\n",
    "wordCount = words \\\n",
    "    .map(lambda word: (word,1)) \\\n",
    "    .reduceByKey(lambda n,m: n+m)\n",
    "\n",
    "local_result = wordCount.collect() # Traigo desde cluster\n",
    "\n",
    "for word, count in local_result[:10]: # tomo 10\n",
    "    print(word, count) # los imprimo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ver la cantidad de tareas en SparkUI - Rta: \n",
    " * Si no se ejecuta la instruccion `collect()` No se ejecuta ninguna tarea\n",
    "   Estp es por que spark tiene el enofoque lazy, no evalua nada y no ejecuta nada\n",
    "   hasta se necesite el re\n",
    " * si se ejecuta, Utiliza 124/124 tareas "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejecución de programas en Spark\n",
    "\n",
    "* En [Zeppelin](http://zeppelin.apache.org/) (como lo hacemos ahora)\n",
    "* En `pyspark` shell (tambien interactivo)\n",
    "* Como programa autónomo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pyspark shell\n",
    "\n",
    "* Abrir una terminal\n",
    "* Ir a la instalación Spark\n",
    "```sh\n",
    "cd ~/spark/spark-2.3.4-bin-hadoop2.7\n",
    "```\n",
    "* Arrancar el shell\n",
    "```sh\n",
    "./bin/pyspark\n",
    "```\n",
    "* Escribir en shell (después apretar `Enter`)\n",
    "```python\n",
    ">>> lines = sc.textFile(\"README.md\")\n",
    ">>> lines.first()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Programa autónomo\n",
    "\n",
    "* Ir a programa\n",
    "```sh\n",
    "cd diplodatos_bigdata/prog/word_count\n",
    "```\n",
    "* Actualizar repositorio\n",
    "```sh\n",
    "git pull\n",
    "```\n",
    "* Ver programa\n",
    "```sh\n",
    "less src/main/python/WordCount.py\n",
    "```\n",
    "  (salir con [`q`])\n",
    "  \n",
    "### Ejecucion de programa\n",
    "\n",
    "* Ejecutar\n",
    "```sh\n",
    "~/spark/spark-2.3.4-bin-hadoop2.7/bin/spark-submit --master local[4] src/main/python/WordCount.py \\\n",
    "        ~/spark/spark-2.3.4-bin-hadoop2.7/licenses/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Versión Spark en Zeppelin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0.1\n"
     ]
    }
   ],
   "source": [
    "print(sc.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principales referencias online:\n",
    "\n",
    "* [Documentación Spark](http://spark.apache.org/docs/2.2.1/)\n",
    "* [API Spark Python](http://spark.apache.org/docs/2.2.1/api/python/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicios MapReduce con Spark\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 1\n",
    "\n",
    "Modifique el programa *word count* siguiente para que cuente la **cantidad de apariciones de cada letra** en el archivo.\n",
    "\n",
    "* Ayuda: solo hay que modificar la linea 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e 102\n",
      "t 92\n",
      "a 83\n",
      "i 73\n",
      "p 61\n",
      "o 59\n",
      "n 58\n",
      "s 56\n",
      "l 54\n",
      "r 54\n"
     ]
    }
   ],
   "source": [
    "lines = sc.textFile(\"../inputs/README.md\")\n",
    "\n",
    "words = lines \\\n",
    "    .flatMap(lambda line: list(line)) \\\n",
    "    .filter(lambda word: word != ' ')\n",
    "\n",
    "\n",
    "#MapReduce\n",
    "wordCount = words \\\n",
    "    .map(lambda word: (word,1)) \\\n",
    "    .reduceByKey(lambda n,m: n+m)\n",
    "\n",
    "result = wordCount \\\n",
    "    .sortBy((lambda p: p[1]), ascending=False) # ordena por cantidad\n",
    "\n",
    "local_result = result.collect() # Traigo desde cluster\n",
    "\n",
    "for word, count in local_result[:10]: # tomo 10\n",
    "    print(word, count) # los imprimo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2\n",
    "\n",
    "Cada línea del archivo `~/diplodatos_bigdata/ds/links_raw.txt` contiene un url de una página web seguido de los links que posee a otras páginas web:\n",
    "```\n",
    "<url> <url link 1> <url link 2> ... <url link n>\n",
    "```\n",
    "\n",
    "Basándose en la utilización de la técnica de *MapReduce* que se mostró en el programa `word count` haga un programa en Spark que cuente la cantidad de links que apuntan a cada página.\n",
    "\n",
    "#### Ayuda\n",
    "\n",
    "A continuación está el comienzo del programa. Falta hacer el *MapReduce* y mostrar el resultado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.yahoo.com/ 199\n",
      "http://www.ca.gov/ 169\n",
      "http://www.leginfo.ca.gov/calaw.html 155\n",
      "http://www.linkexchange.com/ 134\n",
      "http://www.berkeley.edu/ 126\n",
      "http://www.sen.ca.gov/ 123\n",
      "http://home.netscape.com/comprod/mirror/index.html 109\n",
      "http://www.assembly.ca.gov/ 99\n",
      "http://www.epa.gov/ 95\n",
      "http://www.usgs.gov/ 84\n"
     ]
    }
   ],
   "source": [
    "baseDir = \"../inputs\" # llenar con el directorio git\n",
    "\n",
    "lines = sc.textFile(baseDir + \"/ds/links_raw.txt\")\n",
    "\n",
    "linksTo = lines \\\n",
    "    .flatMap(lambda l: l.split(\" \")[1:]) # separo los links y tomo los apuntados\n",
    "\n",
    "# Ahora linksTo tiene las paginas apuntadas\n",
    "\n",
    "# Completar los ...\n",
    "\n",
    "# MapReduce\n",
    "invLinkCount = linksTo.map(lambda link: (link,1)) \\\n",
    "    .reduceByKey(lambda n,m: n+m)\n",
    "\n",
    "result = invLinkCount.sortBy((lambda p: p[1]), ascending = False)\n",
    "\n",
    "local_result = result.collect() # Traigo desde cluster\n",
    "\n",
    "for word, count in local_result[:10]: # tomo 10\n",
    "    print(word, count) # los imprimo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FIN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
