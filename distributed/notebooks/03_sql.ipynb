{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diplomatura en Ciencia de Datos, Aprendizaje Automático y sus Aplicaciones\n",
    "## Programación Distribuida sobre Grandes Volúmenes de Datos\n",
    "\n",
    "Damián Barsotti \n",
    "\n",
    "### Facultad de Matemática Astronomía Física y Computación\n",
    "## Universidad Nacional de Córdoba\n",
    "\n",
    "<img src=\"http://program.ar/wp-content/uploads/2018/07/logo-UNC-FAMAF.png\" alt=\"Drawing\" style=\"width:80%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de comenzar\n",
    "En máquina virtual usando zeppeling\n",
    "\n",
    "1. Lanzar terminal\n",
    "1. Actualizar repo:\n",
    "```sh\n",
    "cd diplodatos_bigdata\n",
    "git pull\n",
    "```\n",
    "1. Lanzar [Zeppelin](http://zeppelin.apache.org/):\n",
    "```sh\n",
    "cd\n",
    "cd spark/zeppelin-0.8.3-bin-all\n",
    "./bin/zeppelin-daemon.sh start\n",
    "```\n",
    "1. En navegador abrir [http://localhost:8080](http://localhost:8080)\n",
    "1. Seleccionar `Import note`\n",
    "1. Elegir json en `diplodatos_bigdata/clases/03_sql/note.json`\n",
    "2. Seleccionar `Clase 03 - SQL`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets/Dataframes\n",
    "\n",
    "* Spark permite interactuar con datos estructurados (Bases de Datos tabulares) o semiestructurados (JSON) con su componente **Spark SQL**.\n",
    "* Sus interfaces son **SQL** y **Dataframe/Dataset** API .\n",
    "    - Programática, parecida a [Python Pandas dataframes](http://pandas.pydata.org/pandas-docs/stable/dsintro.html).\n",
    "    - Demasiado parecida. Ver [Koalas](https://github.com/databricks/koalas).\n",
    "* La API Dataset es tipada y solo existe para Scala y Java.\n",
    "\n",
    "\n",
    "<img src=\"https://bitbucket.org/bigdata_famaf/diplodatos_bigdata/raw/b17129f7118b3389b8c7f2f85fd89c6238fe0edd/clases/03_sql/unified_stack.png\" alt=\"Drawing\" style=\"width:70%;\"/>\n",
    "\n",
    "### API 2.x.x unificada\n",
    "\n",
    "<img src=\"https://bitbucket.org/bigdata_famaf/diplodatos_bigdata/raw/b17129f7118b3389b8c7f2f85fd89c6238fe0edd/clases/03_sql/dataset_dataframe_unificado.png\" alt=\"Drawing\" style=\"width:70%;\"/>\n",
    "\n",
    "\n",
    "### SparkSession\n",
    "\n",
    "* Para acceder al cluster desde la API se utiliza `SparkSession`.\n",
    "* El `SparkContext` deriva de él.\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark ejemplo\") \\\n",
    "    .config(\"spark.some.config.option\", \"algun-valor\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "sc = spark.sparkContext\n",
    "\n",
    "```\n",
    "\n",
    "* En Zeppelin ya están predefinidos: \n",
    "  - `SparkSession` objeto `spark` \n",
    "  - `SparkContext` objeto `sc`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join, abspath\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warehouse_location = abspath('spark-warehouse')\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"03_sql\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", warehouse_location) \\\n",
    "    .config(\"spark.sql.catalogImplementation\", \"hive\")\\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lectura de datos\n",
    "\n",
    "### Estructurados y semiestructurados\n",
    "\n",
    "#### Formatos:\n",
    "\n",
    "* json\n",
    "* csv\n",
    "* parquet\n",
    "* orc\n",
    "* libsvm\n",
    "* text\n",
    "* ...\n",
    "\n",
    "#### Fuentes de datos:\n",
    "\n",
    "* Archivos en fs local o distribuid (ej hdfs)\n",
    "* jdbc (posgress, oracle, mysql,...)\n",
    "* Apache Hive (se usa execution backend Spark en ves de MR)\n",
    "* Amazon Redshift, S3\n",
    "* Azure Storage Services\n",
    "* Cassandra\n",
    "* MongoDB\n",
    "* Neo4j\n",
    "* ...\n",
    "\n",
    "\n",
    "### Ejemplo\n",
    "\n",
    "#### Tabla de perfiles [last.fm](last.fm)\n",
    "\n",
    "Formato:\n",
    "\n",
    "```\n",
    "id \\t gender ('m'|'f'|empty) \\t age (int|empty) \\t country (str|empty) \\t registered (date|empty)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id\tgender\tage\tcountry\tregistered\n",
      "user_000001\tm\t\tJapan\tAug 13, 2006\n",
      "user_000002\tf\t\tPeru\tFeb 24, 2006\n",
      "user_000003\tm\t22\tUnited States\tOct 30, 2005\n",
      "user_000004\tf\t\t\tApr 26, 2006\n",
      "user_000005\tm\t\tBulgaria\tJun 29, 2006\n",
      "user_000006\t\t24\tRussian Federation\tMay 18, 2006\n",
      "user_000007\tf\t\tUnited States\tJan 22, 2006\n",
      "user_000008\tm\t23\tSlovakia\tSep 28, 2006\n",
      "user_000009\tf\t19\tUnited States\tJan 13, 2007\n"
     ]
    }
   ],
   "source": [
    "!head ../inputs/ds/userid-profile.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ver en `inferSchema` siguiente si `inferSchema=False`\n",
    "> todo los datos son tomados como `string`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiles = spark.read.load(\n",
    "    \"../inputs/ds/userid-profile.tsv\", format=\"csv\",\n",
    "    delimiter=\"\\t\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- registered: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "profiles.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+----+------------------+------------+\n",
      "|         id|gender| age|           country|  registered|\n",
      "+-----------+------+----+------------------+------------+\n",
      "|user_000001|     m|null|             Japan|Aug 13, 2006|\n",
      "|user_000002|     f|null|              Peru|Feb 24, 2006|\n",
      "|user_000003|     m|  22|     United States|Oct 30, 2005|\n",
      "|user_000004|     f|null|              null|Apr 26, 2006|\n",
      "|user_000005|     m|null|          Bulgaria|Jun 29, 2006|\n",
      "|user_000006|  null|  24|Russian Federation|May 18, 2006|\n",
      "|user_000007|     f|null|     United States|Jan 22, 2006|\n",
      "|user_000008|     m|  23|          Slovakia|Sep 28, 2006|\n",
      "|user_000009|     f|  19|     United States|Jan 13, 2007|\n",
      "|user_000010|     m|  19|            Poland| May 4, 2006|\n",
      "|user_000011|     m|  21|           Finland| Sep 8, 2005|\n",
      "|user_000012|     f|  28|     United States|Mar 30, 2005|\n",
      "|user_000013|     f|  25|           Romania|Sep 25, 2006|\n",
      "|user_000014|  null|null|              null|Jan 27, 2006|\n",
      "|user_000015|  null|  21|     Cote D'Ivoire| Oct 3, 2006|\n",
      "|user_000016|     m|null|    United Kingdom| Aug 5, 2005|\n",
      "|user_000017|     m|  22|           Morocco|Aug 27, 2007|\n",
      "|user_000018|  null|  22|    United Kingdom|Aug 26, 2005|\n",
      "|user_000019|     f|  29|            Mexico|Nov 10, 2005|\n",
      "|user_000020|     f|  27|           Germany|Jul 24, 2006|\n",
      "+-----------+------+----+------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "profiles.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query SQL plano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------+\n",
      "|           country|cantidad|\n",
      "+------------------+--------+\n",
      "|     United States|     228|\n",
      "|    United Kingdom|     126|\n",
      "|              null|      85|\n",
      "|            Poland|      50|\n",
      "|           Germany|      36|\n",
      "|            Norway|      35|\n",
      "|           Finland|      32|\n",
      "|            Canada|      32|\n",
      "|            Turkey|      28|\n",
      "|             Italy|      27|\n",
      "|            Sweden|      24|\n",
      "|       Netherlands|      23|\n",
      "|         Australia|      22|\n",
      "|Russian Federation|      22|\n",
      "|            Brazil|      20|\n",
      "|             Spain|      17|\n",
      "|            France|      14|\n",
      "|            Mexico|      12|\n",
      "|           Belgium|       9|\n",
      "|         Argentina|       9|\n",
      "+------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "profiles.createOrReplaceTempView(\"users\")\n",
    "\n",
    "#Cantidad de usuarios por país\n",
    "nUsr4Ctry = spark.sql(\n",
    "    \"SELECT country, count(*) AS cantidad FROM users GROUP BY country ORDER BY cantidad DESC\")\n",
    "\n",
    "nUsr4Ctry.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query SQL programático"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------+\n",
      "|           country|cantidad|\n",
      "+------------------+--------+\n",
      "|     United States|     228|\n",
      "|    United Kingdom|     126|\n",
      "|              null|      85|\n",
      "|            Poland|      50|\n",
      "|           Germany|      36|\n",
      "|            Norway|      35|\n",
      "|           Finland|      32|\n",
      "|            Canada|      32|\n",
      "|            Turkey|      28|\n",
      "|             Italy|      27|\n",
      "|            Sweden|      24|\n",
      "|       Netherlands|      23|\n",
      "|         Australia|      22|\n",
      "|Russian Federation|      22|\n",
      "|            Brazil|      20|\n",
      "|             Spain|      17|\n",
      "|            France|      14|\n",
      "|            Mexico|      12|\n",
      "|         Argentina|       9|\n",
      "|           Belgium|       9|\n",
      "+------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "nUsr4Ctry2 = profiles \\\n",
    "                .groupBy(\"country\").agg(count(\"*\").alias(\"cantidad\")) \\\n",
    "                .orderBy(\"cantidad\", ascending=False)\n",
    "# Cada operación SQL es un método\n",
    "\n",
    "nUsr4Ctry2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio\n",
    "\n",
    "Complete los siguientes programas que calculan en un Dataframe \n",
    "la cantidad de usuarios por pais desagregando por sexo y \n",
    "ordenando por la cantidad de mayor a menor, usando **SQL plano y programático**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+--------+\n",
      "|       country|gender|cantidad|\n",
      "+--------------+------+--------+\n",
      "| United States|     m|     113|\n",
      "| United States|     f|     104|\n",
      "|United Kingdom|     m|      81|\n",
      "|          null|  null|      49|\n",
      "|United Kingdom|     f|      34|\n",
      "|        Poland|     f|      29|\n",
      "|          null|     f|      23|\n",
      "|       Germany|     m|      22|\n",
      "|        Poland|     m|      19|\n",
      "|        Turkey|     m|      18|\n",
      "|        Canada|     m|      18|\n",
      "|       Finland|     m|      17|\n",
      "|        Norway|     f|      15|\n",
      "|        Sweden|     m|      15|\n",
      "|        Canada|     f|      14|\n",
      "|         Italy|     m|      14|\n",
      "|       Germany|     f|      14|\n",
      "|       Finland|     f|      13|\n",
      "|         Italy|     f|      13|\n",
      "|   Netherlands|     m|      13|\n",
      "+--------------+------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Con SQL plano\n",
    "\n",
    "nUsr4CtryGen = spark.sql(\n",
    "    \"SELECT country, gender, count(*) AS cantidad FROM users GROUP BY country, gender ORDER BY cantidad DESC\")\n",
    "\n",
    "nUsr4CtryGen.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+--------+\n",
      "|       country|gender|cantidad|\n",
      "+--------------+------+--------+\n",
      "| United States|     m|     113|\n",
      "| United States|     f|     104|\n",
      "|United Kingdom|     m|      81|\n",
      "|          null|  null|      49|\n",
      "|United Kingdom|     f|      34|\n",
      "|        Poland|     f|      29|\n",
      "|          null|     f|      23|\n",
      "|       Germany|     m|      22|\n",
      "|        Poland|     m|      19|\n",
      "|        Turkey|     m|      18|\n",
      "|        Canada|     m|      18|\n",
      "|       Finland|     m|      17|\n",
      "|        Sweden|     m|      15|\n",
      "|        Norway|     f|      15|\n",
      "|        Canada|     f|      14|\n",
      "|       Germany|     f|      14|\n",
      "|         Italy|     m|      14|\n",
      "|          null|     m|      13|\n",
      "|       Finland|     f|      13|\n",
      "|   Netherlands|     m|      13|\n",
      "+--------------+------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Con SQL porgramático\n",
    "\n",
    "nUsr4CtryGen2 = profiles \\\n",
    "                .groupBy(\"country\", \"gender\").agg(count(\"*\").alias(\"cantidad\")) \\\n",
    "                .orderBy(\"cantidad\", ascending=False)\n",
    "\n",
    "nUsr4CtryGen2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lectura desde JDBC (Java Database Connectivity)\n",
    "\n",
    "```python\n",
    "df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://localhost/test\") \\\n",
    "    .option(\"dbtable\", \"projects\") \\\n",
    "    .option(\"user\", \"username\") \\\n",
    "    .option(\"password\", \"password\") \\\n",
    "    .load()\n",
    "```\n",
    "Más información en:\n",
    "\n",
    "* [Spark SQL](http://spark.apache.org/docs/latest/sql-programming-guide.html#jdbc-to-other-databases).\n",
    "* [Ejemplo](https://supergloo.com/spark-sql/spark-sql-mysql-python-example-jdbc/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lectura desde Apache Hive \n",
    "\n",
    "```scala\n",
    "warehouse_location = abspath('spark-warehouse')\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Ejemplo Spark Hive\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", warehouse_location) \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sqlDF = spark.sql(\"SELECT key, value FROM src WHERE key < 10 ORDER BY key\")\n",
    "```\n",
    "Más información en:\n",
    "\n",
    "* [Spark SQL](https://spark.apache.org/docs/latest/sql-programming-guide.html#hive-tables)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"drop table if exists mytable\") # borro la tabla si existe\n",
    "\n",
    "spark.sql(\"create table mytable as select * from users\")\n",
    "# Simula Hive Data Warehouse local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiles.write.mode(\"overwrite\").save(\"./profiles.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio\n",
    "\n",
    "Complete el siguiente programa par calcular la edad promedio por género \n",
    "y guarde el resultado como tabla SQL y como archivo parquet.\n",
    "\n",
    "#### SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"drop table if exists gen_prom\") # borro la tabla si existe\n",
    "\n",
    "spark.sql(\"create table gen_prom as SELECT gender, avg(age) AS age_avg FROM users GROUP BY gender\")\n",
    "\n",
    "#Cargo tabla y muestro su contenido\n",
    "\n",
    "spark.sql(\"select * from gen_prom\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+\n",
      "|gender|           age_avg|\n",
      "+------+------------------+\n",
      "|     m|25.630573248407643|\n",
      "|     f| 24.13157894736842|\n",
      "|  null|              32.0|\n",
      "+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "genProm = profiles \\\n",
    "            .groupBy('gender').agg(avg('age').alias(\"age_avg\"))\n",
    "\n",
    "genProm.write.mode(\"overwrite\").save(\"./gen_prom.parquet\")\n",
    "\n",
    "# Cargo parquet y muestro su contenido\n",
    "spark.read.load(\"./gen_prom.parquet\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mas info\n",
    "\n",
    "* [API Python SQL](http://spark.apache.org/docs/2.2.1/api/python/pyspark.sql.html)\n",
    "* [Function Reference](http://spark.apache.org/docs/2.2.1/api/python/pyspark.sql.html#module-pyspark.sql.functions)\n",
    "* [Doc Spark SQL](http://spark.apache.org/docs/2.2.1/sql-programming-guide.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eficiencia\n",
    "---\n",
    "\n",
    "Los **RDD** tienen el overhead de la *serialización*:\n",
    "\n",
    "* cuando los objetos se transfieren (por red) y guardan (disco)\n",
    "* overhead de garbage collector\n",
    "\n",
    "Los **Datasets** solucionan estos problemas:\n",
    "\n",
    "* Serializa a binario usando **encoders**\n",
    "    - parte del proyecto Tungsten\n",
    "    - permite operaciones sin deserializar\n",
    "    - corre *off-heap* (sin garbage collection)\n",
    "    - código para serialización generado en forma dinámica\n",
    "* Con la información de la estructura (*schema*) Spark hace optimizaciones.\n",
    "    - Usa *Catalyst optimizer*.\n",
    "    - Transfiere solo columnas usadas, no objetos enteros (relational query plan).\n",
    "\n",
    "#### WordCount con RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultado:\n",
      "from 4\n",
      "Apache 3\n",
      "Zeppelin 3\n",
      "and 3\n",
      "to 3\n"
     ]
    }
   ],
   "source": [
    "linesRDD = sc.textFile(\"../inputs/README.md\")\n",
    "\n",
    "wordsRDD = linesRDD \\\n",
    "            .flatMap(lambda l: l.split(\" \")) \\\n",
    "            .filter(lambda w:  w)\n",
    "#MapReduce:\n",
    "wordCountRDD = wordsRDD.map(lambda w: (w,1)) \\\n",
    "                .reduceByKey(lambda nx,ny:  nx+ny)\n",
    "\n",
    "resultRDD = wordCountRDD \\\n",
    "                .sortBy((lambda p: p[1]), ascending = False)\n",
    "                # ordena por cantidad\n",
    "\n",
    "print(\"Resultado:\")\n",
    "\n",
    "for w, c in resultRDD.collect()[:5]: #  traigo resultados\n",
    "    print(w, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://bitbucket.org/bigdata_famaf/diplodatos_bigdata/raw/b17129f7118b3389b8c7f2f85fd89c6238fe0edd/clases/03_sql/Distributed-Wordcount-Chart.png\" alt=\"Drawing\" style=\"width:70%;\"/>\n",
    "\n",
    "#### WordCount con DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultado:\n",
      "+--------+-----+\n",
      "|words   |count|\n",
      "+--------+-----+\n",
      "|from    |4    |\n",
      "|and     |3    |\n",
      "|to      |3    |\n",
      "|Apache  |3    |\n",
      "|Zeppelin|3    |\n",
      "+--------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split, explode\n",
    "\n",
    "linesDF = spark.read.text(\"../inputs/README.md\").toDF(\"lineas\")\n",
    "\n",
    "wordsDF = linesDF \\\n",
    "            .select(explode(split(\"lineas\", ' ')).alias(\"words\")) \\\n",
    "            .filter(\"words != ''\")\n",
    "\n",
    "wordCountDF = wordsDF \\\n",
    "                .groupBy(\"words\").count()\n",
    "\n",
    "resultDF = wordCountDF \\\n",
    "                .orderBy(\"count\", ascending=False)\n",
    "#                // ordena por cantidad\n",
    "\n",
    "print(\"Resultado:\")\n",
    "\n",
    "resultDF.show(n=5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://bitbucket.org/bigdata_famaf/diplodatos_bigdata/raw/b17129f7118b3389b8c7f2f85fd89c6238fe0edd/clases/03_sql/Memory-Usage-when-Caching-Chart.png\" alt=\"Drawing\" style=\"width:70%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tungsten en acción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(0, 10)\n",
      "1000000\n"
     ]
    }
   ],
   "source": [
    "import math \n",
    "\n",
    "ints = range(int(math.pow(10, 6)))\n",
    "print(ints[:10])\n",
    "intsRDD = sc.parallelize(ints).setName(\"intsRDD\").cache()\n",
    "\n",
    "# Fuerzo evaluacion\n",
    "print (intsRDD.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000\n"
     ]
    }
   ],
   "source": [
    "# Ver sparkui storage. Descomentar proximas lineas y ver de nuevo\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "intsDF = spark.createDataFrame(ints, IntegerType()).cache()\n",
    "intsDF.cache()\n",
    "\n",
    "print (intsDF.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la segunda tarea, se demora mas porque se ejecutan en mas etapas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n",
      "+-------+---------+\n",
      "|   name|(age + 1)|\n",
      "+-------+---------+\n",
      "|Michael|     null|\n",
      "|   Andy|       31|\n",
      "| Justin|       20|\n",
      "+-------+---------+\n",
      "\n",
      "+-------+---------+\n",
      "|   name|(age + 1)|\n",
      "+-------+---------+\n",
      "|Michael|     null|\n",
      "|   Andy|       31|\n",
      "| Justin|       20|\n",
      "+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.json(\"../inputs/ds/people.json\")\n",
    "\n",
    "# Displays the content of the DataFrame to stdout\n",
    "df.show()\n",
    "\n",
    "# Selecciona todo incrementando la edad\n",
    "df.selectExpr(\"name\", \"age + 1\").show()\n",
    "\n",
    "# O tambien\n",
    "df.select(\"name\", df.age + 1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 30|Andy|\n",
      "+---+----+\n",
      "\n",
      "+----+-----+\n",
      "| age|count|\n",
      "+----+-----+\n",
      "|  19|    1|\n",
      "|null|    1|\n",
      "|  30|    1|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Selecciona personas con mas de 21 años\n",
    "df.filter(\"age > 21\").show()\n",
    "# Cuenta personas por edad\n",
    "df.groupBy(\"age\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ventajas/desventajas de las diferentes APIs\n",
    "---\n",
    "\n",
    "### APIs tipadas y no tipadas\n",
    "\n",
    "\n",
    "| Lenguaje  | Abstracción Principal  |\n",
    "|---|---|\n",
    "|Scala   | Dataset[T] y Dataframe (Datset[Row])  |\n",
    "|Python   | Dataset[T]  |\n",
    "| R  | Dataframe  |\n",
    "\n",
    "#### Detección de errores\n",
    "\n",
    "<img \n",
    "src=\"https://bitbucket.org/bigdata_famaf/diplodatos_bigdata/raw/b17129f7118b3389b8c7f2f85fd89c6238fe0edd/clases/03_sql/type-safety-spectrum.png\" alt=\"Drawing\" style=\"width:70%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cuando usar Datasets Dataframes o RDD\n",
    "\n",
    "* Si se necesita expresiones de alto nivel, filters, maps, aggregations, promedios, sumatorias, queries SQL, acceso por columna y funciones lambda sobre datos semiestructurados\n",
    "    - para desarrollar aplicaciones finales (Data Ingeeniering) usar **Datasets**.\n",
    "    - para análisis interactivo (Data Scientist) usar **Dataframes**. \n",
    "* Si se necesita mayor seguridad de tipos chequeandolos a tiempo de compilación, objetos JVM, beneficios de optimización Catalyst y código eficiente con Tungsten usar **Datasets**.\n",
    "* Si se quiere una API unificada a traves de la la librerías Spark usar **DataFrames** o **Datasets**.\n",
    "* Si se quiere trabajar en R no queda otra que usar **DataFrames**.\n",
    "* Si se quiere trabajar en Python no queda otra que usar **DataFrames** y recurrir a **RDDs** si se necesita mayor control.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id\tgender\tage\tcountry\tregistered\n",
      "user_000001\tm\t\tJapan\tAug 13, 2006\n",
      "user_000002\tf\t\tPeru\tFeb 24, 2006\n",
      "user_000003\tm\t22\tUnited States\tOct 30, 2005\n",
      "user_000004\tf\t\t\tApr 26, 2006\n",
      "user_000005\tm\t\tBulgaria\tJun 29, 2006\n",
      "user_000006\t\t24\tRussian Federation\tMay 18, 2006\n",
      "user_000007\tf\t\tUnited States\tJan 22, 2006\n",
      "user_000008\tm\t23\tSlovakia\tSep 28, 2006\n",
      "user_000009\tf\t19\tUnited States\tJan 13, 2007\n"
     ]
    }
   ],
   "source": [
    "!head ../inputs/ds/userid-profile.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+----+------------------+------------+\n",
      "|         id|gender| age|           country|  registered|\n",
      "+-----------+------+----+------------------+------------+\n",
      "|user_000001|     m|null|             Japan|Aug 13, 2006|\n",
      "|user_000002|     f|null|              Peru|Feb 24, 2006|\n",
      "|user_000003|     m|  22|     United States|Oct 30, 2005|\n",
      "|user_000004|     f|null|              null|Apr 26, 2006|\n",
      "|user_000005|     m|null|          Bulgaria|Jun 29, 2006|\n",
      "|user_000006|  null|  24|Russian Federation|May 18, 2006|\n",
      "|user_000007|     f|null|     United States|Jan 22, 2006|\n",
      "|user_000008|     m|  23|          Slovakia|Sep 28, 2006|\n",
      "|user_000009|     f|  19|     United States|Jan 13, 2007|\n",
      "|user_000010|     m|  19|            Poland| May 4, 2006|\n",
      "|user_000011|     m|  21|           Finland| Sep 8, 2005|\n",
      "|user_000012|     f|  28|     United States|Mar 30, 2005|\n",
      "|user_000013|     f|  25|           Romania|Sep 25, 2006|\n",
      "|user_000014|  null|null|              null|Jan 27, 2006|\n",
      "|user_000015|  null|  21|     Cote D'Ivoire| Oct 3, 2006|\n",
      "|user_000016|     m|null|    United Kingdom| Aug 5, 2005|\n",
      "|user_000017|     m|  22|           Morocco|Aug 27, 2007|\n",
      "|user_000018|  null|  22|    United Kingdom|Aug 26, 2005|\n",
      "|user_000019|     f|  29|            Mexico|Nov 10, 2005|\n",
      "|user_000020|     f|  27|           Germany|Jul 24, 2006|\n",
      "+-----------+------+----+------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import unix_timestamp, from_unixtime\n",
    "\n",
    "profiles = spark.read.load(\"../inputs/ds/userid-profile.tsv\",\n",
    "                    format=\"csv\", delimiter=\"\\t\", header=True, inferSchema=True)\n",
    "profiles.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|day_week|count|\n",
      "+--------+-----+\n",
      "|     Sun|  148|\n",
      "|    null|    8|\n",
      "|     Mon|  147|\n",
      "|     Thu|  131|\n",
      "|     Sat|  125|\n",
      "|     Wed|  155|\n",
      "|     Fri|  131|\n",
      "|     Tue|  147|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "regByDayOfWeek = profiles.select(\n",
    "    \"registered\", unix_timestamp(\"registered\", format=\"MMM d, yyyy\").alias(\"reg_sec\")) \\\n",
    "        .select(\"*\", from_unixtime('reg_sec',\"E\").alias(\"day_week\"))\n",
    "\n",
    "regByDayOfWeek.groupBy(\"day_week\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fin"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
